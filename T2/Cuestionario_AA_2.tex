%%% LaTeX Template: Two column article
%%%
%%% Source: http://www.howtotex.com/
%%% Feel free to distribute this template, but please keep to referal to http://www.howtotex.com/ here.
%%% Date: February 2011

%%% Preamble
\documentclass[  DIV=calc,%
paper=a4,%
fontsize=11pt]{scrartcl}             % KOMA-article class

\usepackage[spanish]{babel}                    % English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}        % Better typography
\usepackage{amsmath,amsfonts,amsthm}          % Math packages
\usepackage[pdftex]{graphicx}                  % Enable pdflatex
\usepackage[svgnames]{xcolor}                  % Enabling colors by their 'svgnames'
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}  % Custom captions under/above floats
\usepackage{epstopdf}                        % Converts .eps to .pdf
\usepackage{subfig}                          % Subfigures
\usepackage{booktabs}                        % Nicer tables
\usepackage{fix-cm}                          % Custom fontsizes
\usepackage{hyperref}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[light]{kurier}

\usepackage{IEEEtrantools}

%%% Custom sectioning (sectsty package)
\usepackage{sectsty}                          % Custom sectioning (see below)
\allsectionsfont{%                              % Change font of al section commands
\usefont{T1}{mdugm}{b}{it}%                    % bch-b-n: CharterBT-Bold font
}

\sectionfont{%                                % Change font of \section command
\usefont{T1}{mdugm}{b}{it}%                    % bch-b-n: CharterBT-Bold font
}

%%% Headers and footers
\usepackage{fancyhdr}                        % Needed to define custom headers/footers
\pagestyle{fancy}                            % Enabling the custom headers/footers
\usepackage{lastpage}

% Header (empty)
\lhead{}
\chead{}
\rhead{}
% Footer (you may change this to your own needs)
\lfoot{\footnotesize \miit{Alejandro Alcalde} \textbullet ~}
\cfoot{}
\rfoot{\footnotesize Página \thepage\ de \pageref{LastPage}}  % "Page 1 of 2"
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.4pt}



%%% Creating an initial of the very first character of the content
\usepackage{lettrine}
\newcommand{\initial}[1]{%
\lettrine[lines=3,lhang=0.3,nindent=0em]{
\color{DarkGoldenrod}
{\textsf{#1}}}{}}



%%% Title, author and date metadata
\usepackage{titling}                              % For custom titles

\newcommand{\HorRule}{\color{DarkGoldenrod}%      % Creating a horizontal rule
\rule{\linewidth}{1pt}%
}

\pretitle{\vspace{-30pt} \begin{flushleft} \HorRule
\fontsize{50}{50} \usefont{T1}{kurier}{l}{it} \color{DarkRed} \selectfont
}
\title{Aprendizaje Automático: Cuestionario 2}          % Title of your article goes here
\posttitle{\par\end{flushleft}\vskip 0.5em}

\preauthor{\begin{flushleft}
\large \lineskip 0.5em \usefont{T1}{mdugm}{m}{it} \color{DarkRed}}
\author{Alejandro Alcalde,}                      % Author name goes here
\postauthor{\footnotesize \usefont{OT1}{mdugm}{m}{it} \color{Black}
Universidad de Granada                 % Institution of author
\par\end{flushleft}\HorRule}

\date{\usefont{T1}{mdugm}{b}{it}\selectfont\today}                                        % No date

\newcommand{\miit}[1]{{\textbf{\textit{#1}}}}

%%% Begin document
\begin{document}
  \maketitle
  \tableofcontents
  \thispagestyle{fancy}       % Enabling the custom headers/footers for the first page
  % The first character should be within \initial{}

  \centerline {\textbf{Todas las preguntas tienen el mismo valor}}
  \vspace{5pt}
  \begin{enumerate}

    \item Sean $\textbf{x}$ e $\textbf{y}$ dos vectores de observaciones de tamaño $N$. Sea
    \[
    \mathrm{cov}(\textbf{x},\textbf{y})=\frac{1}{N}\sum_{i=1}^N (x_i-\bar{x})(y_i-\bar{y})
    \]
    la covarianza de dichos vectores, donde $\bar{z}$ representa el valor medio de los elementos de $\textbf{z}$. Considere ahora una matriz $\mathrm{X}$ cuyas columnas representan vectores de observaciones. La matriz de covarianzas asociada a la matriz $\mathrm{X}$ es el conjunto de covarianzas definidas por cada dos de sus vectores columnas. Defina la expresión matricial que expresa la matriz $\mathrm{cov}(\mathrm{X})$ en función de la matriz $\mathrm{X}$

    \miit{Respuesta:}

    Como dice el enunciado, vamos a tener una matriz $\mathrm{X}$ en la que cada columna representa vectores de observaciones ($\mathbf{x}_1, \mathbf{x}_2 \dots \mathbf{x}_m$), esto es:
    \[
    X =
    \begin{pmatrix}
      x_{11} & x_{12} & \cdots & x_{1m} \\
      x_{21} & x_{22} & \cdots & x_{2m} \\
      \vdots          & \vdots & \ddots & \vdots \\
      x_{N1} & x_{N2} & \cdots & x_{Nm}
    \end{pmatrix}
    \]

    A cada elemento de un vector columna debemos restarle la media de su columna, es decir:

    \[
    X - \bar{X}=
    \begin{pmatrix}
      x_{11} - \bar{x}_1   & x_{12} - \bar{x}_2   & \cdots & x_{1m} - \bar{x}_m \\
      x_{21} - \bar{x}_1   & x_{22} - \bar{x}_2   & \cdots & x_{2m} - \bar{x}_m \\
      \vdots              & \vdots               & \ddots & \vdots \\
      x_{N1} - \bar{x}_1   & x_{N2} - \bar{x}_2   & \cdots & x_{Nm} - \bar{x}_m
    \end{pmatrix}
    \]

    Para ello crearemos una matriz que contenga las medias de los vectores columnas, la primera matriz es de dimensiones $N\times N$ y la segunda es $X$:

    \[
    \bar{X} =
    \frac{1}{N}
    \begin{pmatrix}
      1   & 1   & \cdots & 1 \\
      1   & 1   & \cdots & 1 \\
      \vdots              & \vdots  & \ddots & \vdots \\
      1   & 1   & \cdots & 1
    \end{pmatrix}
    \begin{pmatrix}
      x_{11} & x_{12} & \cdots & x_{1m} \\
      x_{21} & x_{22} & \cdots & x_{2m} \\
      \vdots          & \vdots & \ddots & \vdots \\
      x_{N1} & x_{N2} & \cdots & x_{Nm}
    \end{pmatrix}
    \]

    Tras esta operación tenemos la matrix $\bar{X}$ con las medias de cada vector:

    \[
    \bar{X}=
    \begin{pmatrix}
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m \\
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m \\
      \vdots              & \vdots        & \ddots & \vdots \\
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m
    \end{pmatrix}
    \]

    Ahora ya tenemos todo lo necesario para re-escribir la Covarianza en forma matricial:

    \[
    \mathrm{cov}(X) = \frac{1}{N}\left( X - \bar{X} \right)^T\left( X - \bar{X} \right)
    \]

    Lo cual dará resultado a una matriz $m\times m$ con las covarianzas de cada dos vectores columnas:

    \[
    \mathrm{cov}(X) =
    \begin{pmatrix}
      \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_1)   & \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_2)  & \cdots   &  \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_m)  \\
      \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_1)  & \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_2)  & \cdots   &  \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_m) \\
      \vdots                                    & \vdots                                      & \ddots   &   \vdots                                   \\
      \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_1)   & \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_2)  & \cdots   & \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_m)
    \end{pmatrix}
    \]

    \item Considerar la matriz hat definida en regresión,  $\mathrm{H}=\mathrm{X(X^TX)^{-1}X^T}$, donde $\mathrm{X}$ es una matriz  $N\times (d+1)$, y $\mathrm{X^TX}$ es invertible.
    \begin{enumerate}
      \item Mostrar que $\mathrm{H}$ es simétrica.

      \miit{Respuesta:}

        Usando las propiedades de las transpuestas comprobamos que en efecto es simétrica:
        \begin{proof}
          \begin{IEEEeqnarray*}{rCl}
            \left (X \left (X^TX\right )^{-1}X^T\right )^T & = & \left ( X^T \right )^T \left ( \left (X^TX \right )^{-1} \right )^T X^T \\
            & = & X\left ( \left ( X^T X\right )^T \right )^{-1}X^T \\
            & = & X \left ( \left ( X^T\left ( X^T \right )^T \right )^{-1} \right )X^T \\
            & = & X \left (X^TX\right )^{-1}X^T
          \end{IEEEeqnarray*}
        \end{proof}
        Se han hecho uso de las siguientes propiedadas de las transpuestas:
        \begin{itemize}
          \item $(A)^T = A$
          \item $(A_1A_2\dots A_k)^T = A_k^T\dots A_2^TA_1^T$
          \item $\left ( A^{-1}\right )^T = \left ( A^{T}\right )^{-1}$
        \end{itemize}
      \item Mostrar que $\mathrm{H^K=H}$ para cualquier entero positivo K.

      \miit{Respuesta:}

      \begin{proof}
        \begin{IEEEeqnarray*}{rCl}
          H^2 & = & X \overbrace{\left (X^TX\right )^{-1}X^T X}^\mathrm{I} \left (X^TX\right )^{-1}X^T \\
          & = & X I \left (X^TX\right )^{-1}X^T \\
          & = & X \left (X^TX\right )^{-1}X^T \qedhere
        \end{IEEEeqnarray*}
      \end{proof}
      Luego $H^2 = H$. Para $H^3$ pasaría igual, y por inducción llegamos a la conclusión $\mathrm{H^K=H}, \forall k \in \mathbb{N}$:
      \begin{IEEEeqnarray*}{rCl}
        H^3 & = & H^2 H; \\
            & = & HH \\
            & = & H^2 \\
            & = & H
      \end{IEEEeqnarray*}
      Ahora para el caso general tenemos:
      \begin{proof}
        \begin{IEEEeqnarray*}{rCl}
          H^{n+1} & = & H^n H; \\
              & = & HH \\
              & = & H \qedhere
        \end{IEEEeqnarray*}
      \end{proof}
    \end{enumerate}

    %
    \item
    Resolver el siguiente problema: Encontrar el punto $(x_0,y_0)$ sobre la línea $ax+by+d=0$ que este más cerca del punto $(x_1,y_1)$.
    %
    \item Consideremos el problema de optimización lineal con restricciones definido por
    \[
    \begin{array}{c}
      \min_{\textbf{z}} \mathrm{\mathbf{c}^T\textbf{z}} \\
      \hbox{Sujeto a } \mathrm{A\textbf{z} \leq \mathbf{b}}
    \end{array}
    \]
    donde \textbf{c} y \textbf{b} son vectores y A es una matriz.

    \begin{enumerate}
      \item Para un conjunto de datos linealmente separable mostrar que para algún $\textbf{w}$ se debe de verificar la condición  $\mathrm{y_n\textbf{w}^T\textbf{x}_n>0 }$ para todo $\mathrm{(\textbf{x}_n,y_n)}$ del conjunto.
      \item Formular un problema de programación lineal que resuelva el problema de la búsqueda del hiperplano separador. Es decir, identifique quienes son  A, \textbf{z}, \textbf{b} y \textbf{c} para este caso.
    \end{enumerate}


    \item Probar que en el caso general de funciones con ruido se verifica que $\mathbb{E}_{\mathcal{D}}[E_{out}]= \sigma^2+\texttt{\textbf{bias}}+\texttt{\textbf{var}}$ ( ver transparencias de clase)

    \item  Consideremos las mismas condiciones generales del enunciado del Ejercicio.2 del apartado de Regresión de la relación de ejercicios.2.
    Considerar ahora $\sigma=0.1$ y $d=8$, ¿cual es el más pequeño tamaño muestral que resultará en un valor esperado de $E_{in}$ mayor de $0.008$?.
    %
    %
    \item En regresión logística mostrar que
    \[
    \nabla E_{in}(\textbf{w})=-\frac{1}{N}\sum_{n=1}^{N}\frac{y_n\textbf{x}_n}{1+e^{y_n \textbf{w}^T\textbf{x}_n}}= \frac{1}{N}\sum_{n=1}^{N}-y_n\textbf{x}_n\sigma(-y_n\textbf{w}^T\textbf{x}_n)
    \]

    Argumentar que un ejemplo mal clasificado contribuye  al gradiente más que un ejemplo bien clasificado.

    \item  Definamos el error en un punto $(\textbf{x}_n,y_n)$ por
    \[
    \textbf{e}_n(\textbf{w})=\max(0,-y_n\textbf{w}^T\textbf{x}_n)
    \]
    Argumentar que el algoritmo PLA puede interpretarse como SGD sobre $\textbf{e}_n$ con tasa de aprendizaje $\nu=1$.
    \item El ruido determinista depende de $\mathcal{H}$, ya que algunos modelos aproximan mejor $f$ que otros.
    \begin{enumerate}
      \item Suponer que $\mathcal{H}$ es fija y que incrementamos la complejidad de $f$.
      \item Suponer que $ f$ es fija y decrementamos la complejidad de $\mathcal{H}$
    \end{enumerate}
    Contestar para ambos escenarios: ¿En general subirá o bajará el ruido determinista? ¿La tendencia a sobrejaustar será mayor o menor? (Ayuda: analizar los detalles que influencian el sobreajuste)
    %
    \item La técnica de regularización de Tikhonov es bastante general al usar la condición
    \[
    \textbf{w}^T\mathrm{\Gamma^T\Gamma}\textbf{w}\leq C
    \]
    que define relaciones entre las $w_i$ (La matriz $\Gamma_i$ se denomina regularizador de Tikhonov)
    \begin{enumerate}
      \item Calcular $\Gamma$ cuando $\sum_{q=0}^Q w_q^2 \leq C$
      \item Calcular $\Gamma$ cuando $(\sum_{q=0}^Q w_q)^2 \leq C$
    \end{enumerate}
    Argumentar si el estudio de los regularizadores de Tikhonov puede hacerse a través de las propiedades algebraicas de las matrices $\Gamma$.
  \end{enumerate}

  \textbf{Bonus}:

  \textbf{B1}. Considerar la matriz hat $\mathrm{H}=\mathrm{X(X^TX)^{-1}X^T}$. Sea $\mathrm{X}$ una matriz  $N\times (d+1)$, y $\mathrm{X^TX}$ invertible. Mostrar que $\mathrm{traza(H)}=d+1$, donde traza significa la suma de los elementos de la diagonal principal. (+1 punto)
  %

\end{document}
