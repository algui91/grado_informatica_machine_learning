%%% LaTeX Template: Two column article
%%%
%%% Source: http://www.howtotex.com/
%%% Feel free to distribute this template, but please keep to referal to http://www.howtotex.com/ here.
%%% Date: February 2011

%%% Preamble
\documentclass[  DIV=calc,%
paper=a4,%
fontsize=11pt]{scrartcl}             % KOMA-article class

\usepackage[spanish]{babel}                    % English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}        % Better typography
\usepackage{amsmath,amsfonts,amsthm}          % Math packages
\usepackage[pdftex]{graphicx}                  % Enable pdflatex
\usepackage[svgnames]{xcolor}                  % Enabling colors by their 'svgnames'
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}  % Custom captions under/above floats
\usepackage{epstopdf}                        % Converts .eps to .pdf
\usepackage{subfig}                          % Subfigures
\usepackage{booktabs}                        % Nicer tables
\usepackage{fix-cm}                          % Custom fontsizes
\usepackage{hyperref}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[light]{kurier}

\usepackage{IEEEtrantools}

%%% Custom sectioning (sectsty package)
\usepackage{sectsty}                          % Custom sectioning (see below)
\allsectionsfont{%                              % Change font of al section commands
\usefont{T1}{mdugm}{b}{it}%                    % bch-b-n: CharterBT-Bold font
}

\sectionfont{%                                % Change font of \section command
\usefont{T1}{mdugm}{b}{it}%                    % bch-b-n: CharterBT-Bold font
}

%%% Headers and footers
\usepackage{fancyhdr}                        % Needed to define custom headers/footers
\pagestyle{fancy}                            % Enabling the custom headers/footers
\usepackage{lastpage}

% Header (empty)
\lhead{}
\chead{}
\rhead{}
% Footer (you may change this to your own needs)
\lfoot{\footnotesize \miit{Alejandro Alcalde} \textbullet ~}
\cfoot{}
\rfoot{\footnotesize Página \thepage\ de \pageref{LastPage}}  % "Page 1 of 2"
\renewcommand{\headrulewidth}{0.0pt}
\renewcommand{\footrulewidth}{0.4pt}



%%% Creating an initial of the very first character of the content
\usepackage{lettrine}
\newcommand{\initial}[1]{%
\lettrine[lines=3,lhang=0.3,nindent=0em]{
\color{DarkGoldenrod}
{\textsf{#1}}}{}}



%%% Title, author and date metadata
\usepackage{titling}                              % For custom titles

\newcommand{\HorRule}{\color{DarkGoldenrod}%      % Creating a horizontal rule
\rule{\linewidth}{1pt}%
}

\pretitle{\vspace{-30pt} \begin{flushleft} \HorRule
\fontsize{50}{50} \usefont{T1}{kurier}{l}{it} \color{DarkRed} \selectfont
}
\title{Aprendizaje Automático: Cuestionario 2}          % Title of your article goes here
\posttitle{\par\end{flushleft}\vskip 0.5em}

\preauthor{\begin{flushleft}
\large \lineskip 0.5em \usefont{T1}{mdugm}{m}{it} \color{DarkRed}}
\author{Alejandro Alcalde,}                      % Author name goes here
\postauthor{\footnotesize \usefont{OT1}{mdugm}{m}{it} \color{Black}
Universidad de Granada                 % Institution of author
\par\end{flushleft}\HorRule}

\date{\usefont{T1}{mdugm}{b}{it}\selectfont\today}                                        % No date

\newcommand{\miit}[1]{{\textbf{\textit{#1}}}}

\usepackage{pgf,tikz}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usetikzlibrary{babel}

%%% Begin document
\begin{document}
  \maketitle
  \thispagestyle{fancy}       % Enabling the custom headers/footers for the first page
  % The first character should be within \initial{}

  \centerline {\textbf{Todas las preguntas tienen el mismo valor}}
  \vspace{5pt}
  \begin{enumerate}

    \item Sean $\textbf{x}$ e $\textbf{y}$ dos vectores de observaciones de tamaño $N$. Sea
    \[
    \mathrm{cov}(\textbf{x},\textbf{y})=\frac{1}{N}\sum_{i=1}^N (x_i-\bar{x})(y_i-\bar{y})
    \]
    la covarianza de dichos vectores, donde $\bar{z}$ representa el valor medio de los elementos de $\textbf{z}$. Considere ahora una matriz $\mathrm{X}$ cuyas columnas representan vectores de observaciones. La matriz de covarianzas asociada a la matriz $\mathrm{X}$ es el conjunto de covarianzas definidas por cada dos de sus vectores columnas. Defina la expresión matricial que expresa la matriz $\mathrm{cov}(\mathrm{X})$ en función de la matriz $\mathrm{X}$

    \miit{Respuesta:}

    Como dice el enunciado, vamos a tener una matriz $\mathrm{X}$ en la que cada columna representa vectores de observaciones ($\mathbf{x}_1, \mathbf{x}_2 \dots \mathbf{x}_m$), esto es:
    \[
    X =
    \begin{pmatrix}
      x_{11} & x_{12} & \cdots & x_{1m} \\
      x_{21} & x_{22} & \cdots & x_{2m} \\
      \vdots          & \vdots & \ddots & \vdots \\
      x_{N1} & x_{N2} & \cdots & x_{Nm}
    \end{pmatrix}
    \]

    A cada elemento de un vector columna debemos restarle la media de su columna, es decir:

    \[
    X - \bar{X}=
    \begin{pmatrix}
      x_{11} - \bar{x}_1   & x_{12} - \bar{x}_2   & \cdots & x_{1m} - \bar{x}_m \\
      x_{21} - \bar{x}_1   & x_{22} - \bar{x}_2   & \cdots & x_{2m} - \bar{x}_m \\
      \vdots              & \vdots               & \ddots & \vdots \\
      x_{N1} - \bar{x}_1   & x_{N2} - \bar{x}_2   & \cdots & x_{Nm} - \bar{x}_m
    \end{pmatrix}
    \]

    Para ello crearemos una matriz que contenga las medias de los vectores columnas, la primera matriz es de dimensiones $N\times N$ y la segunda es $X$:

    \[
    \bar{X} =
    \frac{1}{N}
    \begin{pmatrix}
      1   & 1   & \cdots & 1 \\
      1   & 1   & \cdots & 1 \\
      \vdots              & \vdots  & \ddots & \vdots \\
      1   & 1   & \cdots & 1
    \end{pmatrix}
    \begin{pmatrix}
      x_{11} & x_{12} & \cdots & x_{1m} \\
      x_{21} & x_{22} & \cdots & x_{2m} \\
      \vdots          & \vdots & \ddots & \vdots \\
      x_{N1} & x_{N2} & \cdots & x_{Nm}
    \end{pmatrix}
    \]

    Tras esta operación tenemos la matrix $\bar{X}$ con las medias de cada vector:

    \[
    \bar{X}=
    \begin{pmatrix}
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m \\
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m \\
      \vdots              & \vdots        & \ddots & \vdots \\
      \bar{x}_1   & \bar{x}_2   & \cdots & \bar{x}_m
    \end{pmatrix}
    \]

    Ahora ya tenemos todo lo necesario para re-escribir la Covarianza en forma matricial:

    \[
    \mathrm{cov}(X) = \frac{1}{N}\left( X - \bar{X} \right)^T\left( X - \bar{X} \right)
    \]

    Lo cual dará resultado a una matriz $m\times m$ con las covarianzas de cada dos vectores columnas:

    \[
    \mathrm{cov}(X) =
    \begin{pmatrix}
      \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_1)   & \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_2)  & \cdots   &  \mathrm{cov}(\mathbf{x}_1, \mathbf{x}_m)  \\
      \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_1)  & \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_2)  & \cdots   &  \mathrm{cov}(\mathbf{x}_2, \mathbf{x}_m) \\
      \vdots                                    & \vdots                                      & \ddots   &   \vdots                                   \\
      \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_1)   & \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_2)  & \cdots   & \mathrm{cov}(\mathbf{x}_m, \mathbf{x}_m)
    \end{pmatrix}
    \]

    \item Considerar la matriz hat definida en regresión,  $\mathrm{H}=\mathrm{X(X^TX)^{-1}X^T}$, donde $\mathrm{X}$ es una matriz  $N\times (d+1)$, y $\mathrm{X^TX}$ es invertible.
    \begin{enumerate}
      \item Mostrar que $\mathrm{H}$ es simétrica.

      \miit{Respuesta:}

        Usando las propiedades de las transpuestas comprobamos que en efecto es simétrica:
        \begin{proof}
          \begin{IEEEeqnarray*}{rCl}
            \left (X \left (X^TX\right )^{-1}X^T\right )^T & = & \left ( X^T \right )^T \left ( \left (X^TX \right )^{-1} \right )^T X^T \\
            & = & X\left ( \left ( X^T X\right )^T \right )^{-1}X^T \\
            & = & X \left ( \left ( X^T\left ( X^T \right )^T \right )^{-1} \right )X^T \\
            & = & X \left (X^TX\right )^{-1}X^T
          \end{IEEEeqnarray*}
        \end{proof}
        Se han hecho uso de las siguientes propiedadas de las transpuestas:
        \begin{itemize}
          \item $(A)^T = A$
          \item $(A_1A_2\dots A_k)^T = A_k^T\dots A_2^TA_1^T$
          \item $\left ( A^{-1}\right )^T = \left ( A^{T}\right )^{-1}$
        \end{itemize}
      \item Mostrar que $\mathrm{H^K=H}$ para cualquier entero positivo K.

      \miit{Respuesta:}

      \begin{proof}
        \begin{IEEEeqnarray*}{rCl}
          H^2 & = & X \overbrace{\left (X^TX\right )^{-1}X^T X}^\mathrm{I} \left (X^TX\right )^{-1}X^T \\
          & = & X I \left (X^TX\right )^{-1}X^T \\
          & = & X \left (X^TX\right )^{-1}X^T \qedhere
        \end{IEEEeqnarray*}
      \end{proof}
      Luego $H^2 = H$. Para $H^3$ pasaría igual, y por inducción llegamos a la conclusión $\mathrm{H^K=H}, \forall k \in \mathbb{N}$:
      \begin{IEEEeqnarray*}{rCl}
        H^3 & = & H^2 H; \\
            & = & HH \\
            & = & H^2 \\
            & = & H
      \end{IEEEeqnarray*}
      Ahora para el caso general tenemos:
      \begin{proof}
        \begin{IEEEeqnarray*}{rCl}
          H^{n+1} & = & H^n H; \\
              & = & HH \\
              & = & H \qedhere
        \end{IEEEeqnarray*}
      \end{proof}
    \end{enumerate}

    %
    \item
    Resolver el siguiente problema: Encontrar el punto $(x_0,y_0)$ sobre la línea $ax+by+d=0$ que este más cerca del punto $(x_1,y_1)$.

    \miit{Respuesta:}

    La menor distancia entre el punto y la recta sería la perpendicular:

    \definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
    \definecolor{qqqqff}{rgb}{0.,0.,1.}

    \begin{figure}[!h]
    \centering

    \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.3333333333333333cm,y=1.4166666666666667cm]
    \clip(5.,2.1) rectangle (7.5,4.5);
    \draw [domain=5.:7.5] plot(\x,{(-4.428531747021518--1.782059032545761*\x)/2.802321837438366});
    \draw (5.231756133978087,4.208332847560416) node[anchor=north west] {$(x_1, y_1)$};
    \draw (6.4546255741581,2.6003076781898455) node[anchor=north west] {$(x_0, y_0)$};
    \draw [dash pattern=on 1pt off 1pt,color=qqqqff] (5.490617799800959,3.756375060472591)-- (6.326083313959511,2.4425895954599097);
    \begin{scriptsize}
    \draw[color=black] (-0.3144863663659077,-1.6075186632169782) node {$f$};
    \draw [fill=qqqqff] (5.490617799800959,3.756375060472591) circle (2.5pt);
    \draw [fill=uuuuuu] (6.326083313959511,2.4425895954599097) circle (1.5pt);
    \end{scriptsize}
    \end{tikzpicture}
    \end{figure}

    Usaremos el método de \textit{Lagrange} para calcularlo:

    La función a minimizar es la función de la distancia
    \[
        d(x_0, y_0) = \left ( x_0 - x_1 \right )^2 + \left ( y_0 - y_1 \right )^2
    \]
    Bajo la siguiente restricción
    \[
      g(x_0, y_0) = ax_0 + by_0 + d
    \]

    La función de \textit{Lagrange} es
    \begin{IEEEeqnarray*}{rCl}
      \mathcal{L}(x_0, y_0, \lambda) & = & d(x_0, y_0) + \lambda g(x_0, y_0) \\
      & = &  \left ( x_0 - x_1 \right )^2 + \left ( y_0 - y_1 \right )^2 +
      \lambda \left( ax_0 + by_0 + d \right )
    \end{IEEEeqnarray*}

    Y para resolver el problema necesitamos resolver el siguiente sistema:

    \[
      \left\{ \frac{\partial \mathcal{L}}{\partial x_0}=0,\frac{\partial \mathcal{L}}{\partial y_0}=0,\frac{\partial \mathcal{L}}{\partial \lambda }
=0\right\}
    \]

    \begin{eqnarray}
      \left\{
      \begin{array}{c}
        2(x_0 - x_1) + \lambda a = 0 \\
        2(y_0 - y_1) + \lambda b = 0 \\
        ax_0 + by_0 + d = 0
      \end{array}
      \right.  &\Leftrightarrow & \left\{
      \begin{array}{c}
        x_0 = x_1 - \frac{\lambda a}{2}\\
        y_0 = y_1 - \frac{\lambda b}{2}\\
        a\left(x_1 - \frac{\lambda a}{2}\right) + b\left(y_1 - \frac{\lambda b}{2}\right) + d = 0
      \end{array}
      \right.
  \end{eqnarray}

  Operamos en la tercera ecuación para despejar $\lambda$:

  \begin{IEEEeqnarray*}{rCl}
    ax_1 - \frac{\lambda a^2}{2} + by_1 - \frac{\lambda b^2}{2} + d  & = & 0 \\
    2ax_1 - \lambda a^2 + 2by_1 - \lambda b^2 + 2d & = & 0 \\
    - \lambda a^2 - \lambda b^2 & = & -2ax_1 -2by_1 -2d  \\
    \lambda\left ( a^2 + b^2 \right ) & = & 2ax_1 + 2by_1 + 2d \\
    \lambda & = & \frac{2ax_1 + 2by_1 + 2d}{a^2 + b^2} \\
    \lambda & = & \frac{2 \left ( ax_1 + by_1 + d \right )}{a^2 + b^2}\\
  \end{IEEEeqnarray*}

  Con $\lambda$ despejada, la sustituimos en $x_0, y_0$:

  \begin{IEEEeqnarray*}{rCl}
     x_0 & = & x_1 - \frac{a(ax_1 + by_1 + d)}{a^2 + b^2}\\
     y_0 & = & y_1 - \frac{b(ax_1 + by_1 + d)}{a^2 + b^2}\\
  \end{IEEEeqnarray*}

  Ahora solo  resta sustituir estos valores en la función objetivo $d(x_0, y_0)$ y conseguimos así el valor mínimo:

  \begin{IEEEeqnarray*}{rCl}
     d(x_0, y_0) & = & \left( x_1 - \frac{a(ax_1 + by_1 + d)}{a^2 + b^2} -x_1 \right )^2 + \left ( y_1 - \frac{b(ax_1 + by_1 + d)}{a^2 + b^2} -y_1 \right )^2 \\
     & = & \left(\frac{a(ax_1 + by_1 + d)}{a^2 + b^2}\right )^2 + \left (\frac{b(ax_1 + by_1 + d)}{a^2 + b^2}\right )^2
  \end{IEEEeqnarray*}

    %
    \item Consideremos el problema de optimización lineal con restricciones definido por
    \[
    \begin{array}{c}
      \min_{\textbf{z}} \mathrm{\mathbf{c}^T\textbf{z}} \\
      \hbox{Sujeto a } \mathrm{A\textbf{z} \leq \mathbf{b}}
    \end{array}
    \]
    donde \textbf{c} y \textbf{b} son vectores y A es una matriz.

    \begin{enumerate}
      \item Para un conjunto de datos linealmente separable mostrar que para algún $\textbf{w}$ se debe de verificar la condición  $\mathrm{y_n\textbf{w}^T\textbf{x}_n>0 }$ para todo $\mathrm{(\textbf{x}_n,y_n)}$ del conjunto.

      \miit{Respuesta:}

      Si los datos son linealmente separables, entonces $\exists\mathbf{w}$ que divide los datos correctamente. Supuesto que ese $\mathbf{w}$ existe,
      se cumple que $\forall y_n\mathbf{w}^\mathrm{T}\mathbf{x}_n > 0$ ya que el producto de la etiqueta de un punto por su predicción $\mathbf{w}^\mathrm{T}\mathbf{x}_n$ siempre será $>0$, ya que tanto el signo de $y_n$ como el de $\mathbf{w}^\mathrm{T}\mathbf{x}_n$ es el mismo.

      \item Formular un problema de programación lineal que resuelva el problema de la búsqueda del hiperplano separador. Es decir, identifique quienes son  A, \textbf{z}, \textbf{b} y \textbf{c} para este caso.

      \miit{Respuesta:}

      Supongamos el caso de estar en un espacio 2D, en ese caso las dimensiones de los vectores estarán en $\mathrm{R}^3$.

      El vector $\mathbf{z}$ corresponde al hiperplano que queremos encotrar, $\mathbf{b}$ especifica las restricciones de dicho hiperplano. Así que
      por un lado tenemos a la matriz $A$ de puntos de tamaño $m\times3$, que deben cumplir las restricciones impuestas por $\mathbf{b}$. $\mathbf{c}$
      impone la condición de que $\mathrm{\mathbf{c}^T\textbf{z}}$ sea mínimo sujeto a $\mathrm{A\textbf{z} \leq \mathbf{b}}$.

    \end{enumerate}


    \item Probar que en el caso general de funciones con ruido se verifica que $\mathbb{E}_{\mathcal{D}}[E_{out}]= \sigma^2+\texttt{\textbf{bias}}+\texttt{\textbf{var}}$ ( ver transparencias de clase)

    \miit{Respuesta:}

    Si partimos de
    \[
      \mathbb{E}_D[(g^D(x) - (f(x) + \epsilon(x)))^2]
    \]
    y desarrollamos, llegamos a
    \begin{IEEEeqnarray*}{rCl}
        \mathbb{E}_x[\mathbb{E}_D\underbrace{(g^D(x)^2) - \tilde{g}(x)^2}_{var(x)} + \underbrace{\tilde{g}(x)^2 - 2\tilde{g}(x)f(x) + f(x)^2}_{bias(x)} \\ +
        \epsilon(x)^2 + 2f(x)\epsilon(x) - 2\tilde{g}(x)\epsilon(x) ]
    \end{IEEEeqnarray*}

    Como suponemos que la media del ruido es $0$ y la varianza $\sigma^2$, tenemos que

    \[
      \mathbb{E}_x(\epsilon(x)^2) = \sigma^2
    \]

    \[
    \mathbb{E}_x(2f(x)\epsilon(x) - 2\tilde{g}(x)\epsilon(x)) = 0
    \]

    y por tanto

    \[
    \mathbb{E}_x[\mathbb{E}_D\underbrace{(g^D(x)^2) - \tilde{g}(x)^2}_{var(x)} + \underbrace{\tilde{g}(x)^2 - 2\tilde{g}(x)f(x) + f(x)^2}_{bias(x)} + \underbrace{\epsilon(x)^2}_{\sigma^2}]
    \]

    \item  Consideremos las mismas condiciones generales del enunciado del Ejercicio.2 del apartado de Regresión de la relación de ejercicios.2.
    Considerar ahora $\sigma=0.1$ y $d=8$, ¿cual es el más pequeño tamaño muestral que resultará en un valor esperado de $E_{in}$ mayor de $0.008$?.

    \begin{IEEEeqnarray*}{rCl}
       \mathbb{E}\left [ E_{in}(\mathbf{w}_{lin}) \right ] &=& \sigma^2\left ( 1 - \frac{d+1}{N} \right ) \\
       \frac{\mathbb{E}\left [ E_{in}(\mathbf{w}_{lin}) \right ]}{\sigma^2} - 1 &=& - \frac{d+1}{N} \\
       N\left ( \frac{\mathbb{E}\left [ E_{in}(\mathbf{w}_{lin}) \right ]}{\sigma^2} - 1  \right ) &=& - d+1 \\
       N &=& \frac{-\left( d+1 \right )}{\left ( \frac{\mathbb{E}\left [ E_{in}(\mathbf{w}_{lin}) \right ]}{\sigma^2} - 1  \right )} \\
       N &=& \frac{-\left( 8+1 \right )}{\left ( \frac{0.008}{0.1^2} - 1  \right )} \\
       N &=& 45
    \end{IEEEeqnarray*}

    Y por tanto concluimos que $N > 45$, el valor más pequeño de $N$ sería pues $46$.

    %
    %
    \item En regresión logística mostrar que

    \[
    \nabla E_{in}(\textbf{w})=-\frac{1}{N}\sum_{n=1}^{N}\frac{y_n\textbf{x}_n}{1+e^{y_n \textbf{w}^T\textbf{x}_n}}= \frac{1}{N}\sum_{n=1}^{N}-y_n\textbf{x}_n\sigma(-y_n\textbf{w}^T\textbf{x}_n)
    \]
    \miit{Respuesta:}

    Sabemos que
    \[
      \sigma(x) = \frac{1}{1 + e^{-x}}
    \]
    Si partimos de
    \[
      \frac{1}{N}\sum_{n=1}^{N}-y_n\textbf{x}_n\sigma(-y_n\textbf{w}^T\textbf{x}_n)
    \]
    y aplicamos la fórmula $\sigma(x)$, es trivial llegar a que ambas expresiones son equivalentes:

    \begin{IEEEeqnarray*}{rCl}
      \frac{1}{N}\sum_{n=1}^{N}-y_n\textbf{x}_n\sigma(-y_n\textbf{w}^T\textbf{x}_n)  & = & \frac{1}{N}\sum_{n=1}^{N}-y_n\textbf{x}_n\frac{1}{1 + e^{y_n\textbf{w}^T\textbf{x}_n}} \\
      & = & \frac{1}{N}\sum_{n=1}^{N}-\frac{y_n\textbf{x}_n}{1 + e^{y_n\textbf{w}^T\textbf{x}_n}} \\
      & = & -\frac{1}{N}\sum_{n=1}^{N}\frac{y_n\textbf{x}_n}{1 + e^{y_n\textbf{w}^T\textbf{x}_n}} \\
      & = & \nabla E_{in}(\textbf{w})
    \end{IEEEeqnarray*}

    Argumentar que un ejemplo mal clasificado contribuye  al gradiente más que un ejemplo bien clasificado.

    \miit{Respuesta:}

    Cuando un punto está mal clasificado, $e^{y_n\textbf{w}^T\textbf{x}_n} \to 0$, y por tanto el resultado de la división en $\frac{y_n\textbf{x}_n}{1+e^{y_n \textbf{w}^T\textbf{x}_n}}$ contribuye más a la sumatoria y por tanto al gradiente. Por
    otro lado, cuando un punto está bien clasificado $e^{y_n\textbf{w}^T\textbf{x}_n} \to \infty$ y $\frac{y_n\textbf{x}_n}{1+e^{y_n \textbf{w}^T\textbf{x}_n}} \to 0$, aportando poco al gradiente.

    \item  Definamos el error en un punto $(\mathbf{x}_n,y_n)$ por
    \[
    \textbf{e}_n(\textbf{w})=\max(0,-y_n\textbf{w}^T\textbf{x}_n)
    \]
    Argumentar que el algoritmo PLA puede interpretarse como SGD sobre $\textbf{e}_n$ con tasa de aprendizaje $\nu=1$.

    \miit{Respuesta:}

    La regla de actualización para SGD es:
    \[
      \mathbf{w} = \mathbf{w} - \eta \nabla \mathbf{e}_n(\mathbf{w})
    \]
    y la de PLA:
    \[
      \mathbf{w}(t+1) = \mathbf{w}(t) + y(t)\mathbf{x}(t)
    \]

    El gradiente para la función de error dada es:
    \[
      \nabla\mathbf{e}_n(\mathbf{w}) = \max(0, -y_n \mathbf{x}_n)
    \]

    PLA escoge un punto mal clasificado, lo cual quiere decir $y(t) \neq sign(\mathbf{w}^T(t)\mathbf{x}(t))$ y se le aplica la regla de actualización.
    Para ese mismo punto mal clasificado en SGD con la función de error tenemos (No ponemos $\eta$ ya que vale $1$):

    \begin{IEEEeqnarray*}{rCl}
       \mathbf{w} & = & \mathbf{w} - \nabla \mathbf{e}_n(\mathbf{w}) \\
                  & = & \mathbf{w} - \max(0, -y_n \mathbf{x}_n) \\
                  & = & \mathbf{w} - y_n \mathbf{x}_n
    \end{IEEEeqnarray*}

    Al estar el punto mal clasificado $-y_n\textbf{w}^T\textbf{x}_n > 0 $ y por tanto se escoge ese valor en lugar del $0$.


    \item El ruido determinista depende de $\mathcal{H}$, ya que algunos modelos aproximan mejor $f$ que otros.
    \begin{enumerate}
      \item Suponer que $\mathcal{H}$ es fija y que incrementamos la complejidad de $f$.
      \item Suponer que $ f$ es fija y decrementamos la complejidad de $\mathcal{H}$
    \end{enumerate}
    Contestar para ambos escenarios: ¿En general subirá o bajará el ruido determinista? ¿La tendencia a sobrejaustar será mayor o menor? (Ayuda: analizar los detalles que influencian el sobreajuste)

    \miit{Respuesta:}

    Por lo general, el comportamiento para ambos casos será el mismo. Si fijamos $\mathcal{H}$ e incrementamos la complejidad de $f$, el ruido
    determinista tenderá a aumentar,  ya que habrá más partes de $f$ que $\mathcal{H}$ no será capaz de capturar. Por contra, el sobre ajuste tenderá
    a decrementar por razones similares, si $\mathcal{H}$ mantiene la misma complejidad, pero $f$ la aumenta, el sobre ajuste será menor ya que
    $\mathcal{H}$ no será capaz de “pegarse” tanto a $f$.

    Para el segundo caso, al fijar $f$ y decrementar $\mathcal{H}$, estamos haciendo algo similar, ya que al decrementar la complejidad de
    $\mathcal{H}$ y mantener la de $f$, las nuevas $\mathcal{H}$ no serán capaces de ajustarse tan bien como las de mayor complejidad, y por tanto
    aumentará el ruido determinista. Para el sobre ajuste, al bajar la complejidad $\mathcal{H}$ no se “pegará” tanto a los puntos de $f$, y por tanto
    decrementará.

    %
    \item La técnica de regularización de Tikhonov es bastante general al usar la condición
    \[
      \textbf{w}^T\mathrm{\Gamma^T\Gamma}\textbf{w}\leq C
    \]
    que define relaciones entre las $w_i$ (La matriz $\Gamma_i$ se denomina regularizador de Tikhonov)
    \begin{enumerate}
      \item Calcular $\Gamma$ cuando $\sum_{q=0}^Q w_q^2 \leq C$

      \miit{Respuesta:}

      Para conseguir ese valor, es necesario que $\mathrm{\Gamma^T\Gamma = I}$, este tipo de matrices son ortogonales, ya que para ellas se cumple
      $\mathrm{\Gamma^T\Gamma = \Gamma\Gamma^T = I}$. Teniendo $\mathrm{\Gamma^T\Gamma = I}$ se cumpliría $\sum_{q=0}^Q w_q^2 \leq C$.

      \item Calcular $\Gamma$ cuando $(\sum_{q=0}^Q w_q)^2 \leq C$

      \miit{Respuesta:}

      En este caso necesitamos que $\mathrm{\Gamma^T\Gamma}$ sea una matriz de todo unos. Para conseguirlo, $\mathrm{\Gamma}$ debe ser Suponiendo dimensión $2\times2$:
      \[
      \mathrm{\Gamma} =
        \begin{pmatrix}
          \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
          \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
        \end{pmatrix}
      \]

      De este modo $\mathrm{\Gamma^T\Gamma}$ es
      \[
      \begin{pmatrix}
        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
      \end{pmatrix}
      \begin{pmatrix}
        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
        \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
      \end{pmatrix} =
      \begin{pmatrix}
        1 & 1\\
        1 & 1
      \end{pmatrix}
      \]
      y al hacer la operación

      \begin{IEEEeqnarray*}{rCl}
        \begin{pmatrix}
          \mathbf{w}_0 & \mathbf{w}_1
        \end{pmatrix}
        \begin{pmatrix}
          1 & 1\\
          1 & 1
        \end{pmatrix}
        \begin{pmatrix}
          \mathbf{w}_0 \\
          \mathbf{w}_1
        \end{pmatrix}
          & = &
        \left(
          \mathbf{w}_0 + \mathbf{w}_1
        \right)^2
      \end{IEEEeqnarray*}

      Que generalizada sería
      \[
        (\sum_{q=0}^Q w_q)^2 \leq C
      \]
    \end{enumerate}
    Argumentar si el estudio de los regularizadores de Tikhonov puede hacerse a través de las propiedades algebraicas de las matrices $\Gamma$.
  \end{enumerate}

  \textbf{Bonus}:

  \textbf{B1}. Considerar la matriz hat $\mathrm{H}=\mathrm{X(X^TX)^{-1}X^T}$. Sea $\mathrm{X}$ una matriz  $N\times (d+1)$, y $\mathrm{X^TX}$ invertible. Mostrar que $\mathrm{traza(H)}=d+1$, donde traza significa la suma de los elementos de la diagonal principal. (+1 punto)
  %

\end{document}
