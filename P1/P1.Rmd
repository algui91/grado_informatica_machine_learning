---
title: "Aprendizaje Automático - Práctica 1"
author: "Alejandro Alcalde"
date: "March 3, 2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: zenburn
    # keep_tex: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# library(parallel)
# Calculate the number of cores
# cores.number <- detectCores()
# Initialize seed
# clusterSetRNGStream(cl, iseed = 1000000007)
set.seed(1000000007)
```

# Funciones de utilidad

```{r utilidades gráficas}
MyPlot <- function(data, labels = NULL, myMain = NULL, pch = 20, xlab = "x",
                   ylab = "y", ...) {
  # Wrapper function for plotting
  #
  # Args:
  #   Data: A data structure containing the data in the form x,y
  #   labels: Labels to color the data points
  #   rest of params: Graphical params to plot
  if (length(labels) != 0) {
    labels[labels == 1] <- "#1A1314"
    labels[labels == -1] <- "#D2372B"
  } else {
    labels <- "#1A1314"
  }

  if (is.null(dim(labels))) {
    plot(data,
         col = labels,
         pch = pch,
         xlab = xlab,
         ylab = ylab,
         ...)
  } else {
    for (i in seq(1, dim(labels)[2])) {
      plot(data,
           col = unlist(labels[i]),
           main = myMain[i],
           pch = pch,
           xlab = xlab,
           ylab = ylab,
           ...)
      cat("\r\n\r\n") # Fix for pdf generation
    }
  }
}

DrawBoundary <- function(w, color = '#525F7F', ...) {
  # Draw the line defined by the weight vector w.
  #
  # Thanks to http://stackoverflow.com/a/19069020/1612432 for the explanation:
  # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision
  # boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
  # 
  # |w3|/||w|| is the distance from the origin, w3 itself does not have a good
  # geometrical interpretation (as long as w is not unit-length).
  # 
  # In order to plot line with such equation you can simply draw a line through
  # (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
  # 
  # Args:
  #   w: The weight vector
  #   color: The color of the line to draw
  w <- as.numeric(w)
  
  b <- w[1]
  w1 <- w[2]
  w2 <- w[3]
  
  slope <- -(w1 / w2)
  intercept <- -(b / w2)
  
  abline(a   = intercept,
         b   = slope,
         col = color,
         ...)
}
```

# Ejercicio de Generación y Visualización de datos

## Ejercicio 1

Construir una función _lista <- SimulaUnif (N, dim, rango)_ que calcule una 
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios uniformes en el intervalo _rango_.

```{r Exercise 1}
SimulaUnif <- function(N, dimen = 1, rango = c(0:1)) {
  # Generate a list of vectors populated with uniform distributed random numbers
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   rango: Range to use generating the numbers
  #
  # Returns:
  #   A list of vectors populated randomly
  array(runif(dimen*N, rango[1], rango[2]),
        dim = c(N, dimen))
}

SimulaUnif(10, 5, c(10,100))
SimulaUnif(10, 2)
SimulaUnif(10)
```

## Ejercicio 2

Construir una función _lista <- simula_gaus(N, dim, sigma)_ que calcule una
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios gaussianos de media 0 y varianza dadas por el vector de _sigma_.

```{r Exercise 2}
SimulaGauss <- function(N, dimen = 1, sig = 1){
  # Generate a list of vectors populated using the normal (Gaussian)
  # distribution
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   sigma: Vector of Standard deviations
  #
  # Returns:
  #   A list of vectors populated randomly
  array(rnorm(dimen*N, sd = sqrt(sig)),
        dim = c(N, dimen))
}

SimulaGauss(10, 5, c(1,2))
```

## Ejercicio 3

Suponer _N=50, dim=2, rango=[-50,+50]_ en cada dimensión. Dibujar una gráfica
de la salida de la función correspondiente.

```{r Exercise 3}
datos <- SimulaUnif(50, 2, c(-50,50))
print(datos)
MyPlot(datos,
       main = "Gráfica datos de SimulaUnif(50, 2, c(-50,50))")
```

## Ejercicio 4

Suponer _N=50, dim=2, $\sigma=[5,7]$_ dibujar una gráfica de la salida de la
función correspondiente.

```{r Exercise 4}
datos <- SimulaGauss(50, 2, c(5,7))
print(datos)
MyPlot(datos,
       main = "Gráfica datos de  SimulaGauss(50, 2, c(5,7))",
       type = "b")
```

## Ejercicio 5

Construir la función _v <- SimulaRecta(intervalo)_ que calcula los parámetros _v <- (a,b)_ de una recta aleatoria, _y <- ax + b_, que corte al cuadrado _[-50, 50] x [-50, 50]_.

```{r Exercise 5}

SimulaRecta <- function(interval = c(-50, 50)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  <- SimulaUnif(2, 2, interval)
  
  # Compute the slope
  m <- (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 <- m(x − x1)
  line <- m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  c(m, line, thePoints)
}

interval <- c(-50, 50);
x <- SimulaUnif(5000, 2, interval);

result <- SimulaRecta(interval)
m <- result[1]
print(m)
line <- result[2]
print(line)
data <- array(result[3:6], c(2,2))
print(data)
```

## Ejercicio 6

Generar una muestra 2D de puntos usando _SimulaUnif_ y etiquetar la muestra usando el signo de la función $f(x,y) <- y - ax - b$ de cada punto a una recta simulada con _SimulaRecta_. Mostrar una gráfica con el resultado de la muestra etiquetada junto con la recta usada para ello.

```{r Exercise 6}
interval <- c(-100, 100)
# Generate a set of points
x <- SimulaUnif(500, 2, interval)

fsign <- function(x, y, m, b){
  # Function to label a 2D Point
  y - m * x - b
}

straight <- SimulaRecta(interval)
m <- straight[1]
line <- straight[2]

# Apply fsign to each point of the set, each x,y will be passed to fsign
result <- mapply(fsign, x[,1], x[,2],
                 m = m,
                 b = line)

# Label the result with -1/1
result <- ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
data.fsign <- data.frame(x      = x[,1],
                         y      = x[,2],
                         result = result)

# Draw the result
MyPlot(data.fsign[1:2],
       data.fsign$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse = " "),
       sub  = paste(c("Interval <- ", interval), collapse = " "),
       ylab = "")
abline(line,m)
```

## Ejercicio 7

Usar la muestra generada en el apartado anterior y etiquetarla con +1, -1 usando el signo de cada una de las siguientes funciones.

- $f(x, y) <- (x - 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x + 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x - 10)^2 - (y + 20)^2 - 400$
- $f(x, y) <- y - 20x^2 - 5x + 3$

Visualizar el resultado del etiquetado de cada función junto con su gráfica y comparar el resultado con el caso lineal. ¿Qué consecuencias extrae sobre las regiones positiva y negativa?

```{r Exercise 7}
# Define the functions
F1 <- function(x, y) {
  (x - 10) ^ 2 + (y - 20) ^ 2 - 400
}
F2 <- function(x, y){
  0.5 * (x + 10) ^ 2 + (y - 20) ^ 2 - 400
}
F3 <- function(x, y){
  0.5 * (x - 10) ^ 2 - (y + 20) ^ 2 - 400
}
F4 <- function(x, y){
  y - 20 * x ^ 2 - 5 * x + 3
}

# Generate a vector of functions
MultiFun <- function(x, y) {
      c(F1 = F1(x, y), 
        F2 = F2(x, y),
        F3 = F3(x, y),
        F4 = F4(x, y))
}

# Apply the functions
result <- mapply(MultiFun, x[,1], x[,2])

# Label the results
result <- ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
data.functions <- data.frame(x  = x[,1],
                             y  = x[,2],
                             F1 = result[1,],
                             F2 = result[2,],
                             F3 = result[3,],
                             F4 = result[4,])

# Draw the result
MyPlot(data.functions[1:2], data.functions[3:6], c("F1", "F2", "F3", "F4"))
```

Si observamos el caso lineal, vemos que las muestras son separables por una 
línea recta. Esto se debe a que la función que generó el etiquetado
($f(x,y) <- y - ax - b$) es lineal.

Las cuatro funciones usadas para el etiquetado en esta ocasión, son
__no lineales__, lo cual implica que los datos etiquetados no serán separables
por una simple recta. Se necesita una función más compleja que pueda realizar el
particionamiento de los datos positivos y negativos.

## Ejercicio 8

Considerar de nuevo la muestra etiquetada en el ejercicio 6. Modifique las
etiquetas de un 10% aleatorio de muestras positivas y otro 10% aleatorio de
negativas.

- Visualice los puntos con las nuevas etiquetas y la recta del ejercicio 6
- En una gráfica a parte visualice de nuevo los mismos puntos pero junto con
las funciones del ejercicio 7.

Observe las gráficas y diga qué consecuencias extrae del proceso de modificación
de etiquetas en el proceso de aprendizaje.

Para el primer apartado, cambiamos el 10% de las muestras positivas y negativas
y obtenemos:

```{r Exercise 8.a}
GetPercentageOfData <- function(x, condition = 1, percentage = .1){
  # Get the percentage of samples that meet condition
  #
  # Args:
  #   x: A vector containing the data
  #   condition: Condition that the data need to satisfy
  #   percentaje: What percentage of samples to get
  #
  # Returns:
  #   Indexes of the percentage of the samples that meet the condition
  meetCondition <- which(x == condition)
  sample(meetCondition, length(meetCondition) * percentage)
}

# Get a 10% of samples labeled with a 1
positive.index <- GetPercentageOfData(data.fsign$result)
data.fsign.noisy <- data.fsign
data.fsign.noisy$result[positive.index] <- -1

negative.index <- GetPercentageOfData(data.fsign$result, -1)
data.fsign.noisy$result[negative.index] <- 1

# Draw the result
MyPlot(data.fsign.noisy[1:2],
       data.fsign.noisy$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse <- " "),
       sub  = paste(c("Interval <- ", interval), collapse <- " "),
       ylab = "")
abline(line,m)
```

La única observación que puede hacerse, es que ahora los datos ya no son separables y estamos introduciendo ruido a la muestra.

```{r Exercise 8.b}
# Initiate cluster
# cl <- makeCluster(cores.number, type = "FORK")
# Get a 10% of samples labeled with a 1 in all 4 functions
# system.time(positive.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData))
positive.index <- lapply(data.functions[3:6], GetPercentageOfData)

data.functions.noisy <- data.functions
# Change 1 by -1
data.functions.noisy$F1[positive.index$F1] <- -1
data.functions.noisy$F2[positive.index$F2] <- -1
data.functions.noisy$F3[positive.index$F3] <- -1
data.functions.noisy$F4[positive.index$F4] <- -1

# Get a 10% of samples labeled with a 1 in all 4 functions
# negative.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData, condition = -1)
negative.index <- lapply(data.functions[3:6], GetPercentageOfData, condition = -1)

# Free memory of the cluster
# stopCluster(cl)

# Change -1 by 1
data.functions.noisy$F1[negative.index$F1] <- 1
data.functions.noisy$F2[negative.index$F2] <- 1
data.functions.noisy$F3[negative.index$F3] <- 1
data.functions.noisy$F4[negative.index$F4] <- 1

# Draw the result
# par(mfrow <- c(2,2))
MyPlot(data.functions.noisy[1:2], data.functions.noisy[3:6],
       c("F1", "F2", "F3", "F4"))
```

En esta ocasión, al igual que en la anterior, al cambiar deliberadamente el
valor de las etiquetas por su clase contraria, estamos introduciendo mucho
ruido en los datos. Como en el caso anterior, los datos dejan de ser separables
y difícilmente se aprenderá algo de este conjunto de datos, ya que las dos
clases estás distribuidas uniformemente por todo el espacio.

# Ejercicio de Ajuste del Algoritmo Perceptron

## Ejercicio 1

Implementar la función _sol <- ajusta\_PLA(datos, label, max_iter, vini)_ que
calcula el hiperplano solución a un problema de clasificación binaria usando el
algoritmo PLA. La entrada _datos_ es una matriz donde cada item con su etiqueta
está representado por una fila de la matriz, _label_ el vector de etiquetas
(cada etiqueta es un valor +1 o -1), _max\_iter_ es el número máximo de
iteraciones permitidas y _vini_ el valor inicial del vector. La salida _sol_
devuelve los coeficientes del hiperplano. 

```{r Exercise 2.1}
AjustaPla <- function(data,
                      label,
                      max_iter       = 1000,
                      vini           = matrix(0, 1, 3),
                      visual         = F,
                      show.last.iter = F,
                      memory         = F) {
  # Get the weight to learn from the data passed as parameter
  # using the 2D Perceptron algorithm
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   label: The labels for the data passed in
  #   max_iter: How much iter to find a solution
  #   vini: Initial weights
  #   visual: Draw each hyperplane found
  #   show.last.iter: Draw the last hyperplane found, even when the algorithm
  #     does not converge
  #   memory: Tells the algorithm if it has to store the best result in each
  #     iteration in the case of not converging
  #
  # Returns:
  #   The weights learned to adjust the data set and how many iterations it 
  #   did in a data frame.
  #   If the PLA did not converged in within max_iter, the currents weights
  #   are returned and the iterations are set to -1
  
  GetSign <- function(values){
    # Return the sign for the values
    #
    # Args:
    #   values: The values to get the sign of.
    #
    # Returns:
    #   The sign of the values
    return(ifelse(values >= 0, 1, -1))
  }
  
  PLADrawBoundary <- function(w, color = '#525F7F', ...) {
    # Draw the line defined by the weight vector w.
    #
    # Thanks to http://stackoverflow.com/a/19069020/1612432 for the explanation:
    # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision
    # boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
    # 
    # |w3|/||w|| is the distance from the origin, w3 itself does not have a good
    # geometrical interpretation (as long as w is not unit-length).
    # 
    # In order to plot line with such equation you can simply draw a line through
    # (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
    # 
    # Args:
    #   w: The weight vector
    #   color: The color of the line to draw
    w <- as.numeric(w)
    
    b <- w[1]
    w1 <- w[2]
    w2 <- w[3]
    
    slope <- -(w1 / w2)
    intercept <- -(b / w2)
    
    abline(a   = intercept,
           b   = slope,
           col = color,
           ...)
  }
  
  converged <- F
  alpha <- 50
  
  if (visual == T) {
    max.item = max(data)
    x.lim = max.item + max.item * .15
    MyPlot(data, label, xlim = c(-x.lim, x.lim), ylim = c(-x.lim, x.lim))
    # Fix for rmd pdf generation, this put every plot below each other, no overflows
    cat('\r\n\r\n')
  }
  
  # Add x0
  data <- cbind(rep(1, nrow(data)), data)
  
  # Convert weights as matrix
  as.matrix(vini)
  
  # Begin the perceptron
  nIters <- 0
  
  vini.best <- vini
  e.in.best <- nrow(data)
  
  while (!converged && nIters <= max_iter) {
    # while there are mis-classifications
    
    nIters <- nIters + 1
    
    # Calculate h(x) with the weight vector vini and the data input data
    h.function <- GetSign(vini %*% t(data))
    
    # Calculate the misclassified mask
    misclassified.subseting <- h.function != label

    if (all(misclassified.subseting == F)) {
      converged <- T
    } else {
      # Update the weight vector for a point randomly selected
      
      # Get the misclassified points out
      misclassified.points <- data[misclassified.subseting, , drop = F]
      misclassified.points.labels <- label[misclassified.subseting]
      
      # Get one of them
      misclassified.point.index <- sample(dim(misclassified.points)[1], 1)
      misclassified.point <- misclassified.points[misclassified.point.index,
                                                  , drop = F]
      misclassified.point.label <-
        misclassified.points.labels[misclassified.point.index]
      
      # Always store the best of all results
      if (memory == T && !all(vini == 0)) {
        e.in.current <- sum(misclassified.subseting)
        if (e.in.current < e.in.best) {
          e.in.best <- e.in.current
          vini.best <- vini
        }
      }
      
      # update the weights
      vini <- vini + misclassified.point.label %*% misclassified.point
      
      
      if (visual == T) {
        if (alpha >= 128) {
          alpha <- 10
        }
        PLADrawBoundary(vini, rgb(82/255,95/255,127/255, alpha/255))
        alpha <- alpha + 1
      }
      
    }
  }

  if (!converged) {
    nIters <- -1
    
    # Return the best solution found in all iterations, not just the last one
    if (memory == T && !all(vini == 0)) {
      vini <- vini.best
    }
    
    if (show.last.iter == T && visual == T) {
      PLADrawBoundary(vini, "#D2372B", lwd = 3)
      legend('bottomright',
           c("Last Iter", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#D2372B","#525F7F"))
    }
    
  } else if (visual == T) {
    PLADrawBoundary(vini, "#F9CE0C", lwd = 3)
    legend('bottomright',
           c("Final", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#F9CE0C","#525F7F"))
  }
  
  # return as a unit vector
  vini <- vini / sqrt(sum(vini * vini))
  
  data.frame(vini = vini,
             iter = nIters)
}

LabelData <- function(p, line = NULL){
  # Returns the corresponding label to the data, giving +1/-1 depending on
  # which side of the line the point lies
  #
  # Args:
  #   p: The points to label
  #   line: If we want to give a line instead of generate one randomly
  #
  # Returns:
  #   A list with the labels (+1/-1) and the line
  
  if (is.null(line)) {
    # Initialize a random plane to separate the -1, +1
    line <- matrix(runif(4, -1, 1), 2, 2)
  }
  
  # Given two points, determine if a point from the data set lies on the -1
  # side or the 1 side.
  # The points are A, B, the query points (X,Y)
  # The equation is (Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax)
  values <- (line[2,1] - line[1,1]) * (p[,2] - line[1,2]) -
    (line[2,2] - line[1,2]) * (p[,1] - line[1,1])
  
  res = list(labels = ifelse(values > 0, 1, -1), 
             line = line)
  
  return(res)
}

# Initialize N random points, and Y
n <- 500
x <- matrix(runif(n*2, -1, 1), n, 2)
y <- LabelData(x)

# Run perceptron algorithm
w <- AjustaPla(x, y$labels, visual = T)

ShowPlaResult <- function(w, x, y){
  iter <- w[4]
  w <- as.double(w[-4])
  
  if (iter != -1) {
    ## PLA converged
    
    # Plot the points according to its actual class
    MyPlot(x, y, main = paste("Perceptron training against N <- ", n))
    
    # Plot the learned boundary
    # DrawBoundary(w)
    # legend('bottomright', c("Exact", "Learned"), lty=c(1,1), lwd=c(2.5,2.5),col=c("green","red"))
  } else {
    cat("PLA did not converge")
  }
}

# ShowPlaResult(w, x, y)
```

## Ejercicio 2

Ejecutar el algoritmo PLA con los valores simulados en el apartado 6,
inicializando el algoritmo con el vector cero y con vectores de números
aleatorios en [0,1], (10 veces). 

Anotar el número medio de iteraciones necesarias en ambos para converger.
Valorar el resultado.

```{r Exercise 2.2}

perceptron.data <- as.matrix(data.fsign[1:2])
y <- LabelData(perceptron.data)$labels

perceptron.result.10times.weights.zero <- replicate(100, {
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y)
  perceptron.result[4]
})

perceptron.result.10times.weights.random <- replicate(100, {
  
  weights <- matrix(runif(3, -1, 1), 1,3)
  
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y,
                                 vini  = weights)
  perceptron.result[4]
})

cat("The mean iterations of execute PLA 100 times with a vini of 0 is",
    as.character(mean(as.numeric(perceptron.result.10times.weights.zero))),
    "\nIts sd is ", 
    as.character(sd(as.numeric(perceptron.result.10times.weights.zero))))

cat("The mean iterations of execute PLA 100 times with a random vini",
    as.character(mean(as.numeric(perceptron.result.10times.weights.random))),
    "\nIts sd is ", 
    as.character(sd(as.numeric(perceptron.result.10times.weights.random))))
```

Con 1000 iteraciones como máximo, y ejecutándolo 100 veces, no se puede decir 
nada sobre si el vector inicial afecta al comportamiento del algoritmo. Como 
se aprecia en la salida, tanto la media como la desviación estándar del número de 
iteraciones necesarias para converger es similar, ya se inicialize el vector
a cero o de forma aleatoria.

## Ejercicio 3

Ejecutar el algoritmo PLA con los datos generados en el apartado 8 del
ejercicio 4.2 usando valores de 10, 100 y 1000 para _max_iter_. Etiquetar los
datos de la muestra usando la función solución encontrada y contar el número de
errores respecto de las etiquetas originales. Valorar el resultado.

```{r Exercise 2.3}
# Show the noisy data
MyPlot(data   = data.fsign.noisy[1:2],
       labels = data.fsign.noisy[3])

perceptron.data.noisy <- as.matrix(data.fsign.noisy[1:2])

#  Parallel version
# mcmapply(AjustaPla, 
#                       max_iter = c(100, 1000, 10000),
#                       MoreArgs = list(data   = perceptron.data.noisy,
#                                       label  = data.fsign.noisy[3],
#                                       visual = T),
#          mc.cores = detectCores())

# Launch the algorithm three times, with different values for max_iter
# and draw the result of the last hyperplane calculated
ws <- mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data           = perceptron.data.noisy,
                             label          = data.fsign.noisy[3],
                             visual         = T,
                             show.last.iter = T))
ws <- ws[-4,]
    
w.10.iters <- ws[,1]
w.100.iters <- ws[,2]
w.1000.iters <- ws[,3]

GetLabelsFromWeight <- function(w, data) {
  # Get the labels asociated with the hyperplane passed in as parameter.
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   w: Weights from which decide if a point lies on one side or another
  #
  # Returns:
  #   A vector with labels for the data
  w <- as.numeric(w)
  
  b <- w[1]
  w1 <- w[2]
  w2 <- w[3]

  slope <- -(w1 / w2)
  intercept <- -(b / w2)
  
  ifelse(slope * data[,1] + intercept - data[,2] >= 0, 1, -1)
}

# this are where the points divided by the perceptron lies
# TODO, replace with sign
error.10.iters <- sum(GetLabelsFromWeight(w.10.iters, perceptron.data.noisy) != data.fsign.noisy[3])
error.100.iters <- sum(GetLabelsFromWeight(w.100.iters, perceptron.data.noisy) != data.fsign.noisy[3])
error.1000.iters <- sum(GetLabelsFromWeight(w.1000.iters, perceptron.data.noisy) != data.fsign.noisy[3])

cat("Missclassified points with 10 iters: ", error.10.iters)
cat("Missclassified points with 100 iters: ", error.100.iters)
cat("Missclassified points with 1000 iters: ", error.1000.iters)

# user.input <- readline("Press <return> to continue to the next exercise") 
```

En esta ocasión, al haber introducido en uno de los ejercicios ruido en los
datos, el algoritmo perceptrón no va a converger nunca, ya que los datos no
son separables.

Como coclusión podemos resaltar que no importa el número
de iteraciones que haga el algoritmo, ya que al no tener memoria, en algunas 
ejecuciones el número de datos mal clasificados puede ser muy bajo para 10 
iteraciones y muy alto para 1000 iteraciones, y en otras ocasiones al contrario.

## Ejercicio 4

Repetir el análisis del punto anterior usando la primera función del ejercicio 7.

```{r Exercise 2.4}
MyPlot(data   = data.functions[1:2],
       labels = data.functions$F1,
       main   = "The data labeles with the function F1")

perceptron.data <- as.matrix(data.functions[1:2])

ws <- mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data           = perceptron.data,
                             label          = data.functions$F1,
                             visual         = T,
                             show.last.iter = T))
ws <- ws[-4,]
w.10.iters <- ws[,1]
w.100.iters <- ws[,2]
w.1000.iters <- ws[,3]

error.10.iters <- sum(GetLabelsFromWeight(w.10.iters, perceptron.data) != data.functions$F1)
error.100.iters <- sum(GetLabelsFromWeight(w.100.iters, perceptron.data) != data.functions$F1)
error.1000.iters <- sum(GetLabelsFromWeight(w.1000.iters, perceptron.data) != data.functions$F1)

cat("Missclassified points with 10 iters: ", error.10.iters)
cat("Missclassified points with 100 iters: ", error.100.iters)
cat("Missclassified points with 1000 iters: ", error.1000.iters)

# user.input <- readline("Press <return> to continue to the next exercise") 
```

En el ejercicio 7 ya se vio que los datos etiquetados usando la función F1 no 
son linealmente separables. Nos encontramos por tanto ante el mismo problema
del ejercicio anterior, el Perceptrón no será capaz de dividir los datos nunca.

## Ejercicio 5

Modique la función `AjustaPla` para que le permita visualizar los datos y
soluciones que va encontrando a lo largo de las iteraciones. Ejecute con la nueva
versión el ejercicio 3.

Este ejercicio se hizo sin querer al implementar el algoritmo en el ejercicio 1
con motivo de tener una ayuda visual y de esta forma ver que se ejecutaba
correctamente. Se explica a continuación el significado de los parámetros de
`AjustaPla`:

```{r exercise 2.5, eval=F}
AjustaPla <- function(data,
                      label,
                      max_iter       = 1000,
                      vini           = matrix(0, 1, 3),
                      visual         = F,
                      show.last.iter = F)
```

Los parámetros relevantes para la visualización son `visual` y `show.last.iter`.
Si el primero es verdadero, se mostrará una gráfica con todos los hiperplanos
calculados en cada iteración, y en caso de converger, se mostrará el correcto 
de otro color. Cuando el algoritmo no converge, pero aún así queremos ver cual
fue el último hiperplano encontrado, hay que establecer a verdadero el segundo
parámetro.

Ejecutemos ahora el ejercicio 3 de forma visual:

```{r exercise 2.5b}
mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data           = perceptron.data.noisy,
                             label          = data.fsign.noisy[3],
                             visual         = T,
                             show.last.iter = T))
```

Como se aprecia en las gráficas, se agotan todas las iteraciones antes de encontrar
una solución, lo cual es lógico al no ser los datos separables. Debido al
funcionamiento del perceptrón, que no tiene memoria, el hiperplano que escoja
será el que calcule en la última iteración, y no el mejor encontrado hasta el 
momento. El no tener memoria junto a la propiedad estocástica propia del algoritmo, 
hacen que en distintas ejecuciones se obtengan muy pocos datos mal clasificados,
mientras que en otras existan muchos datos mal clasificados.

## Ejercicio 6

A la vista de la conducta de las soluciones observada en el apartado anterior, 
proponga e implemente una modicación de la función original _sol = AjustaPLA\_MOD_
que permita obtener soluciones razonables sobre datos no linealmente separables. Mostrar
y valorar el resultado encontrado usando los datos del ejercicio 7.

Para mejorar el algoritmo se ha realizado una pequeña modificación sobre el
anterior, ahora, se recuerda cual es el hiperplano que clasificaba incorrectamente
el menor número de puntos, de este modo, si llegamos a la última iteración sin 
haber encontrado solución, se devolverá el mejor encontrado. La modificación
es la siguiente:

```{r eval = F}
best.weight <- vini
best.e.in <- nrow(data)

current.e.in <- sum(misclassified.subseting)

if (current.e.in < best.e.in) {
  best.e.in <- current.e.in
  best.weight <- vini
}
```

Veamos primero cómo mejora la solución, para ello, ejecutaremos el algoritmo
con los mismos números aleatorios dos veces, una con esta mejora, y otra sin ella

```{r}
MyPlot(perceptron.data, data.fsign[3])
set.seed(1000000007)
w.no.memory <- AjustaPla(data           = perceptron.data.noisy,
                         label          = data.fsign.noisy[3],
                         visual         = T,
                         show.last.iter = T)
set.seed(1000000007)
w.memory <- AjustaPla(data           = perceptron.data.noisy,
                      label          = data.fsign.noisy[3],
                      visual         = T,
                      show.last.iter = T,
                      memory         = T)
```

Como vemos, la línea encontrada es distinta, así como el número de errores cometidos:

```{r}
sum(GetLabelsFromWeight(w.memory[-4], perceptron.data.noisy) != data.fsign.noisy[3])
sum(GetLabelsFromWeight(w.no.memory[-4], perceptron.data.noisy) != data.fsign.noisy[3])
```

Ahora veamos la mejora con respecto a los datos del ejercicio 7:

```{r exercise 2.6}
perceptron.data <- as.matrix(data.functions[1:2])
ws <- mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data           = perceptron.data,
                             label          = data.functions$F1,
                             visual         = T,
                             show.last.iter = T,
                             memory         = T))
ws <- ws[-4,]
w.10.iters <- ws[,1]
w.100.iters <- ws[,2]
w.1000.iters <- ws[,3]

error.10.iters <- sum(GetLabelsFromWeight(w.10.iters, perceptron.data) != data.functions$F1)
error.100.iters <- sum(GetLabelsFromWeight(w.100.iters, perceptron.data) != data.functions$F1)
error.1000.iters <- sum(GetLabelsFromWeight(w.1000.iters, perceptron.data) != data.functions$F1)

cat("Missclassified points with 10 iters: ", error.10.iters)
cat("Missclassified points with 100 iters: ", error.100.iters)
cat("Missclassified points with 1000 iters: ", error.1000.iters)

# user.input <- readline("Press <return> to continue to the next exercise") 
```

Como vemos, hay una mejora substancial con respecto al número de errores que 
comete el algoritmo perceptrón sin memoria, y esta última implementación. Al 
guardar la mejor solución encontrada hasta el momento, nos aseguramos obtener
un mejor resultado cuando los datos no son separables.

```{r reset env}
# tmp <- ls()
# tmp <- tmp[tmp != "MyPlot" &
#            tmp != "DrawBoundary" &
#            tmp != "SimulaUnif" &
#            tmp != "LabelData"]
# rm(list = tmp)
```


# Ejercicios sobre Regresión Lineal

## Ejercicio 2

Leer el fichero _ZipDigits.train_ y visualice las imágenes. Seleccione sólo las 
instancias de los números 1 y 5. Guárdelas como matrices de tamaño 16x16.

```{r ex 3.2}
digits <- read.table("./DigitosZip/zip.train",
                     quote = "",
                     comment.char = "",
                     stringsAsFactors = F)

# ones.and.fives <- digits[which( digits$V1 == 5 | digits$V1 == 1) , ]
ones <- digits[digits$V1 == 1, -1]
fives <- digits[digits$V1 == 5, -1]

# Thanks to http://stackoverflow.com/a/36196635/1612432
# Create a list of 16x16 matrix
ones <- lapply(1:nrow(ones), function(i){matrix(ones[i, ], nrow = 16)})
fives <- lapply(1:nrow(fives), function(i){matrix(fives[i, ], nrow = 16)})

par(mfrow = c(2,2))
tmp <- lapply(1:4, function(i){
  image(matrix(unlist(ones[[i]][, 16:1]),
               nrow = 16),
        col = grey.colors(256))
})
tmp <- lapply(1:4, function(i){
  image(matrix(unlist(fives[[i]][, 16:1]),
               nrow = 16),
        col = grey.colors(256))
})
par(mfrow = c(1,1))
```

## Ejercicio 3

Para cada matriz de números calcularemos su valor medio y su grado de simetría
vertical. Para calcular la simetría calculamos la suma del valor absoluto de
las diferencias en cada píxel entre la imagen original y la imagen que obtenemos
invertiendo el orden de las columnas. Finalmente le cambiamos el signo.

```{r ex 3.3}
ones.length <- length(ones)
fives.length <- length(fives)

ones.mean <- sapply(1:ones.length, function(i) mean(as.double(ones[[i]])))
fives.mean <- sapply(1:fives.length, function(i) mean(as.double(fives[[i]])))


# Compute vertical simmetry
ones.inverted <- lapply(1:ones.length,
                        function(i) ones[[i]][, 16:1])
fives.inverted <- lapply(1:fives.length,
                        function(i) fives[[i]][, 16:1])

ones.vertical.symmetry <- sapply(1:ones.length,
                                 function(i) -sum(abs(as.numeric(ones[[i]]) 
                                             - as.numeric(ones.inverted[[i]]))))

fives.vertical.symmetry <- sapply(1:fives.length,
                                 function(i) -sum(abs(as.numeric(fives[[i]])
                                             - as.numeric(fives.inverted[[i]]))))
```

## Ejercicio 4

Representar en los ejes {X = Intensidad promedio, Y = Simetría} las instancias
seleccionadas de 1 y 5.

```{r ex 3.4}
symmetry.intensity <- data.frame(
                                int = c(ones.mean,
                                        fives.mean),
                                sym = c(ones.vertical.symmetry,
                                        fives.vertical.symmetry),
                                class = c(rep("1", ones.length),
                                          rep("5", fives.length)))

plot(symmetry.intensity$sym ~ symmetry.intensity$int,
     data = symmetry.intensity,
     pch  = 20, 
     col  = symmetry.intensity$class,
     xlab = "Mean Intensity",
     ylab = "Vertical Symmetry",
     main = "Mean intensity and Vertical Symmetry")
legend("bottomright", legend = levels(symmetry.intensity$class),
       col = unique(symmetry.intensity$class), ncol = 3, pch = 20, bty = "n")
```

## Ejercicio 5

Implementar la función _RegressLin(data, label)_ que permita ajustar un modelo
de regesión lineal (Usar SVD). Los datos de entrada se interpretan igual que 
en clasificación.

```{r ex 3.5}
RegressLin <- function(data, labels) {
  # Fit a Regression model using SVD (Singular Value Decomposition)
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   labels: The labels corresponding to the data, only binary classification
  #
  # Returns:
  #   The weigths fitting the data using Linear Regression
  if (!is.matrix(data)) {
    data <- as.matrix(data)
  }
  if (!is.matrix(labels)) {
    labels <- as.matrix(labels)
  }
  # Get the inverse of Xt*X
  x <- t(data) %*% data
  duv <- svd(x)
  x.inv <- duv$v %*% diag(1 / duv$d) %*% t(duv$u)
  # Now compute the pseudoinverse of X
  x.pseudo.inv <- x.inv %*% t(data)
  # Finally, get the weights
  w <- x.pseudo.inv %*% labels
  
  return(w)
}
```

## Ejercicio 6

Ajustar un modelo de regresión lineal a los datos de (Intensidad promedio,
simetría) y pintar la solución con los datos, valorar el resultado.

```{r ex 3.6}
ones.fives.train <- digits[digits$V1 == 1 | digits$V1 == 5,]

rm(digits)

nrows <- nrow(symmetry.intensity)
X <- matrix(data  = c(rep(1, nrows), symmetry.intensity$int, symmetry.intensity$sym), 
             nrow = nrows)
y <- matrix(data = c(rep(1, ones.length), rep(-1, fives.length)),
            nrow = nrow(X))

w <- RegressLin(X, y)
plot(X[,-1],
     pch  = 20, 
     col  = factor(y),
     xlab = "Mean Intensity",
     ylab = "Vertical Symmetry",
     main = "Mean intensity and Vertical Symmetry")
legend("bottomright", legend = levels(symmetry.intensity$class),
       col = unique(symmetry.intensity$class), ncol = 3, pch = 20, bty = "n")
DrawBoundary(w)
```

Con los datos que tenemos, el modelo ajusta bastante bien los datos. A
diferencia del perceptrón, aunque estos datos no son separables, se 
encuentra una buena solución. 

Quizá se estén sobre ajustando un poco, ya que hay dos puntos correspondientes
a los cincos que están muy cercanos a la frontera. 

## Ejercicio 7

En este ejercicio exploramos cómo funciona regresión lineal en problemas de
clasificación. Para ello generamos datos usando el mismo procedimiento que en
ejercicios anteriores. Suponemos $\mathcal{X} = [-10, 10]\times [-10,10]$ y elegimos
muestras aleatorias uniformes dentro de $\mathcal{X}$. La función f en cada
caso será una recta aleatoria que corta a X y que asigna etiqueta a cada punto
con el valor de su signo. En cada apartado generamos una muestra y le asignamos
etiqueta con la función f generada. En cada ejecución generamos una nueva
función f.

- Fijar el tamaño de muestra $N = 100$. Usar regresión lineal para encontrar
$g$ y evaluar $E_{in}$, (el porcentaje de puntos incorrectamente clasificados).
Repetir el experimento 1000 veces y promediar los resultados ¿Qué valor obtiene
para $E_{in}$?

```{r ex 3.7.1}
NewDataSet <- function(n, range = c(-10, 10), line = NULL) {
  # Generate a new data set from uniform random data in the given range
  # 
  # Args:
  #   n: The size of the data set
  #   range: In which range get the data
  #   line: If we want to give a line instead of generate one randomly 
  # 
  # Returns:
  #   A matrix with the data labeled by a random plane
  x <- SimulaUnif(n, 2, range)
  x <- cbind(rep(1, n), x)
  labels.line <- LabelData(x, line = line)
  
  res <- list(x = x,
              y = labels.line$labels,
              line = labels.line$line)
  return(res)
}

# GetEin <- function(x, w, y) {
#   # Computes the in sample error
#   # 
#   # Args:
#   #   x: The data
#   #   w: The weight vector
#   #   y: The labels for the data
#   # 
#   # Returns:
#   #   The squared in-sample error
#   if (!is.matrix(x))
#     x <- as.matrix(x)
#   if (!is.matrix(w))
#     w <- as.matrix(w)
#   if (!is.matrix(y))
#     y <- as.matrix(y)
#   
#   n <- nrow(x)
#   
#   1/n * (x %*% w - y) ^ 2
# }

data <- NewDataSet(100)
x <- data$x
y <- data$y
w <- RegressLin(x, y)
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
sum(h.label != y)/nrow(x)

# TODO, do this for all code, sign
e.in.1000.times <- replicate(1000, {
  data <- NewDataSet(100)
  x <- data$x
  y <- data$y
  w <- RegressLin(x, y)
  h.label <- sign(t(w) %*% t(x))

  sum(h.label != y)/nrow(x)
})

cat("Ein mean over 1000 iterations:",
    as.character(mean(e.in.1000.times)),
    ". ~",
    as.character(mean(e.in.1000.times) * 100), "%")
```

- Fijar el tamaño de la muestra $N = 100$. Usar regresión lineal para encontrar
g y evaluar $E_{out}$. Para ello generar 1000 puntos nuevos y usarlos para
estimar el error fuera de la muestra, $E_{out}$ (Porcentaje de puntos mal 
clasificados). De nuevo, ejecutar el experimento 1000 veces y tomar el promedio.
¿Qué valor obtiene de $E_{out}$? Valore los resultados.

```{r ex 3.7.2}
data <- NewDataSet(100)
x <- data$x
y <- data$y
w <- RegressLin(x, y)
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
cat("In sample error: ", sum(h.label != y)/nrow(x))

data <- NewDataSet(1000, line = data$line)
x <- data$x
y <- data$y
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
cat("Out of sample error: ", sum(h.label != y)/nrow(x))

e.out.1000.times <- replicate(1000, {
  data <- NewDataSet(100)
  x <- data$x
  y <- data$y
  w <- RegressLin(x, y)
  h.label <- sign(t(w) %*% t(x))
  
  res <- list(ein = sum(h.label != y)/nrow(x))
  
  data <- NewDataSet(1000, line = data$line)
  x <- data$x
  y <- data$y
  h.label <- sign(t(w) %*% t(x))
  
  res$eout <- sum(h.label != y)/nrow(x)
  
  res
})
cat("Ein mean:",
    as.character(mean(unlist(e.out.1000.times["ein",]))),
    ". ~",
    as.character(mean(unlist(e.out.1000.times["ein",])) * 100), "%")

cat("Eout mean:",
    as.character(mean(unlist(e.out.1000.times["eout",]))),
    ". ~",
    as.character(mean(unlist(e.out.1000.times["eout",])) * 100), "%")

```

Cuando se trata de aproximar la función original, hemos de encontar el
equilibrio entre $E_{out}$ y $E_{in}$. Nos interesa tener
$E_{out}\approx E_{in}$, pero además también nos interesa un
$E_{in} \approx 0$. Lo ideal es tener $E_{out}\approx E_{in}$ y que además
estos valores sean cercanos a cero, pero esto no siempre es posible.

En este caso, al estar jungando con tan pocos datos, 100 para training
y 1000 para test, tenemos unos valores para $E_{out}$ entorno al 2.5%, y para
$E_{in}$ en torno al 1.9%. Para conseguir bajar estos valores, y que sigan
siendo cercanos el uno al otro, es necesario tener una cantidad de datos
para training que representen lo mejor posible la función f, la real pero
desconocida. De este modo, conseguiremos ajustar bien los datos, obteniendo
un $E_{in}$ pequeño y además cercano a $E_{out}$, representando con una mayor
exactitud por tanto, la función f.

También entra en juego la complejidad de la función f, cuando la f es compleja,
usaremos un conjunto de hipótesis $\mathcal{H}$ más complejo para obtener un
$E_{in} \approx 0$, pero para conseguirlo necesitaremos más datos.

Podemos comprobar que efectivamente, que a mayor representación de la f obtenemos
mejores resultados para ambas medidas, si fijamos los datos para estimar fuera
de la muestra a 1000, y usamos para calcular $E_{out}$ 10000 datos, los 
porcentajes de error están en torno a $E_{out} = 1.96\%$ y $E_{in} = 1.83\%$, como
se aprecia, son mucho más similares entre sí, y más cercanos a cero. Si incrementamos
10 veces el tamaño de los datos, obtenemos $E_{in} = 1.84465\%$ y $E_{out} = 1.85712\%$



## PAra regresión, images(nivelesdeGrises)
## Ficheros, read.table(, ..., ...) stringfactor = False.

# EN lugar de trabajar con todas, reducimos a dos caracteristicas, la media de la intensidad, 

a = info de los 16x16, 

cogemos la variable a complementaria, para restarla


```{r Generate R file, echo=FALSE}
# library(knitr)
# purl("P1.Rmd")
```