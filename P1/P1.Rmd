---
title: "Aprendizaje Automático - Práctica 1"
author: "Alejandro Alcalde"
date: "March 3, 2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 12pt
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1000000007)
```

# Funciones de utilidad

```{r utilidades gráficas}
myPlot = function(data, labels = NULL, myMain = NULL, pch = 20, xlab = "x", ylab = "y", ...) {
  # Wrapper function for plotting
  #
  # Args:
  #   Data: A data structure containing the data in the form x,y
  #   labels: Labels to color the data points
  #   rest of params: Graphical params to plot
  if (length(labels) != 0) {
    labels[labels == 1] = "#525F7F"
    labels[labels == -1] = "#D2372B"
  } else {
    labels = "#525F7F"
  }
  
  if (is.null(dim(labels))) {
    plot(data, col = labels, pch = pch, xlab = xlab, ylab = ylab, ...)
  } else {
    for (i in seq(1, dim(labels)[2])) {
      plot(data, col = unlist(labels[i]), main = myMain[i], pch = pch, xlab = xlab, ylab = ylab, ...)
    }
  }
}
```

# Ejercicio de Generación y Visualización de datos

## Ejercicio 1

Construir una función _lista = simula_unif (N, dim, rango)_ que calcule una lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números aleatorios uniformes en el intervalo _rango_.

```{r Exercise 1}
simula_unif <- function(N, dimen = 1, rango = c(0:1)) {
  # Generate a list of vectors populated with uniform distributed random numbers
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   rango: Range to use generating the numbers
  #
  # Returns:
  #   A list of vectors populated randomly
  array(runif(dimen*N, rango[1], rango[2]), dim = c(N, dimen))
}

simula_unif(10, 5, c(10,100))
simula_unif(10, 2)
simula_unif(10)
```

## Ejercicio 2

Construir una función _lista = simula_gaus(N, dim, sigma)_ que calcule una lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números aleatorios gaussianos de media 0 y varianza dadas por el vector de _sigma_.

```{r Exercise 2}
simula_gauss <- function(N, dimen = 1, sig = 1){
  # Generate a list of vectors populated using the normal (Gaussian) distribution
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   sigma: Vector of Standard deviations
  #
  # Returns:
  #   A list of vectors populated randomly
  array(rnorm(dimen*N, sd = sqrt(sig)), dim = c(N, dimen))
}

simula_gauss(10, 5, c(1,2))
```

## Ejercicio 3

Suponer _N=50, dim=2, rango=[-50,+50]_ en cada dimensión. Dibujar una gráfica de la salida de la función correspondiente.

```{r Exercise 3}
datos = simula_unif(50, 2, c(-50,50)); datos
myPlot(datos, main="Gráfica datos de simula_unif(50, 2, c(-50,50))")
```

## Ejercicio 4

Suponer _N=50, dim=2, $\sigma=[5,7]$_ dibujar una gráfica de la salida de la función correspondiente.

```{r Exercise 4}
datos = simula_gauss(50, 2, c(5,7)); datos
myPlot(datos, main="Gráfica datos de  simula_gauss(50, 2, c(5,7))", type = "b")
```

## Ejercicio 5

Construir la función _v = simula_recta(intervalo)_ que calcula los parámetros _v = (a,b)_ de una recta aleatoria, _y = ax + b_, que corte al cuadrado _[-50, 50] x [-50, 50]_.

```{r Exercise 5}

simula_recta <- function(interval = c(-50, 50)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  = simula_unif(2, 2, interval)
  ## Compute the slope
  m = (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 = m(x − x1)
  line = m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  c(m, line, thePoints)
}

interval = c(-50, 50);
x = simula_unif(5000, 2, interval);

result = simula_recta(interval)
m = result[1]; m
line = result[2]; line
data = array(result[3:6], c(2,2)); data
```

## Ejercicio 6

Generar una muestra 2D de puntos usando _simula_unif_ y etiquetar la muestra usando el signo de la función $f(x,y) = y - ax - b$ de cada punto a una recta simulada con _simula_recta_. Mostrar una gráfica con el resultado de la muestra etiquetada junto con la recta usada para ello.

```{r Exercise 6}
interval = c(-100, 100)
# Generate a set of points
x = simula_unif(5000, 2, interval)

fsign <- function(x, y, m, b){
  # Function to label a 2D Point
  y - m*x - b
}

straight = simula_recta(interval)
m = straight[1]
line = straight[2]

# Apply fsign to each point of the set, each x,y will be passed to fsign
result = mapply(fsign, x[,1], x[,2], m = m, b = line)
# Label the result with -1/1
result = ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
datafsign = data.frame(x = x[,1], y = x[,2], result = result)

# Draw the result
myPlot(datafsign[1:2], datafsign$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse = " "),
       sub = paste(c("Interval = ", interval), collapse = " "),
       ylab = "")
abline(line,m)
```

## Ejercicio 7

Usar la muestra generada en el apartado anterior y etiquetarla con +1, -1 usando el signo de cada una de las siguientes funciones.

- $f(x, y) = (x - 10)^2 + (y - 20)^2 - 400$
- $f(x, y) = 0.5(x + 10)^2 + (y - 20)^2 - 400$
- $f(x, y) = 0.5(x - 10)^2 - (y + 20)^2 - 400$
- $f(x, y) = y - 20x^2 - 5x + 3$

Visualizar el resultado del etiquetado de cada función junto con su gráfica y comparar el resultado con el caso lineal. ¿Qué consecuencias extrae sobre las regiones positiva y negativa?

```{r Exercise 7}
# Define the functions
f1 <- function(x, y) {
  (x - 10) ^ 2 + (y - 20) ^ 2 - 400
}
f2 <- function(x, y){
  0.5 * (x + 10)^2 + (y - 20)^2 - 400
}
f3 <- function(x, y){
  0.5 * (x - 10)^2 - (y + 20)^2 - 400
}
f4 <- function(x, y){
  y - 20*x^2 - 5*x + 3
}

# Generate a vector of functions
multi.fun <- function(x, y) {
      c(f1 = f1(x, y), f2 = f2(x, y), f3 = f3(x, y), f4 = f4(x, y))
}

# Apply the functions
result = mapply(multi.fun, x[,1], x[,2])

# Label the results
result = ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
datafunctions = data.frame(x = x[,1], y = x[,2], 
                  f1 = result[1,], f2 = result[2,], f3 = result[3,], f4 = result[4,])

# Draw the result

myPlot(datafunctions[1:2], datafunctions[3:6], c("f1", "f2", "f3", "f4"))
```

Si observamos el caso lineal, vemos que las muestras son separables por una línea recta. Esto se debe a que la función que generó el etiquetado ($f(x,y) = y - ax - b$) es lineal.

Las cuatro funciones usadas para el etiquetado en esta ocasión, son __no lineales__, lo cual implica que los datos etiquetados no serán separables por una simple recta. Se necesita una función más compleja que pueda realizar el particionamiento de los datos positivos y negativos.

## Ejercicio 8

Considerar de nuevo la muestra etiquetada en el ejercicio 6. Modifique las etiquetas de un 10% aleatorio de muestras positivas y otro 10% aleatorio de negativas.

- Visualice los puntos con las nuevas etiquetas y la recta del ejercicio 6
- En una gráfica a parte visualice de nuevo los mismos puntos pero junto con las funciones del ejercicio 7.

Observe las gráficas y diga qué consecuencias extrae del proceso de modificación de etiquetas en el proceso de aprendizaje.

Para el primer apartado, cambiamos el 10% de las muestras positivas y negativas y obtenemos:

```{r Exercise 8.a}
getPercentageOfData <- function(x, condition = 1, percentage = .1){
  # Get the percentage of samples that meet condition
  #
  # Args:
  #   x: A vector containing the data
  #   condition: Condition that the data need to satisfy
  #   percentaje: What percentage of samples to get
  #
  # Returns:
  #   Indexes of the percentage of the samples that meet the condition
  meetCondition = which(x == condition)
  sample(meetCondition, length(meetCondition) * percentage)
}

# Get a 10% of samples labeled with a 1
indexPositive = getPercentageOfData(datafsign$result)
datafsign$result[indexPositive] = -1

indexNegative = getPercentageOfData(datafsign$result, -1)
datafsign$result[indexNegative] = 1

# Draw the result
myPlot(datafsign[1:2], datafsign$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse = " "),
       sub = paste(c("Interval = ", interval), collapse = " "),
       ylab = "")
abline(line,m)
```

La única observación que puede hacerse, es que ahora los datos ya no son separables y estamos introduciendo ruido a la muestra.

```{r Exercise 8.b}
# Get a 10% of samples labeled with a 1 in all 4 functions
indexPositive = lapply(datafunctions[3:6], getPercentageOfData)
# Change 1 by -1
datafunctions$f1[indexPositive$f1] = -1
datafunctions$f2[indexPositive$f2] = -1
datafunctions$f3[indexPositive$f3] = -1
datafunctions$f4[indexPositive$f4] = -1

# Get a 10% of samples labeled with a 1 in all 4 functions
indexNegative = lapply(datafunctions[3:6], getPercentageOfData, condition = -1)
# Change -1 by 1
datafunctions$f1[indexNegative$f1] = 1
datafunctions$f2[indexNegative$f2] = 1
datafunctions$f3[indexNegative$f3] = 1
datafunctions$f4[indexNegative$f4] = 1

# Draw the result
# par(mfrow = c(2,2))
myPlot(datafunctions[1:2], datafunctions[3:6], c("f1", "f2", "f3", "f4"))
```

En esta ocasión, al igual que en la anterior, al cambiar deliberadamente el valor de las etiquetas por su clase contraria, estamos introduciendo mucho ruido en los datos. Como en el caso anterior, los datos dejan de ser separables y difícilmente se aprenderá algo de este conjunto de datos, ya que las dos clases estás distribuidas uniformemente por todo el espacio.

# Ejercicio de Ajuste del Algoritmo Perceptron

## Ejercicio 1

Implementar la función _sol = ajusta\_PLA(datos, label, max_iter, vini)_ que calcula el hiperplano solución a un problema de clasificación binaria usando el algoritmo PLA. La entrada _datos_ es una matriz donde cada item con su etiqueta está representado por una fila de la matriz, _label_ el vector de etiquetas (cada etiqueta es un valor +1 o -1), _max\_iter_ es el número máximo de iteraciones permitidas y _vini_ el valor inicial del vector. La salida _sol_ devuelve los coeficientes del hiperplano. 

```{r Exercise 2.1}
ajusta_PLA = function(data,
                      label,
                      max_iter = 100,
                      vini = NULL) {
  # Get the weight to learn the data passed as parameter
  # using the 2D Perceptron algorithm 
  #
  # Args:
  #   data: An array/dataframe of with four columns (x0, x1, x2, y)
  #   label: How to label the dataset, must be of equal lenght of data
  #   max_iter: How much iter to find a solution
  #   vini: Initial weights
  #
  # Returns:
  #   The weights learned to adjust the data set
  
  #### Helper functions ####
  evaluate <- function(d,w){ 
    # returns current classifications on data 
    # for a given set of params, w
    # 
    # Args:
    #   d: should be of shape m x n
    #   w: should be of shape 1 x n
    #
    # Returns:
    #   The labels for the data
    
    evaluation = apply(d, 1, function(x) sum(x*w))
    
    ifelse(evaluation > 0, 1, -1)
  }
  
  random_row <- function(wrongClassified){
    # select a random row number from a data frame of choices
    #
    # Args: 
    #   wrongClassified: Data incorrectly classified from which to choose a random row
    #
    # Returns:
    #   The index of the row picked
    
    pickedRandomRow <- if(nrow(wrongClassified)==1){ #if there is only one misclassification, choose it
      as.numeric(rownames(wrongClassified))
    } else { #otherwise, choose randomly from the misclassified rows
      sample(as.numeric(rownames(wrongClassified)),1) 
    }
    pickedRandomRow
  }
  
  # Check that the data parameter is of the correct form
  if (dim(data)[2] != 4 ){
    print("Data need to have 4 columns. (x0, x1, x2, y)")
    return()
  }
  xs = data[-4]
  
  #### Begin the perceptron learning ####
  
  # If no vini was specified, generate one randomly
  vini <- if (is.null(vini)) runif(3, -.5, .5)
  
  f_function = unname(unlist(data[4]))
  g_function = label
  
  nIters = 0
  
  while (!all(g_function == f_function) && nIters <= max_iter) {
    #while there are mis-classifications
    
    #store the number of iterations
    nIters <- nIters + 1
    
    #randomly choose a mis-classified example
    wrongClassified = xs[g_function != f_function, ]
    pickRandomRow = random_row(wrongClassified)
    
    #update the weights
    vini =
      vini + f_function[pickRandomRow] * as.vector(wrongClassified[rownames(wrongClassified) ==
                                                                pickRandomRow, ])
    #update predictions with new boudary (and plot)
    g_function = evaluate(xs, vini)
  }
  
  if (nIters > max_iter){
    print("Max iter reached, no solution found, the current vini was:")
  } else {
    print(paste("Finished in ", nIters, "iterations"))
    # add_boundary(vini, color='red') #plot final boundary
  }

  vini
}

# Generate randomly the true weights
w <- runif(3, -5, 5)

# Generate Data to test
N = 100
x0 <- rep(1, N)
x1 <- runif(N, -10, 10)
x2 <- runif(N,-10, 10)
df <- data.frame(x0, x1, x2)
df$y = apply(df, 1, function(x) sum(x*w))
df$y = ifelse(df$y > 0, 1, -1)
dataset = df[-4]

label = ifelse(runif(N, -1, 1) >0 , 1,-1)
learnedWeights = ajusta_PLA(data = df, label = label); learnedWeights
```

```{r Generate R file, echo=FALSE}
# library(knitr)
# purl("P1.Rmd")
```