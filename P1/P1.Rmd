---
title: "Aprendizaje Automático - Práctica 1"
author: "Alejandro Alcalde"
date: "March 3, 2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: zenburn
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# library(parallel)
# Calculate the number of cores
# cores.number <- detectCores()
# Initialize seed
# clusterSetRNGStream(cl, iseed = 1000000007)
set.seed(1000000007)
```

# Funciones de utilidad

```{r utilidades gráficas}
MyPlot <- function(data, labels = NULL, myMain = NULL, pch = 20, xlab = "x",
                   ylab = "y", ...) {
  # Wrapper function for plotting
  #
  # Args:
  #   Data: A data structure containing the data in the form x,y
  #   labels: Labels to color the data points
  #   rest of params: Graphical params to plot
  if (length(labels) != 0) {
    labels[labels == 1] <- "#1A1314"
    labels[labels == -1] <- "#D2372B"
  } else {
    labels <- "#1A1314"
  }

  if (is.null(dim(labels))) {
    plot(data,
         col = labels,
         pch = pch,
         xlab = xlab,
         ylab = ylab,
         ...)
  } else {
    for (i in seq(1, dim(labels)[2])) {
      plot(data,
           col = unlist(labels[i]),
           main = myMain[i],
           pch = pch,
           xlab = xlab,
           ylab = ylab,
           ...)
    }
  }
}
```

# Ejercicio de Generación y Visualización de datos

## Ejercicio 1

Construir una función _lista <- SimulaUnif (N, dim, rango)_ que calcule una 
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios uniformes en el intervalo _rango_.

```{r Exercise 1}
SimulaUnif <- function(N, dimen = 1, rango = c(0:1)) {
  # Generate a list of vectors populated with uniform distributed random numbers
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   rango: Range to use generating the numbers
  #
  # Returns:
  #   A list of vectors populated randomly
  array(runif(dimen*N, rango[1], rango[2]),
        dim = c(N, dimen))
}

SimulaUnif(10, 5, c(10,100))
SimulaUnif(10, 2)
SimulaUnif(10)
```

## Ejercicio 2

Construir una función _lista <- simula_gaus(N, dim, sigma)_ que calcule una
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios gaussianos de media 0 y varianza dadas por el vector de _sigma_.

```{r Exercise 2}
SimulaGauss <- function(N, dimen = 1, sig = 1){
  # Generate a list of vectors populated using the normal (Gaussian)
  # distribution
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   sigma: Vector of Standard deviations
  #
  # Returns:
  #   A list of vectors populated randomly
  array(rnorm(dimen*N, sd = sqrt(sig)),
        dim = c(N, dimen))
}

SimulaGauss(10, 5, c(1,2))
```

## Ejercicio 3

Suponer _N=50, dim=2, rango=[-50,+50]_ en cada dimensión. Dibujar una gráfica
de la salida de la función correspondiente.

```{r Exercise 3}
datos <- SimulaUnif(50, 2, c(-50,50))
print(datos)
MyPlot(datos,
       main = "Gráfica datos de SimulaUnif(50, 2, c(-50,50))")
```

## Ejercicio 4

Suponer _N=50, dim=2, $\sigma=[5,7]$_ dibujar una gráfica de la salida de la
función correspondiente.

```{r Exercise 4}
datos <- SimulaGauss(50, 2, c(5,7))
print(datos)
MyPlot(datos,
       main = "Gráfica datos de  SimulaGauss(50, 2, c(5,7))",
       type = "b")
```

## Ejercicio 5

Construir la función _v <- SimulaRecta(intervalo)_ que calcula los parámetros _v <- (a,b)_ de una recta aleatoria, _y <- ax + b_, que corte al cuadrado _[-50, 50] x [-50, 50]_.

```{r Exercise 5}

SimulaRecta <- function(interval = c(-50, 50)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  <- SimulaUnif(2, 2, interval)
  
  # Compute the slope
  m <- (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 <- m(x − x1)
  line <- m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  c(m, line, thePoints)
}

interval <- c(-50, 50);
x <- SimulaUnif(5000, 2, interval);

result <- SimulaRecta(interval)
m <- result[1]
print(m)
line <- result[2]
print(line)
data <- array(result[3:6], c(2,2))
print(data)
```

## Ejercicio 6

Generar una muestra 2D de puntos usando _SimulaUnif_ y etiquetar la muestra usando el signo de la función $f(x,y) <- y - ax - b$ de cada punto a una recta simulada con _SimulaRecta_. Mostrar una gráfica con el resultado de la muestra etiquetada junto con la recta usada para ello.

```{r Exercise 6}
interval <- c(-100, 100)
# Generate a set of points
x <- SimulaUnif(500, 2, interval)

fsign <- function(x, y, m, b){
  # Function to label a 2D Point
  y - m * x - b
}

straight <- SimulaRecta(interval)
m <- straight[1]
line <- straight[2]

# Apply fsign to each point of the set, each x,y will be passed to fsign
result <- mapply(fsign, x[,1], x[,2],
                 m = m,
                 b = line)

# Label the result with -1/1
result <- ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
data.fsign <- data.frame(x      = x[,1],
                        y      = x[,2],
                        result = result)

# Draw the result
MyPlot(data.fsign[1:2],
       data.fsign$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse = " "),
       sub  = paste(c("Interval <- ", interval), collapse = " "),
       ylab = "")
abline(line,m)
```

## Ejercicio 7

Usar la muestra generada en el apartado anterior y etiquetarla con +1, -1 usando el signo de cada una de las siguientes funciones.

- $f(x, y) <- (x - 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x + 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x - 10)^2 - (y + 20)^2 - 400$
- $f(x, y) <- y - 20x^2 - 5x + 3$

Visualizar el resultado del etiquetado de cada función junto con su gráfica y comparar el resultado con el caso lineal. ¿Qué consecuencias extrae sobre las regiones positiva y negativa?

```{r Exercise 7}
# Define the functions
F1 <- function(x, y) {
  (x - 10) ^ 2 + (y - 20) ^ 2 - 400
}
F2 <- function(x, y){
  0.5 * (x + 10) ^ 2 + (y - 20) ^ 2 - 400
}
F3 <- function(x, y){
  0.5 * (x - 10) ^ 2 - (y + 20) ^ 2 - 400
}
F4 <- function(x, y){
  y - 20 * x ^ 2 - 5 * x + 3
}

# Generate a vector of functions
MultiFun <- function(x, y) {
      c(F1 = F1(x, y), 
        F2 = F2(x, y),
        F3 = F3(x, y),
        F4 = F4(x, y))
}

# Apply the functions
result <- mapply(MultiFun, x[,1], x[,2])

# Label the results
result <- ifelse(result >= 0, 1, -1)

# Create a data frame to store the result
data.functions <- data.frame(x  = x[,1],
                            y  = x[,2],
                            F1 = result[1,],
                            F2 = result[2,],
                            F3 = result[3,],
                            F4 = result[4,])

# Draw the result
MyPlot(data.functions[1:2], data.functions[3:6], c("F1", "F2", "F3", "F4"))
```

Si observamos el caso lineal, vemos que las muestras son separables por una 
línea recta. Esto se debe a que la función que generó el etiquetado
($f(x,y) <- y - ax - b$) es lineal.

Las cuatro funciones usadas para el etiquetado en esta ocasión, son
__no lineales__, lo cual implica que los datos etiquetados no serán separables
por una simple recta. Se necesita una función más compleja que pueda realizar el
particionamiento de los datos positivos y negativos.

## Ejercicio 8

Considerar de nuevo la muestra etiquetada en el ejercicio 6. Modifique las
etiquetas de un 10% aleatorio de muestras positivas y otro 10% aleatorio de
negativas.

- Visualice los puntos con las nuevas etiquetas y la recta del ejercicio 6
- En una gráfica a parte visualice de nuevo los mismos puntos pero junto con
las funciones del ejercicio 7.

Observe las gráficas y diga qué consecuencias extrae del proceso de modificación
de etiquetas en el proceso de aprendizaje.

Para el primer apartado, cambiamos el 10% de las muestras positivas y negativas
y obtenemos:

```{r Exercise 8.a}
GetPercentageOfData <- function(x, condition = 1, percentage = .1){
  # Get the percentage of samples that meet condition
  #
  # Args:
  #   x: A vector containing the data
  #   condition: Condition that the data need to satisfy
  #   percentaje: What percentage of samples to get
  #
  # Returns:
  #   Indexes of the percentage of the samples that meet the condition
  meetCondition <- which(x == condition)
  sample(meetCondition, length(meetCondition) * percentage)
}

# Get a 10% of samples labeled with a 1
positive.index <- GetPercentageOfData(data.fsign$result)
data.fsign.noisy <- data.fsign
data.fsign.noisy$result[positive.index] <- -1

negative.index <- GetPercentageOfData(data.fsign$result, -1)
data.fsign.noisy$result[negative.index] <- 1

# Draw the result
MyPlot(data.fsign.noisy[1:2],
       data.fsign.noisy$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse <- " "),
       sub  = paste(c("Interval <- ", interval), collapse <- " "),
       ylab = "")
abline(line,m)
```

La única observación que puede hacerse, es que ahora los datos ya no son separables y estamos introduciendo ruido a la muestra.

```{r Exercise 8.b}
# Initiate cluster
# cl <- makeCluster(cores.number, type = "FORK")
# Get a 10% of samples labeled with a 1 in all 4 functions
# system.time(positive.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData))
positive.index <- lapply(data.functions[3:6], GetPercentageOfData)

data.functions.noisy <- data.functions
# Change 1 by -1
data.functions.noisy$F1[positive.index$F1] <- -1
data.functions.noisy$F2[positive.index$F2] <- -1
data.functions.noisy$F3[positive.index$F3] <- -1
data.functions.noisy$F4[positive.index$F4] <- -1

# Get a 10% of samples labeled with a 1 in all 4 functions
# negative.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData, condition = -1)
negative.index <- lapply(data.functions[3:6], GetPercentageOfData, condition = -1)

# Free memory of the cluster
# stopCluster(cl)

# Change -1 by 1
data.functions.noisy$F1[negative.index$F1] <- 1
data.functions.noisy$F2[negative.index$F2] <- 1
data.functions.noisy$F3[negative.index$F3] <- 1
data.functions.noisy$F4[negative.index$F4] <- 1

# Draw the result
# par(mfrow <- c(2,2))
MyPlot(data.functions.noisy[1:2], data.functions.noisy[3:6],
       c("F1", "F2", "F3", "F4"))
```

En esta ocasión, al igual que en la anterior, al cambiar deliberadamente el
valor de las etiquetas por su clase contraria, estamos introduciendo mucho
ruido en los datos. Como en el caso anterior, los datos dejan de ser separables
y difícilmente se aprenderá algo de este conjunto de datos, ya que las dos
clases estás distribuidas uniformemente por todo el espacio.

# Ejercicio de Ajuste del Algoritmo Perceptron

## Ejercicio 1

Implementar la función _sol <- ajusta\_PLA(datos, label, max_iter, vini)_ que
calcula el hiperplano solución a un problema de clasificación binaria usando el
algoritmo PLA. La entrada _datos_ es una matriz donde cada item con su etiqueta
está representado por una fila de la matriz, _label_ el vector de etiquetas
(cada etiqueta es un valor +1 o -1), _max\_iter_ es el número máximo de
iteraciones permitidas y _vini_ el valor inicial del vector. La salida _sol_
devuelve los coeficientes del hiperplano. 

```{r Exercise 2.1}
AjustaPla <- function(data,
                      label,
                      max_iter = 1000,
                      vini     = matrix(0, 1, 3),
                      visual = F,
                      show.last.iter = F) {
  # Get the weight to learn from the data passed as parameter
  # using the 2D Perceptron algorithm
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   label: The labels for the data passed in
  #   max_iter: How much iter to find a solution
  #   vini: Initial weights
  #   visual: Draw each hyperplane found
  #   show.last.iter: Draw the last hyperplane found, even when the algorithm
  #     does not converge
  #
  # Returns:
  #   The weights learned to adjust the data set and how many iterations it 
  #   did in a data frame.
  #   If the PLA did not converged in within max_iter, the currents weights
  #   are returned and the iterations are set to -1
  
  GetSign <- function(values){
    # Return the sign for the values
    #
    # Args:
    #   values: The values to get the sign of.
    #
    # Returns:
    #   The sign of the values
    return(ifelse(values > 0, 1, -1))
  }
  
  DrawBoundary <- function(w, color = '#525F7F', ...) {
  # Plot decision boundary defined by
  # a parameter vector w
  
  # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
  # 
  # |w3|/||w|| is the distance from the origin, w3 itself does not have a good geometrical interpretation (as long as w is not unit-length).
  # 
  # In order to plot line with such equation you can simply draw a line through (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
  
    w <- as.numeric(w)
    
    b <- w[1]
    w1 <- w[2]
    w2 <- w[3]
    
    slope <- -(w1 / w2)
    intercept <- -(b / w2)
    
    abline(a   = intercept,
           b   = slope,
           col = color,
           ...)
  }
  
  converged <- F
  alpha <- 50
  
  if (visual == T) {
    MyPlot(data, label)
  }
  
  # Add x0
  data <- cbind(rep(1, nrow(data)), data)
  
  # Convert weights as matrix
  as.matrix(vini)
  
  # Begin the perceptron
  nIters <- 0
  
  
  while (!converged && nIters <= max_iter) {
    # while there are mis-classifications
    
    nIters <- nIters + 1
    
    # Calculate h(x) with the weight vector vini and the data input data
    h.function <- GetSign(vini %*% t(data))
    
    # Calculate the misclassified mask
    misclassified.subseting <- h.function != label
    
    if (all(misclassified.subseting == F)) {
      converged <- T
    } else {
      # Update the weight vector for a point randomly selected
      
      # Get the misclassified points out
      misclassified.points <- data[misclassified.subseting, , drop = F]
      misclassified.points.labels <- label[misclassified.subseting]
      
      # Get one of them
      misclassified.point.index <- sample(dim(misclassified.points)[1], 1)
      misclassified.point <- misclassified.points[misclassified.point.index,
                                                  , drop = F]
      misclassified.point.label <-
        misclassified.points.labels[misclassified.point.index]
      
      # update the weights
      vini <- vini + misclassified.point.label %*% misclassified.point
      
      if (visual == T) {
        if (alpha >= 128) {
          alpha <- 10
        }
        DrawBoundary(vini, rgb(82/255,95/255,127/255, alpha/255))
        alpha <- alpha + 1
      }
    }
  }
  
  if (!converged) {
    nIters <- -1
    if (show.last.iter == T && visual == T) {
      DrawBoundary(vini, "#D2372B", lwd = 5)
    }
  } else if (visual == T) {
    DrawBoundary(vini, "#F9CE0C", lwd = 5)
    legend('bottomright',
           c("Final", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#F9CE0C","#525F7F"))
  }
  
  data.frame(vini = vini,
             iter = nIters)
}

LabelData <- function(p){
  # Returns the corresponding label to the data, giving +1/-1 depending on
  # which side of the line the point lies
  #
  # Args:
  #   p: The points to label
  #
  # Returns:
  #   The labels (+1/-1) for the data
  
  # Initialize a random plane to separate the -1, +1
  line <- matrix(runif(4, -1, 1), 2, 2)
  line[1,2] <- -1
  line[2,2] <- +1
  
  # Given two points, determine if a point from the data set lies on the -1
  # side or the 1 side.
  # The points are A, B, the query points (X,Y)
  # The equation is (Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax)
  values <- (line[2,1] - line[1,1]) * (p[,2] - line[1,2]) -
    (line[2,2] - line[1,2]) * (p[,1] - line[1,1])
  
  return(ifelse(values > 0, 1, -1))
}

# Initialize N random points, and Y
n <- 500
x <- matrix(runif(n*2, -1, 1), n, 2)
y <- LabelData(x)

# Run perceptron algorithm
w <- AjustaPla(x, y, visual = T)

ShowPlaResult <- function(w, x, y){
  iter <- w[4]
  w <- as.double(w[-4])
  
  if (iter != -1) {
    ## PLA converged
    
    # Plot the points according to its actual class
    MyPlot(x, y, main = paste("Perceptron training against N <- ", n))
    
    # Plot the learned boundary
    # DrawBoundary(w)
    # legend('bottomright', c("Exact", "Learned"), lty=c(1,1), lwd=c(2.5,2.5),col=c("green","red"))
  } else {
    cat("PLA did not converge")
  }
}

# ShowPlaResult(w, x, y)
```

## Ejercicio 2

Ejecutar el algoritmo PLA con los valores simulados en el apartado 6,
inicializando el algoritmo con el vector cero y con vectores de números
aleatorios en [0,1], (10 veces). 

Anotar el número medio de iteraciones necesarias en ambos para converger.
Valorar el resultado.

```{r Exercise 2.2}

perceptron.data <- as.matrix(data.fsign[1:2])
y <- LabelData(perceptron.data)

perceptron.result.10times.weights.zero <- replicate(10, {
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y)
  perceptron.result[4]
})

perceptron.result.10times.weights.random <- replicate(10, {
  
  weights <- matrix(runif(3, -1, 1), 1,3)
  
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y,
                                 vini  = weights)
  perceptron.result[4]
})

print(paste("The mean iterations of execute PLA 10 times with a vini of 0 is ",
            mean(as.numeric(perceptron.result.10times.weights.zero))))
print(paste("The mean iterations of execute PLA 10 times with a random vini ",
            mean(as.numeric(perceptron.result.10times.weights.random))))
```

Por lo general, cuando se inicializa el vector a cero el preceptrón tiende a
converger en menos iteraciones. Todo depende del punto de partida. No importa
como se inicialce

## Ejercicio 3

Ejecutar el algoritmo PLA con los datos generados en el apartado 8 del
ejercicio 4.2 usando valores de 10, 100 y 1000 para _max_iter_. Etiquetar los
datos de la muestra usando la función solución encontrada y contar el número de
errores respecto de las etiquetas originales. Valorar el resultado.

```{r Exercise 2.3}
# Show the noisy data
MyPlot(data   = data.fsign.noisy[1:2],
       labels = data.fsign.noisy[3])
perceptron.data.noisy <- as.matrix(data.fsign.noisy[1:2])

#  Parallel version
# mcmapply(AjustaPla, 
#                       max_iter = c(100, 1000, 10000),
#                       MoreArgs = list(data   = perceptron.data.noisy,
#                                       label  = data.fsign.noisy[3],
#                                       visual = T),
#          mc.cores = detectCores())

# Launch the algorithm three times, with different values for max_iter
# and draw the result of the last hyperplane calculated
ws <- mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data   = perceptron.data.noisy,
                             label  = data.fsign.noisy[3],
                             visual = F,
                             show.last.iter = T))
ws <- ws[-4,]
    
w.10.iters <- ws[,1]
w.100.iters <- ws[,2]
w.1000.iters <- ws[,3]

GetLabelsFromWeight <- function(w, data) {
  # Get the labels asociated with the hyperplane passed in as parameter.
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   w: Weights from which decide if a point lies on one side or another
  #
  # Returns:
  #   A vector with labels for the data
  w <- as.numeric(w)
  
  b <- w[1]
  w1 <- w[2]
  w2 <- w[3]
  
  slope <- -(w1 / w2)
  intercept <- -(b / w2)
  
  ifelse(slope * data[,1] + intercept - data[,2] >= 0, 1, -1)
}

# this are where the points divided by the perceptron lies
error.10.iters <- sum(GetLabelsFromWeight(w.10.iters, perceptron.data.noisy) != data.fsign.noisy[3])
error.100.iters <- sum(GetLabelsFromWeight(w.100.iters, perceptron.data.noisy) != data.fsign.noisy[3])
error.1000.iters <- sum(GetLabelsFromWeight(w.1000.iters, perceptron.data.noisy) != data.fsign.noisy[3])

cat("Missclassified points with 10 iters: ", error.10.iters)
cat("Missclassified points with 100 iters: ", error.100.iters)
cat("Missclassified points with 1000 iters: ", error.1000.iters)

user.input <- readline("Press <return> to continue to the next exercise") 
```

En esta ocasión, al haber introducido en uno de los ejercicios ruido en los
datos, el algoritmo perceptrón no va a converger nunca, ya que los datos no
son separables.

Como coclusión podemos resaltar que no importa el número
de iteraciones que haga el algoritmo, ya que al no tener memoria, en algunas 
ejecuciones el número de datos mal clasificados puede ser muy bajo para 10 
iteraciones y muy alto para 1000 iteraciones, y en otras ocasiones al contrario.

```{r Generate R file, echo=FALSE}
# library(knitr)
# purl("P1.Rmd")
```


## PAra regresión, images(nivelesdeGrises)
## Ficheros, read.table(, ..., ...) stringfactor = False.

# EN lugar de trabajar con todas, reducimos a dos caracteristicas, la media de la intensidad, 

a = info de los 16x16, 

cogemos la variable a complementaria, para restarla

