---
title: "Aprendizaje Automático - Práctica 1"
author: "Alejandro Alcalde"
date: "March 3, 2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output: 
  # md_document:
  #   variant: markdown_github
  #   toc: true
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: zenburn
    # keep_tex: yes
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# library(parallel)
# Calculate the number of cores
# cores.number <- detectCores()
# Initialize seed
# clusterSetRNGStream(cl, iseed = 1000000007)
set.seed(1000000007)
p <- "Press <return> to continue to the next exercise"
```

# Funciones de utilidad

```{r utilidades gráficas}
MyPlot <- function(data, labels = NULL, myMain = NULL, pch = 20, xlab = "x",
                   ylab = "y", ...) {
  # Wrapper function for plotting
  #
  # Args:
  #   Data: A data structure containing the data in the form x,y
  #   labels: Labels to color the data points
  #   rest of params: Graphical params to plot
  if (length(labels) != 0) {
    labels[labels == 1] <- "#1A1314"
    labels[labels == -1] <- "#D2372B"
  } else {
    labels <- "#1A1314"
  }

  if (is.null(dim(labels))) {
    plot(data,
         col = labels,
         pch = pch,
         xlab = xlab,
         ylab = ylab,
         ...)
  } else {
    for (i in seq(1, dim(labels)[2])) {
      plot(data,
           col = unlist(labels[i]),
           main = myMain[i],
           pch = pch,
           xlab = xlab,
           ylab = ylab,
           ...)
      cat("\r\n\r\n") # Fix for pdf generation
    }
  }
}

DrawBoundary <- function(w, color = '#525F7F', ...) {
  # Draw the line defined by the weight vector w.
  #
  # Thanks to http://stackoverflow.com/a/19069020/1612432 for the explanation:
  # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision
  # boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
  # 
  # |w3|/||w|| is the distance from the origin, w3 itself does not have a good
  # geometrical interpretation (as long as w is not unit-length).
  # 
  # In order to plot line with such equation you can simply draw a line through
  # (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
  # 
  # Args:
  #   w: The weight vector
  #   color: The color of the line to draw
  w <- as.numeric(w)
  
  b <- w[1]
  w1 <- w[2]
  w2 <- w[3]
  
  slope <- -(w1 / w2)
  intercept <- -(b / w2)
  
  abline(a   = intercept,
         b   = slope,
         col = color,
         ...)
}
```

# Ejercicio de Generación y Visualización de datos

## Ejercicio 1

*Construir una función _lista <- SimulaUnif (N, dim, rango)_ que calcule una 
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios uniformes en el intervalo _rango_.*

```{r Exercise 1}
SimulaUnif <- function(N, dimen = 1, rango = c(0:1)) {
  # Generate a list of vectors populated with uniform distributed random numbers
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   rango: Range to use generating the numbers
  #
  # Returns:
  #   A list of vectors populated randomly
  array(runif(dimen*N, rango[1], rango[2]),
        dim = c(N, dimen))
}
```

Vemos ahora unos cuantos ejemplos de uso de esta función:

```{r}
head(SimulaUnif(10, 5, c(10,100)), n = 3)
head(SimulaUnif(10, 2), n = 3)
head(SimulaUnif(10), n = 3)

user.input <- readline(p) 
```

## Ejercicio 2

*Construir una función _lista <- simula_gaus(N, dim, sigma)_ que calcule una
lista de longitud _N_ de vectores de dimensión _dim_ conteniendo números
aleatorios gaussianos de media 0 y varianza dadas por el vector de _sigma_.*

```{r Exercise 2}
SimulaGauss <- function(N, dimen = 1, sig = 1){
  # Generate a list of vectors populated using the normal (Gaussian)
  # distribution
  #
  # Args:
  #   N: Length of the list
  #   dimen: The dimension of each vector in the list
  #   sigma: Vector of Standard deviations
  #
  # Returns:
  #   A list of vectors populated randomly
  array(rnorm(dimen*N, sd = sqrt(sig)),
        dim = c(N, dimen))
}

head(SimulaGauss(10, 5, c(1,2)), n = 3)

user.input <- readline(p) 
```

## Ejercicio 3

*Suponer _N=50, dim=2, rango=[-50,+50]_ en cada dimensión. Dibujar una gráfica
de la salida de la función correspondiente.*

```{r Exercise 3}
datos <- SimulaUnif(50, 2, c(-50,50))
head(datos, n = 3)
MyPlot(datos,
       main = "Plot of SimulaUnif(50, 2, c(-50,50))")
user.input <- readline(p) 
```

## Ejercicio 4

*Suponer _N=50, dim=2, $\sigma=[5,7]$_ dibujar una gráfica de la salida de la
función correspondiente.*

```{r Exercise 4}
datos <- SimulaGauss(50, 2, c(5,7))
head(datos, n = 3)
MyPlot(datos,
       main = "Gráfica datos de  SimulaGauss(50, 2, c(5,7))",
       type = "b")
user.input <- readline(p) 
```

## Ejercicio 5

*Construir la función _v <- SimulaRecta(intervalo)_ que calcula los parámetros _v <- (a,b)_ de una recta aleatoria, _y <- ax + b_, que corte al cuadrado _[-50, 50] x [-50, 50]_.*

```{r Exercise 5}

SimulaRecta <- function(interval = c(-50, 50)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  <- SimulaUnif(2, 2, interval)
  
  # Compute the slope
  m <- (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 <- m(x − x1)
  line <- m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  res <- list(slope  = m,
              line   = line,
              points = thePoints)
  
  res
}

interval <- c(-50, 50);
x <- SimulaUnif(5000, 2, interval);

result <- SimulaRecta(interval)
print(result$slope)
print(result$line)
data <- array(result$points, c(2,2))
print(data)

user.input <- readline(p) 
```

## Ejercicio 6

*Generar una muestra 2D de puntos usando _SimulaUnif_ y etiquetar la muestra usando el signo de la función $f(x,y) <- y - ax - b$ de cada punto a una recta simulada con _SimulaRecta_. Mostrar una gráfica con el resultado de la muestra etiquetada junto con la recta usada para ello.*

```{r Exercise 6}
interval <- c(-100, 100)
# Generate a set of points
x <- SimulaUnif(500, 2, interval)

fsign <- function(x, y, m, b){
  # Function to label a 2D Point
  ifelse(y - m * x - b >= 0, 1, -1)
}

straight <- SimulaRecta(interval)
m <- straight$slope
line <- straight$line

# Apply fsign to each point of the set, each x,y will be passed to fsign
result <- mapply(fsign, x[,1], x[,2],
                 m = m,
                 b = line)

# Create a data frame to store the result
data.fsign <- data.frame(x      = x[,1],
                         y      = x[,2],
                         result = result)

# Draw the result
MyPlot(data.fsign[1:2],
       data.fsign$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse = " "),
       sub  = paste(c("Interval <- ", interval), collapse = " "),
       ylab = "")
abline(line,m)

user.input <- readline(p) 
```

## Ejercicio 7

*Usar la muestra generada en el apartado anterior y etiquetarla con +1, -1 usando el signo de cada una de las siguientes funciones.*

- $f(x, y) <- (x - 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x + 10)^2 + (y - 20)^2 - 400$
- $f(x, y) <- 0.5(x - 10)^2 - (y + 20)^2 - 400$
- $f(x, y) <- y - 20x^2 - 5x + 3$

*Visualizar el resultado del etiquetado de cada función junto con su gráfica y comparar el resultado con el caso lineal. ¿Qué consecuencias extrae sobre las regiones positiva y negativa?*

```{r Exercise 7}
# Define the functions
F1 <- function(x, y) {
  ifelse( ((x - 10) ^ 2 + (y - 20) ^ 2 - 400 ) >= 0, 1, -1)
}
F2 <- function(x, y){
  ifelse( (0.5 * (x + 10) ^ 2 + (y - 20) ^ 2 - 400) >= 0, 1, -1)
}
F3 <- function(x, y){
  ifelse( (0.5 * (x - 10) ^ 2 - (y + 20) ^ 2 - 400) >= 0, 1, -1)
}
F4 <- function(x, y){
  ifelse( (y - 20 * x ^ 2 - 5 * x + 3) >= 0, 1, -1)
}

# Generate a vector of functions
MultiFun <- function(x, y) {
      c(F1 = F1(x, y), 
        F2 = F2(x, y),
        F3 = F3(x, y),
        F4 = F4(x, y))
}

# Apply the functions
result <- mapply(MultiFun, x[,1], x[,2])

# Create a data frame to store the result
data.functions <- data.frame(x  = x[,1],
                             y  = x[,2],
                             F1 = result[1,],
                             F2 = result[2,],
                             F3 = result[3,],
                             F4 = result[4,])

# Draw the result
MyPlot(data.functions[1:2], data.functions[3:6], c("F1", "F2", "F3", "F4"))

user.input <- readline(p) 
```

Si observamos el caso lineal, vemos que las muestras son separables por una 
línea recta. Esto se debe a que la función que generó el etiquetado
($f(x,y) <- y - ax - b$) es lineal.

Las cuatro funciones usadas para el etiquetado en esta ocasión, son
__no lineales__, lo cual implica que los datos etiquetados no serán separables
por una simple recta. Se necesita una función más compleja que pueda realizar el
particionamiento de los datos positivos y negativos.

## Ejercicio 8

*Considerar de nuevo la muestra etiquetada en el ejercicio 6. Modifique las
etiquetas de un $10\%$ aleatorio de muestras positivas y otro $10\%$ aleatorio de
negativas.*

- *Visualice los puntos con las nuevas etiquetas y la recta del ejercicio 6*
- *En una gráfica a parte visualice de nuevo los mismos puntos pero junto con
las funciones del ejercicio 7.*

*Observe las gráficas y diga qué consecuencias extrae del proceso de modificación
de etiquetas en el proceso de aprendizaje.*

Para el primer apartado, cambiamos el $10\%$ de las muestras positivas y negativas
y obtenemos:

```{r Exercise 8.a}
GetPercentageOfData <- function(x, condition = 1, percentage = .1){
  # Get the percentage of samples that meet condition
  #
  # Args:
  #   x: A vector containing the data
  #   condition: Condition that the data need to satisfy
  #   percentaje: What percentage of samples to get
  #
  # Returns:
  #   Indexes of the percentage of the samples that meet the condition
  meetCondition <- which(x == condition)
  sample(meetCondition, length(meetCondition) * percentage)
}

# Get a 10% of samples labeled with a 1
positive.index <- GetPercentageOfData(data.fsign$result)
data.fsign.noisy <- data.fsign
data.fsign.noisy$result[positive.index] <- -1

negative.index <- GetPercentageOfData(data.fsign$result, -1)
data.fsign.noisy$result[negative.index] <- 1

# Draw the result
MyPlot(data.fsign.noisy[1:2],
       data.fsign.noisy$result,
       main = "Points labeled in function of y - m*x - b",
       xlab = paste(c("m=", m, "line=", line), collapse <- " "),
       sub  = paste(c("Interval <- ", interval), collapse <- " "),
       ylab = "")
abline(line,m)

user.input <- readline(p) 
```

La única observación que puede hacerse, es que ahora los datos ya no son 
separables y estamos introduciendo ruido a la muestra.

```{r Exercise 8.b}
# Initiate cluster
# cl <- makeCluster(cores.number, type = "FORK")
# Get a 10% of samples labeled with a 1 in all 4 functions
# system.time(positive.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData))
positive.index <- lapply(data.functions[3:6], GetPercentageOfData)

data.functions.noisy <- data.functions
# Change 1 by -1
data.functions.noisy$F1[positive.index$F1] <- -1
data.functions.noisy$F2[positive.index$F2] <- -1
data.functions.noisy$F3[positive.index$F3] <- -1
data.functions.noisy$F4[positive.index$F4] <- -1

# Get a 10% of samples labeled with a 1 in all 4 functions
# negative.index <- parLapply(cl, data.functions[3:6], GetPercentageOfData, condition = -1)
negative.index <- lapply(data.functions[3:6], GetPercentageOfData, condition = -1)

# Free memory of the cluster
# stopCluster(cl)

# Change -1 by 1
data.functions.noisy$F1[negative.index$F1] <- 1
data.functions.noisy$F2[negative.index$F2] <- 1
data.functions.noisy$F3[negative.index$F3] <- 1
data.functions.noisy$F4[negative.index$F4] <- 1

# Draw the result
# par(mfrow <- c(2,2))
MyPlot(data.functions.noisy[1:2], data.functions.noisy[3:6],
       c("F1", "F2", "F3", "F4"))

user.input <- readline(p) 
```

En esta ocasión, al igual que en la anterior, al cambiar deliberadamente el
valor de las etiquetas por su clase contraria, estamos introduciendo mucho
ruido en los datos. Como en el caso anterior, los datos dejan de ser separables
y difícilmente se aprenderá algo de este conjunto de datos, ya que las dos
clases estás distribuidas uniformemente por todo el espacio.

# Ejercicio de Ajuste del Algoritmo Perceptron

## Ejercicio 1

*Implementar la función _sol <- ajusta\_PLA(datos, label, max_iter, vini)_ que
calcula el hiperplano solución a un problema de clasificación binaria usando el
algoritmo PLA. La entrada _datos_ es una matriz donde cada item con su etiqueta
está representado por una fila de la matriz, _label_ el vector de etiquetas
(cada etiqueta es un valor +1 o -1), _max\_iter_ es el número máximo de
iteraciones permitidas y _vini_ el valor inicial del vector. La salida _sol_
devuelve los coeficientes del hiperplano.*

```{r Exercise 2.1}
AjustaPla <- function(data,
                      label,
                      max_iter       = 1000,
                      vini           = matrix(0, 1, 3),
                      visual         = F,
                      show.last.iter = F,
                      memory         = F) {
  # Get the weight to learn from the data passed as parameter
  # using the 2D Perceptron algorithm
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   label: The labels for the data passed in
  #   max_iter: How much iter to find a solution
  #   vini: Initial weights
  #   visual: Draw each hyperplane found
  #   show.last.iter: Draw the last hyperplane found, even when the algorithm
  #     does not converge
  #   memory: Tells the algorithm if it has to store the best result in each
  #     iteration in the case of not converging
  #
  # Returns:
  #   The weights learned to adjust the data set and how many iterations it 
  #   did in a data frame.
  #   If the PLA did not converged in within max_iter, the currents weights
  #   are returned and the iterations are set to -1
  
  GetSign <- function(values){
    # Return the sign for the values
    #
    # Args:
    #   values: The values to get the sign of.
    #
    # Returns:
    #   The sign of the values
    return(ifelse(values >= 0, 1, -1))
  }
  
  PLADrawBoundary <- function(w, color = '#525F7F', ...) {
    # Draw the line defined by the weight vector w.
    #
    # Thanks to http://stackoverflow.com/a/19069020/1612432 for the explanation:
    # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision
    # boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
    # 
    # |w3|/||w|| is the distance from the origin, w3 itself does not have a good
    # geometrical interpretation (as long as w is not unit-length).
    # 
    # In order to plot line with such equation you can simply draw a line through
    # (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
    # 
    # Args:
    #   w: The weight vector
    #   color: The color of the line to draw
    w <- as.numeric(w)
    
    if (w[3] != 0) {
      b <- w[1]
      w1 <- w[2]
      w2 <- w[3]
      
      slope <- -(w1 / w2)
      intercept <- -(b / w2)
      
      abline(a   = intercept,
             b   = slope,
             col = color,
             ...)
    }
  }
  
  converged <- F
  alpha <- 50
  
  if (visual == T) {
    max.item = max(data)
    x.lim = max.item + max.item * .15
    MyPlot(data, label, xlim = c(-x.lim, x.lim), ylim = c(-x.lim, x.lim))
    # Fix for rmd pdf generation, this put every plot below each other, no overflows
    cat('\r\n\r\n')
  }
  
  # Add x0
  data <- cbind(rep(1, nrow(data)), data)
  
  # Convert weights as matrix
  as.matrix(vini)
  
  # Begin the perceptron
  nIters <- 0
  
  vini.best <- vini
  e.in.best <- nrow(data)
  
  while (!converged && nIters <= max_iter) {
    # while there are mis-classifications
    
    nIters <- nIters + 1
    
    # Calculate h(x) with the weight vector vini and the data input data
    h.function <- GetSign(vini %*% t(data))
    
    # Calculate the misclassified mask
    misclassified.subseting <- h.function != label

    if (all(misclassified.subseting == F)) {
      converged <- T
    } else {
      # Update the weight vector for a point randomly selected
      
      # Get the misclassified points out
      misclassified.points <- data[misclassified.subseting, , drop = F]
      misclassified.points.labels <- label[misclassified.subseting]
      
      # Get one of them
      misclassified.point.index <- sample(dim(misclassified.points)[1], 1)
      misclassified.point <- misclassified.points[misclassified.point.index,
                                                  , drop = F]
      misclassified.point.label <-
        misclassified.points.labels[misclassified.point.index]
      
      # Always store the best of all results
      if (memory == T && !all(vini == 0)) {
        e.in.current <- sum(misclassified.subseting)
        if (e.in.current < e.in.best) {
          e.in.best <- e.in.current
          vini.best <- vini
        }
      }
      
      # update the weights
      vini <- vini + misclassified.point.label %*% misclassified.point
      
      
      if (visual == T) {
        if (alpha >= 128) {
          alpha <- 10
        }
        PLADrawBoundary(vini, rgb(82/255,95/255,127/255, alpha/255))
        alpha <- alpha + 1
      }
      
    }
  }

  if (!converged) {
    nIters <- -1
    
    # Return the best solution found in all iterations, not just the last one
    if (memory == T && !all(vini == 0)) {
      vini <- vini.best
    }
    
    if (show.last.iter == T && visual == T) {
      PLADrawBoundary(vini, "#D2372B", lwd = 3)
      legend('bottomright',
           c("Last Iter", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#D2372B","#525F7F"))
    }
    
  } else if (visual == T) {
    PLADrawBoundary(vini, "#F9CE0C", lwd = 3)
    legend('bottomright',
           c("Final", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#F9CE0C","#525F7F"))
  }
  
  # return as a unit vector
  vini <- vini / sqrt(sum(vini * vini))
  
  data.frame(vini = vini,
             iter = nIters)
}

LabelData <- function(p, line = NULL){
  # Returns the corresponding label to the data, giving +1/-1 depending on
  # which side of the line the point lies
  #
  # Args:
  #   p: The points to label
  #   line: If we want to give a line instead of generate one randomly
  #
  # Returns:
  #   A list with the labels (+1/-1) and the line
  
  if (is.null(line)) {
    # Initialize a random plane to separate the -1, +1
    line <- matrix(runif(4, -1, 1), 2, 2)
  }
  
  # Given two points, determine if a point from the data set lies on the -1
  # side or the 1 side.
  # The points are A, B, the query points (X,Y)
  # The equation is (Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax)
  values <- (line[2,1] - line[1,1]) * (p[,2] - line[1,2]) -
    (line[2,2] - line[1,2]) * (p[,1] - line[1,1])
  
  res = list(labels = ifelse(values > 0, 1, -1), 
             line = line)
  
  res
}

# Initialize N random points, and Y
n <- 500
x <- matrix(runif(n*2, -1, 1), n, 2)
y <- LabelData(x)

# Run perceptron algorithm
w <- AjustaPla(x, y$labels, visual = T)

user.input <- readline(p)
```

## Ejercicio 2

*Ejecutar el algoritmo PLA con los valores simulados en el apartado 6,
inicializando el algoritmo con el vector cero y con vectores de números
aleatorios en $[0,1]$, ($10$ veces).*

*Anotar el número medio de iteraciones necesarias en ambos para converger.
Valorar el resultado.*

```{r Exercise 2.2}
perceptron.data <- as.matrix(data.fsign[1:2])
y <- LabelData(perceptron.data)$labels

perceptron.result.10times.weights.zero <- replicate(100, {
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y)
  perceptron.result[4]
})

perceptron.result.10times.weights.random <- replicate(100, {
  
  weights <- matrix(runif(3, -1, 1), 1,3)
  
  perceptron.result <- AjustaPla(data  = perceptron.data,
                                 label = y,
                                 vini  = weights)
  perceptron.result[4]
})

mean.w.zero <- mean(as.numeric(perceptron.result.10times.weights.zero))
sd.w.zero <- sd(as.numeric(perceptron.result.10times.weights.zero))
mean.w.random <- mean(as.numeric(perceptron.result.10times.weights.random))
sd.w.random <- sd(as.numeric(perceptron.result.10times.weights.random))
  
cat("The mean iterations of execute PLA 100 times with a vini of 0 is",
    mean.w.zero,
    "\nIts sd is ", 
    sd.w.zero)

cat("The mean iterations of execute PLA 100 times with a random vini",
    mean.w.random,
    "\nIts sd is ", 
    sd.w.random)

user.input <- readline(p)
```

Con 1000 iteraciones como máximo, y ejecutándolo 100 veces, no se puede decir 
nada sobre si el vector inicial afecta al comportamiento del algoritmo. Como 
se aprecia en la salida, tanto la media como la desviación estándar del número de 
iteraciones necesarias para converger es similar, ya se inicialize el vector
a cero o de forma aleatoria.

## Ejercicio 3

*Ejecutar el algoritmo PLA con los datos generados en el apartado 8 del
ejercicio 4.2 usando valores de $10$, $100$ y $1000$ para _max_iter_. Etiquetar los
datos de la muestra usando la función solución encontrada y contar el número de
errores respecto de las etiquetas originales. Valorar el resultado.*

```{r Exercise 2.3}
# Show the noisy data
MyPlot(data   = data.fsign.noisy[1:2],
       labels = data.fsign.noisy[3])



GetPlaErrorByIterations <- function(data, labels, viters = c(10, 100, 1000)){
  # Get the errors of executing PLA for a different number of iterations
  # 
  # Args:
  #   data: The data
  #   labels: The labels of the data
  #   viters: A vector with how many iters do in each PLA execution
  # 
  # Returns:
  #   A list with the errors for the iters
  
  #  Parallel version
  # mcmapply(AjustaPla, 
  #                       max_iter = c(100, 1000, 10000),
  #                       MoreArgs = list(data   = perceptron.data.noisy,
  #                                       label  = data.fsign.noisy[3],
  #                                       visual = T),
  #          mc.cores = detectCores())
  # Launch the algorithm three times, with different values for max_iter
  # and draw the result of the last hyperplane calculated
  ws <- mapply(AjustaPla, 
               max_iter = viters,
               MoreArgs = list(data           = data,
                               label          = labels,
                               visual         = T,
                               show.last.iter = T))
  ws <- ws[-4,]
      
  w.10.iters <- as.matrix(unlist(ws[,1]))
  w.100.iters <- as.matrix(unlist(ws[,2]))
  w.1000.iters <- as.matrix(unlist(ws[,3]))
  data <- cbind(rep(1, nrow(data)), data)
  
  # Compute how many points are miss classified
  error.10.iters <- sum(as.numeric(sign(t(w.10.iters) %*% t(data))) != labels)
  error.100.iters <- sum(as.numeric(sign(t(w.100.iters) %*% t(data))) != labels)
  error.1000.iters <- sum(as.numeric(sign(t(w.1000.iters) %*% t(data))) != labels)
  
  res <- list(error1 = error.10.iters,
              error2 = error.100.iters,
              error3 = error.1000.iters)
  res
}

perceptron.data.noisy <- as.matrix(data.fsign.noisy[1:2])
result <- GetPlaErrorByIterations(perceptron.data.noisy, data.fsign.noisy[3])

cat("Missclassified points with 10 iters: ", result$error1)
cat("Missclassified points with 100 iters: ", result$error2)
cat("Missclassified points with 1000 iters: ", result$error3)

user.input <- readline(p)
```

En esta ocasión, al haber introducido en uno de los ejercicios ruido en los
datos, el algoritmo perceptrón no va a converger nunca, ya que los datos no
son separables.

Como coclusión podemos resaltar que no importa el número
de iteraciones que haga el algoritmo, ya que al no tener memoria, en algunas 
ejecuciones el número de datos mal clasificados puede ser muy bajo para 10 
iteraciones y muy alto para 1000 iteraciones, y en otras ocasiones al contrario.

## Ejercicio 4

*Repetir el análisis del punto anterior usando la primera función del ejercicio 7.*

```{r Exercise 2.4}
MyPlot(data   = data.functions[1:2],
       labels = data.functions$F1,
       main   = "The data labeles with the function F1")

perceptron.data <- as.matrix(data.functions[1:2])

result <- GetPlaErrorByIterations(perceptron.data, data.functions$F1)

cat("Missclassified points with 10 iters: ", result$error1)
cat("Missclassified points with 100 iters: ", result$error2)
cat("Missclassified points with 1000 iters: ", result$error3)

user.input <- readline(p)
```

En el ejercicio 7 ya se vio que los datos etiquetados usando la función F1 no 
son linealmente separables. Nos encontramos por tanto ante el mismo problema
del ejercicio anterior, el Perceptrón no será capaz de dividir los datos nunca.

## Ejercicio 5

*Modique la función `AjustaPla` para que le permita visualizar los datos y
soluciones que va encontrando a lo largo de las iteraciones. Ejecute con la nueva
versión el ejercicio 3.*

Este ejercicio se hizo sin querer al implementar el algoritmo en el ejercicio 1
con motivo de tener una ayuda visual y de esta forma ver que se ejecutaba
correctamente. Se explica a continuación el significado de los parámetros de
`AjustaPla`:

```{r exercise 2.5, eval=F}
AjustaPla <- function(data,
                      label,
                      max_iter       = 1000,
                      vini           = matrix(0, 1, 3),
                      visual         = F,
                      show.last.iter = F)
user.input <- readline(p)
```

Los parámetros relevantes para la visualización son `visual` y `show.last.iter`.
Si el primero es verdadero, se mostrará una gráfica con todos los hiperplanos
calculados en cada iteración, y en caso de converger, se mostrará el correcto 
de otro color. Cuando el algoritmo no converge, pero aún así queremos ver cual
fue el último hiperplano encontrado, hay que establecer a verdadero el segundo
parámetro.

Ejecutemos ahora el ejercicio 3 de forma visual:

```{r exercise 2.5b}
mapply(AjustaPla, 
             max_iter = c(10, 100, 1000),
             MoreArgs = list(data           = perceptron.data.noisy,
                             label          = data.fsign.noisy[3],
                             visual         = T,
                             show.last.iter = T))
user.input <- readline(p)
```

Como se aprecia en las gráficas, se agotan todas las iteraciones antes de encontrar
una solución, lo cual es lógico al no ser los datos separables. Debido al
funcionamiento del perceptrón, que no tiene memoria, el hiperplano que escoja
será el que calcule en la última iteración, y no el mejor encontrado hasta el 
momento. El no tener memoria junto a la propiedad estocástica propia del algoritmo, 
hacen que en distintas ejecuciones se obtengan muy pocos datos mal clasificados,
mientras que en otras existan muchos datos mal clasificados.

## Ejercicio 6

*A la vista de la conducta de las soluciones observada en el apartado anterior, 
proponga e implemente una modicación de la función original _sol = AjustaPLA\_MOD_
que permita obtener soluciones razonables sobre datos no linealmente separables. Mostrar
y valorar el resultado encontrado usando los datos del ejercicio 7.*

Para mejorar el algoritmo se ha realizado una pequeña modificación sobre el
anterior, ahora, se recuerda cual es el hiperplano que clasificaba incorrectamente
el menor número de puntos, de este modo, si llegamos a la última iteración sin 
haber encontrado solución, se devolverá el mejor encontrado. La modificación
es la siguiente:

```{r eval = F}
best.weight <- vini
best.e.in <- nrow(data)

current.e.in <- sum(misclassified.subseting)

if (current.e.in < best.e.in) {
  best.e.in <- current.e.in
  best.weight <- vini
}
```

Veamos primero cómo mejora la solución, para ello, ejecutaremos el algoritmo
con los mismos números aleatorios dos veces, una con esta mejora, y otra sin ella

```{r}
MyPlot(perceptron.data[,-1], data.fsign[3])
set.seed(1000000009)
w.no.memory <- AjustaPla(data           = perceptron.data.noisy,
                         label          = data.fsign.noisy[3],
                         visual         = T,
                         show.last.iter = T)
set.seed(1000000009)
w.memory <- AjustaPla(data           = perceptron.data.noisy,
                      label          = data.fsign.noisy[3],
                      visual         = T,
                      show.last.iter = T,
                      memory         = T)
```

Como vemos, la línea encontrada es distinta, así como el número de errores cometidos:

```{r}
w.memory <- as.matrix(w.memory)
w.no.memory <- as.matrix(w.no.memory)

perceptron.data.noisy <- cbind(rep(1, nrow(perceptron.data.noisy)), perceptron.data.noisy)

error.w.memory <- 
  sum(as.numeric(sign(t(w.memory[-4]) %*% t(perceptron.data.noisy))) != data.fsign.noisy[3])
error.w.no.memory <- 
  sum(as.numeric(sign(t(w.no.memory[-4]) %*% t(perceptron.data.noisy))) != data.fsign.noisy[3])

user.input <- readline(p)
```

Cometiendo el algoritmo con memoria `r error.w.memory` fallos y sin memoria
`r error.w.no.memory`. Una mejora bastante sustancial.

Ahora veamos la mejora con respecto a los datos del ejercicio 7:

```{r exercise 2.6}
result <- GetPlaErrorByIterations(as.matrix(data.functions[1:2]), data.functions$F1)

cat("Missclassified points with 10 iters: ", result$error1)
cat("Missclassified points with 100 iters: ", result$error2)
cat("Missclassified points with 1000 iters: ", result$error3)

user.input <- readline(p)
```

Como vemos, hay una mejora substancial con respecto al número de errores que 
comete el algoritmo perceptrón sin memoria, y esta última implementación. Al 
guardar la mejor solución encontrada hasta el momento, nos aseguramos obtener
un mejor resultado cuando los datos no son separables.

```{r reset env, echo = FALSE}
tmp <- ls()
tmp <- tmp[tmp != "MyPlot" &
           tmp != "DrawBoundary" &
           tmp != "SimulaUnif" &
           tmp != "LabelData" &
           tmp != "AjustaPla" &
           tmp != "GetPercentageOfData" &
           tmp != "p"]
rm(list = tmp)
```


# Ejercicios sobre Regresión Lineal

## Ejercicio 2

*Leer el fichero _ZipDigits.train_ y visualice las imágenes. Seleccione sólo las 
instancias de los números 1 y 5. Guárdelas como matrices de tamaño 16x16.*

```{r ex 3.2}
digits <- read.table("./datos/DigitosZip/zip.train",
                     quote = "",
                     comment.char = "",
                     stringsAsFactors = F)

# ones.and.fives <- digits[which( digits$V1 == 5 | digits$V1 == 1) , ]
ones <- digits[digits$V1 == 1, -1]
fives <- digits[digits$V1 == 5, -1]

# Thanks to http://stackoverflow.com/a/36196635/1612432
# Create a list of 16x16 matrix
ones <- lapply(1:nrow(ones), function(i){matrix(ones[i, ], nrow = 16)})
fives <- lapply(1:nrow(fives), function(i){matrix(fives[i, ], nrow = 16)})

par(mfrow = c(2,2))
tmp <- lapply(1:4, function(i){
  image(matrix(unlist(ones[[i]][, 16:1]),
               nrow = 16),
        col = grey.colors(256))
})
tmp <- lapply(1:4, function(i){
  image(matrix(unlist(fives[[i]][, 16:1]),
               nrow = 16),
        col = grey.colors(256))
})
par(mfrow = c(1,1))

user.input <- readline(p)
```

## Ejercicio 3

*Para cada matriz de números calcularemos su valor medio y su grado de simetría
vertical. Para calcular la simetría calculamos la suma del valor absoluto de
las diferencias en cada píxel entre la imagen original y la imagen que obtenemos
invertiendo el orden de las columnas. Finalmente le cambiamos el signo.*

```{r ex 3.3}
ones.length <- length(ones)
fives.length <- length(fives)

ones.mean <- sapply(1:ones.length, function(i) mean(as.double(ones[[i]])))
fives.mean <- sapply(1:fives.length, function(i) mean(as.double(fives[[i]])))


# Compute vertical simmetry
ones.inverted <- lapply(1:ones.length,
                        function(i) ones[[i]][, 16:1])
fives.inverted <- lapply(1:fives.length,
                        function(i) fives[[i]][, 16:1])

ones.vertical.symmetry <- sapply(1:ones.length,
                                 function(i) -sum(abs(as.numeric(ones[[i]]) 
                                             - as.numeric(ones.inverted[[i]]))))

fives.vertical.symmetry <- sapply(1:fives.length,
                                 function(i) -sum(abs(as.numeric(fives[[i]])
                                             - as.numeric(fives.inverted[[i]]))))
```

## Ejercicio 4

*Representar en los ejes {X = Intensidad promedio, Y = Simetría} las instancias
seleccionadas de 1 y 5.*

```{r ex 3.4}
symmetry.intensity <- data.frame(
                                int = c(ones.mean,
                                        fives.mean),
                                sym = c(ones.vertical.symmetry,
                                        fives.vertical.symmetry),
                                class = c(rep("1", ones.length),
                                          rep("5", fives.length)))

plot(symmetry.intensity$sym ~ symmetry.intensity$int,
     data = symmetry.intensity,
     pch  = 20, 
     col  = symmetry.intensity$class,
     xlab = "Mean Intensity",
     ylab = "Vertical Symmetry",
     main = "Mean intensity and Vertical Symmetry")
legend("bottomright", legend = levels(symmetry.intensity$class),
       col = unique(symmetry.intensity$class), ncol = 3, pch = 20, bty = "n")

user.input <- readline(p)
```

## Ejercicio 5

*Implementar la función _RegressLin(data, label)_ que permita ajustar un modelo
de regesión lineal (Usar SVD). Los datos de entrada se interpretan igual que 
en clasificación.*

```{r ex 3.5}
RegressLin <- function(data, labels) {
  # Fit a Regression model using SVD (Singular Value Decomposition)
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   labels: The labels corresponding to the data, only binary classification
  #
  # Returns:
  #   The weigths fitting the data using Linear Regression
  if (!is.matrix(data)) {
    data <- as.matrix(data)
  }
  if (!is.matrix(labels)) {
    labels <- as.matrix(labels)
  }
  
  # Get the inverse of Xt*X
  x <- t(data) %*% data
  duv <- svd(x)
  x.inv <- duv$v %*% diag(1 / duv$d) %*% t(duv$u)
  # Now compute the pseudoinverse of X
  x.pseudo.inv <- x.inv %*% t(data)
  # Finally, get the weights
  w <- x.pseudo.inv %*% labels
  
  w
}

user.input <- readline(p)
```

## Ejercicio 6

*Ajustar un modelo de regresión lineal a los datos de (Intensidad promedio,
simetría) y pintar la solución con los datos, valorar el resultado.*

```{r ex 3.6}
ones.fives.train <- digits[digits$V1 == 1 | digits$V1 == 5,]

rm(digits)

nrows <- nrow(symmetry.intensity)
X <- matrix(data  = c(rep(1, nrows), symmetry.intensity$int, symmetry.intensity$sym), 
             nrow = nrows)
y <- matrix(data = c(rep(1, ones.length), rep(-1, fives.length)),
            nrow = nrow(X))

w <- RegressLin(X, y)
plot(X[,-1],
     pch  = 20, 
     col  = factor(y),
     xlab = "Mean Intensity",
     ylab = "Vertical Symmetry",
     main = "Mean intensity and Vertical Symmetry")
legend("bottomright", legend = levels(symmetry.intensity$class),
       col = unique(symmetry.intensity$class), ncol = 3, pch = 20, bty = "n")
DrawBoundary(w)

user.input <- readline(p)
```

Con los datos que tenemos, el modelo ajusta bastante bien los datos. A
diferencia del perceptrón, aunque estos datos no son separables, se 
encuentra una buena solución. 

Quizá se estén sobre ajustando un poco, ya que hay dos puntos correspondientes
a los cincos que están muy cercanos a la frontera. 

## Ejercicio 7

*En este ejercicio exploramos cómo funciona regresión lineal en problemas de
clasificación. Para ello generamos datos usando el mismo procedimiento que en
ejercicios anteriores. Suponemos $\mathcal{X} = [-10, 10]\times [-10,10]$ y elegimos
muestras aleatorias uniformes dentro de $\mathcal{X}$. La función f en cada
caso será una recta aleatoria que corta a X y que asigna etiqueta a cada punto
con el valor de su signo. En cada apartado generamos una muestra y le asignamos
etiqueta con la función f generada. En cada ejecución generamos una nueva
función f.*

- *Fijar el tamaño de muestra $N = 100$. Usar regresión lineal para encontrar
$g$ y evaluar $E_{in}$, (el porcentaje de puntos incorrectamente clasificados).
Repetir el experimento 1000 veces y promediar los resultados ¿Qué valor obtiene
para $E_{in}$?*

```{r ex 3.7.1}
NewDataSet <- function(n, range = c(-10, 10), line = NULL) {
  # Generate a new data set from uniform random data in the given range
  # 
  # Args:
  #   n: The size of the data set
  #   range: In which range get the data
  #   line: If we want to give a line instead of generate one randomly 
  # 
  # Returns:
  #   A matrix with the data labeled by a random plane
  x <- SimulaUnif(n, 2, range)
  x <- cbind(rep(1, n), x)
  labels.line <- LabelData(x, line = line)
  
  res <- list(x = x,
              y = labels.line$labels,
              line = labels.line$line)
  res
}

data <- NewDataSet(100)
x <- data$x
y <- data$y
w <- RegressLin(x, y)
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
sum(h.label != y)/nrow(x)

e.in.1000.times <- replicate(1000, {
  data <- NewDataSet(100)
  x <- data$x
  y <- data$y
  w <- RegressLin(x, y)
  h.label <- sign(t(w) %*% t(x))

  sum(h.label != y)/nrow(x)
})

cat("Ein mean over 1000 iterations:",
    mean(e.in.1000.times),
    ". ~",
    mean(e.in.1000.times) * 100, "%")

user.input <- readline(p)
```

- *Fijar el tamaño de la muestra $N = 100$. Usar regresión lineal para encontrar
g y evaluar $E_{out}$. Para ello generar 1000 puntos nuevos y usarlos para
estimar el error fuera de la muestra, $E_{out}$ (Porcentaje de puntos mal 
clasificados). De nuevo, ejecutar el experimento 1000 veces y tomar el promedio.
¿Qué valor obtiene de $E_{out}$? Valore los resultados.*

```{r ex 3.7.2}
data <- NewDataSet(100)
x <- data$x
y <- data$y
w <- RegressLin(x, y)
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
cat("In sample error: ", sum(h.label != y)/nrow(x))

data <- NewDataSet(1000, line = data$line)
x <- data$x
y <- data$y
MyPlot(x[,-1], y)
DrawBoundary(w)
h.label <- sign(t(w) %*% t(x))
cat("Out of sample error: ", sum(h.label != y)/nrow(x))

e.out.1000.times <- replicate(1000, {
  data <- NewDataSet(100)
  x <- data$x
  y <- data$y
  w <- RegressLin(x, y)
  h.label <- sign(t(w) %*% t(x))
  
  res <- list(ein = sum(h.label != y)/nrow(x))
  
  data <- NewDataSet(1000, line = data$line)
  x <- data$x
  y <- data$y
  h.label <- sign(t(w) %*% t(x))
  
  res$eout <- sum(h.label != y)/nrow(x)
  
  res
})

ein.percentaje <- mean(unlist(e.out.1000.times["ein",])) * 100
eout.percentaje <- mean(unlist(e.out.1000.times["eout",])) * 100

cat("Ein mean:",
    mean(unlist(e.out.1000.times["ein",])),
    ". ~",
    ein.percentaje, "%")

cat("Eout mean:",
    mean(unlist(e.out.1000.times["eout",])),
    ". ~",
    eout.percentaje, "%")

user.input <- readline(p)
```

Cuando se trata de aproximar la función original, hemos de encontar el
equilibrio entre $E_{out}$ y $E_{in}$. Nos interesa tener
$E_{out}\approx E_{in}$, pero además también nos interesa un
$E_{in} \approx 0$. Lo ideal es tener $E_{out}\approx E_{in}$ y que además
estos valores sean cercanos a cero, pero esto no siempre es posible.

En este caso, al estar jungando con tan pocos datos, $100$ para estimar $E_{in}$
y $1000$ para $E_{out}$, tenemos unos valores para $E_{out}$ entorno al $`r eout.percentaje`\%$, y para
$E_{in}$ en torno al $`r ein.percentaje`\%$. Para conseguir bajar estos valores, y que sigan
siendo cercanos el uno al otro, es necesario tener una cantidad de datos
a estimar dentro de la muestra que representen lo mejor posible la función $f$, 
la real pero desconocida. De este modo, conseguiremos ajustar bien los datos,
obteniendo un $E_{in}$ pequeño y además cercano a $E_{out}$, representando con
una mayor exactitud por tanto, la función $f$.

También entra en juego la complejidad de la función $f$, cuando la $f$ es compleja,
usaremos un conjunto de hipótesis $\mathcal{H}$ más complejo para obtener un
$E_{in} \approx 0$, pero para conseguirlo necesitaremos más datos.

Podemos comprobar que efectivamente, a mayor representación de la $f$ obtenemos
mejores resultados para ambas medidas, si fijamos los datos para estimar fuera
de la muestra a $1000$, y usamos para calcular $E_{out}$ $10000$ datos, los 
porcentajes de error están en torno a $E_{out} = 1.96\%$ y $E_{in} = 1.83\%$, como
se aprecia, son mucho más similares entre sí, y más cercanos a cero. Si incrementamos
$10$ veces el tamaño de los datos, obtenemos $E_{in} = 1.84465\%$ y $E_{out} = 1.85712\%$

- *Ahora fijamos $N = 10$, ajustamos regresión lineal y usamos el vector de pesos
encontrado como un vector inicial de pesos para PLA. Ejecutar PLA hasta que
converja a un vector de pesos final que separe completamente la muestra de 
entrenamiento. Anote el número de iteraciones y repita el experimento 1.000 veces
¿Cual es valor promedio de iteraciones que tarda PLA en converger? (En cada
iteración de PLA elija un punto aleatorio del conjunto de mal clasificados). Valore
los resultados*


```{r ex 3.7.3}
perceptron.mean.iter <- replicate(1000, {
  data <- NewDataSet(10)
  x <- data$x
  y <- data$y
  w <- RegressLin(x, y)

    res <- AjustaPla(data = x[,-1],
            label = y,
            vini = t(w))
  
  res$iter
})

cat("Mean iterations of running PLA 1000 times with w set to the one found
    by Linear Regression:", mean(perceptron.mean.iter))

user.input <- readline(p)
```

Por lo general, la media de iteraciones de PLA oscila entre las $4$ y $5.5$. En 
bastantes ocasiones PLA encuentra una solución en una sola iteración. Cosa que
no es sorprendente, ya que coincide cuando el vector de pesos encontrado por 
regresión lineal separa completamente los datos. Es por esto que en esas ocasiones
PLA itera una única vez y devuelve una solución, ya que le estamos pasando el 
vector de pesos que separa totalmente los datos.

Sin embargo, el evento que dispara la media de iteraciones sucede cuando el 
algoritmo de regresión lineal encuentra una solución que no separa por completo
los datos, es aquí cuando PLA necesita iterar hasta conseguir una solución
correcta.

## Ejercicio 8

*En este ejercicio exploramos el uso de transformaciones no lineales. Consideremos la
función objetivo $f (x1 , x2 ) = sign(x_1^2 + x_2^2 − 25,0)$ Generar una muestra
de entrenamiento de $N = 1000$ puntos a partir de
$\mathcal{X} = [-10, 10]\times[-10, 10]$ muestreando cada punto $x\in \mathcal{X}$
uniformemente. Generar las salidas usando el signo de la función en los puntos
muestreados. Generar ruido sobre las etiquetas cambiando el signo de las salidas
a un $10\%$ de puntos del conjunto aleatorio generado.*

```{r ex 3.8}
F.sign8 <- function(x1, x2) sign(x1 ^ 2 + x2 ^ 2 - 25)

data <- SimulaUnif(N     = 1000,
                   dimen = 2,
                   rango = c(-10, 10))

y <- mapply(F.sign8, data[,1], data[,2])
data <- cbind(rep(1, nrow(data)), data)
# Generate a 10% noise
noise <- sample(1:length(y), length(y) * .1)
y[noise] <- -y[noise]

MyPlot(data[,-1], labels = y)

user.input <- readline(p)
```

- *Ajustar regresion lineal, para estimar los pesos $w$. Ejecutar el experimento
$1000$ veces y calcular el valor promedio del error de entrenamiento $E_{in}$.
Valorar el resultado.*

```{r Ex 3.8.1}
non.separable.linear.regress <- replicate(1000, {
  data <- SimulaUnif(N     = 1000,
                     dimen = 2,
                     rango = c(-10, 10))
  
  y <- mapply(F.sign8, data[,1], data[,2])
  data <- cbind(rep(1, nrow(data)), data)
  # Generate a 10% noise
  noise <- sample(1:length(y), length(y) * .1)
  y[noise] <- -y[noise]
  w <- RegressLin(data, y)
  h.label <- sign(t(w) %*% t(data))
  
  sum(h.label != y)/nrow(x)
})

cat("Ein mean:",
    mean(unlist(non.separable.linear.regress)),
    ". ~",
    mean(unlist(non.separable.linear.regress)) * 100, "%")

user.input <- readline(p)
```

Con la fórmula usada, los datos no son linealmente separables y por tanto
regresión lineal no puede encontrar ninguna solución buena. Termina por dejar
todos los puntos a un lado de la recta y por tanto clasificará mal toda una 
categoría de datos, en este caso los puntos rojos. Así, $E_{in}$ corresponde 
con el número de puntos rojos mal clasificados, que como hemos dicho son todos.

Por lo mencionado en el párrafo anterior, el error medio en la muestra está en
torno al $25\%$, que coincide con el porcentaje total de puntos rojos en la muestra.

- *Ahora consideremos $N = 1000$ datos de entrenamiento y el siguiente vector de
variables: $(1, x_1, x_2, x_1 x_2, x_1^2, x_2^2)$. Ajustar de nuevo regresión
lineal y calcular el nuevo vector de pesos $\hat{w}$. Mostrar el resultado*

```{r ex 3.8.2}
range <- c(-10, 10)
data <- SimulaUnif(N     = 1000,
                   dimen = 2,
                   rango = range)
  
y <- mapply(F.sign8, data[,1], data[,2])
data <- cbind(data, data[,1] * data[,2], data[,1] ^ 2, data[,2] ^ 2)
data <- cbind(rep(1, nrow(data)), data)

# Generate a 10% noise
noise <- sample(1:length(y), length(y) * .1)
y[noise] <- -y[noise]

w <- RegressLin(data, y)
labels.with.new.variables <- sign(t(w) %*% t(data))
labels.with.new.variables[noise] <- -labels.with.new.variables[noise]

MyPlot(data[,c(2,3)], labels = y)

xs <- seq(range[1],range[2],.1)
ys <- seq(range[1],range[2],.1)
zs <- outer(xs, ys, 
            function(x1, x2) w[1] + w[2] * x1 + w[3] * x2 + w[4] * x1 * x2 + w[5] * x1 ^ 2 + w[6] * x2 ^ 2)
zs2 <- outer(xs, ys, F.sign8)

contour(xs, ys, zs, nlev = 0, add = T, col = "blue", lty = 2)
contour(xs, ys, zs2, nlev = 0, add = T, col = "green", lty = 2)

legend("bottomright", legend = c("Real boundary", "Learned boundary"),
       col = c("green", "blue"), lty = 2)

user.input <- readline(p)
```

Como vemos, ahora sí que es posible separar los datos, aunque no con una función
lineal. El $\hat{w}$ calculado mediante regresión es: 
```{r}
w
```

- *Repetir el experimento anterior $1000$ veces calculando en cada ocasión el 
error fuera de la muestra. Para ello generar en cada ejecución $1000$ puntos
nuevos y valorar sobre ellos la función ajustada. Promediar los valores obtenidos.
¿Qué valor obtiene? Valorar el resultado.*

```{r ex 3.8.3}
e.out.1000.times <- replicate(1000, {
  data <- SimulaUnif(N     = 1000,
                     dimen = 2,
                     rango = range)
  
  y <- mapply(F.sign8, data[,1], data[,2])
  data <- cbind(data, data[,1] * data[,2], data[,1] ^ 2, data[,2] ^ 2)
  data <- cbind(rep(1, nrow(data)), data)
  
  w <- RegressLin(data, y)
  labels.with.new.variables <- sign(t(w) %*% t(data))
  
  res <- list(ein = sum(labels.with.new.variables != y)/nrow(data))
  
  data <- SimulaUnif(N     = 100,
                     dimen = 2,
                     rango = range)
  
  y <- mapply(F.sign8, data[,1], data[,2])
  data <- cbind(data, data[,1] * data[,2], data[,1] ^ 2, data[,2] ^ 2)
  data <- cbind(rep(1, nrow(data)), data)
  
  labels.with.new.variables <- sign(t(w) %*% t(data))
  
  res$eout <- sum(labels.with.new.variables != y)/nrow(data)
  
  res
})

ein <- mean(unlist(e.out.1000.times["ein",]))
eout <- mean(unlist(e.out.1000.times["eout",]))

cat("Ein mean:",
    ein,
    ". ~",
    ein * 100, "%")

cat("Eout mean:",
    eout,
    ". ~",
    eout * 100, "%")

user.input <- readline(p)
```

Como vemos, el $\hat{w}$ ahora tiene $6$ componentes

```{r}
w
```

Ya que ha sido necesario hacer un cambio de espacio para poder separar los datos
usando regresión lineal. Hasta ahora habíamos tenido porcentajes de errores bajos
tanto dentro como fuera de la muestra, en esta ocasión han subido entorno al 
$`r ein * 100`\%$ para $E_{in}$ y el $`r eout * 100`\%$ para $E_{out}$. Aunque 
es buena señal que sigan teniendo valores cercanos entre ellos, eso significa
que el muestreo realizado representa con bastante exactitud la función $f$.

```{r Generate R file, echo=FALSE, include = FALSE}
# library(knitr)
# purl("P1.Rmd")
```