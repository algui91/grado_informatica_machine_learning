---
title: "Aprendizaje Automático - Práctica 2"
author: "Alejandro Alcalde"
date: "17/04/2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output: html_document
  # md_document:
  #   variant: markdown_github
  #   toc: true
  # pdf_document:
  #   latex_engine: xelatex
  #   toc: true
  #   highlight: pygments
  #   # keep_tex: yes
---
```{r Load Libraries, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">>", cache = T)
set.seed(1000000007)

# Función para verificar si un paquete está instalado o no
is.installed <- function(paquete) is.element(
  paquete, installed.packages())

if (!is.installed("ggplot2"))
  install.packages("ggplot2", dependencies = T)
if (!is.installed("gridExtra"))
  install.packages("gridExtra", dependencies = T)
if (!is.installed("sos"))
  install.packages("sos")
if (!is.installed("pracma"))
  install.packages("pracma")
if (!is.installed("parallel"))
  install.packages("parallel")
if (!is.installed("orthopolynom"))
  install.packages("orthopolynom")
if (!is.installed("MASS"))
  install.packages("MASS")
if (!is.installed("glmnet"))
  install.packages("glmnet")

library(ggplot2)
library(gridExtra)  # for presenting plots side by side
library(sos)
library(pracma)
library(parallel)
library(orthopolynom)
library(MASS)
library(glmnet)
```

```{r Load aux functions}
# Calculate the number of cores
cores.number <- detectCores()
cl <- makeCluster(cores.number, type = "FORK")
# Initialize seed
clusterSetRNGStream(cl, iseed = 1000000007)

# Utils
GetLine <- function(interval = c(-1, 1)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  <- matrix(runif(4, min = interval[1], max = interval[2]), 2)
  
  # Compute the slope
  m <- (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 <- m(x − x1)
  line <- m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  res <- list(slope  = m,
              line   = line,
              points = thePoints)
  
  res
}

LabelData <- function(p, line = NULL){
  # Returns the corresponding label to the data, giving +1/-1 depending on
  # which side of the line the point lies
  #
  # Args:
  #   p: The points to label
  #   line: If we want to give a line instead of generate one randomly
  #
  # Returns:
  #   A list with the labels (+1/-1) and the line
  
  if (is.null(line)) {
    # Initialize a random plane to separate the -1, +1
    line <- matrix(runif(4, -1, 1), 2, 2)
  }
  
  # Given two points, determine if a point from the data set lies on the -1
  # side or the 1 side.
  # The points are A, B, the query points (X,Y)
  # The equation is (Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax)
  values <- (line[2,1] - line[1,1]) * (p[,2] - line[1,2]) -
    (line[2,2] - line[1,2]) * (p[,1] - line[1,1])
  
  res = list(labels = ifelse(values > 0, 1, -1), 
             line = line)
  
  res
}

NewDataSet <- function(n, range = c(-1, 1), line = NULL) {
  # Generate a new data set from uniform random data in the given range
  # 
  # Args:
  #   n: The size of the data set
  #   range: In which range get the data
  #   line: If we want to give a line instead of generate one randomly 
  # 
  # Returns:
  #   A matrix with the data labeled by a random plane
  xx <- matrix(runif(n*2, range[1],range[2]), n, 2)
  yy <- as.vector(LabelData(xx, line))
  xx <- cbind(rep(1, n), xx)

  list(x = xx, y = yy$labels)  
}

GetInterceptAndSlope <- function(w) {
  # Get the intercept and slope from weights
  #
  # Args:
  #   w: Vector weights
  #
  #  Returns:
  #    A list with intercept and slope
  w <- as.vector(w)
  
  b <- w[1]
  w1 <- w[2]
  w2 <- w[3]
  
  intercept <- -(w1 / w2)
  slope <- -(b / w2)

  list(intercept = intercept, slope = slope)
}

AjustaPla <- function(data,
                      label,
                      max_iter       = 1000,
                      vini           = matrix(0, 1, 3),
                      visual         = F,
                      show.last.iter = F,
                      memory         = F) {
  # Get the weight to learn from the data passed as parameter
  # using the 2D Perceptron algorithm
  #
  # Args:
  #   data: A matrix with the data (xn, yn)
  #   label: The labels for the data passed in
  #   max_iter: How much iter to find a solution
  #   vini: Initial weights
  #   visual: Draw each hyperplane found
  #   show.last.iter: Draw the last hyperplane found, even when the algorithm
  #     does not converge
  #   memory: Tells the algorithm if it has to store the best result in each
  #     iteration in the case of not converging
  #
  # Returns:
  #   The weights learned to adjust the data set and how many iterations it 
  #   did in a data frame.
  #   If the PLA did not converged in within max_iter, the currents weights
  #   are returned and the iterations are set to -1
  
  GetSign <- function(values){
    # Return the sign for the values
    #
    # Args:
    #   values: The values to get the sign of.
    #
    # Returns:
    #   The sign of the values
    return(ifelse(values >= 0, 1, -1))
  }
  
  PLADrawBoundary <- function(w, color = '#525F7F', ...) {
    # Draw the line defined by the weight vector w.
    #
    # Thanks to http://stackoverflow.com/a/19069020/1612432 for the explanation:
    # As your decision function is simply sgn(w1*x+w2*y+w3) then the decision
    # boundary equation is a line with canonical form w1*x + w2*y + w3 <- 0.
    # 
    # |w3|/||w|| is the distance from the origin, w3 itself does not have a good
    # geometrical interpretation (as long as w is not unit-length).
    # 
    # In order to plot line with such equation you can simply draw a line through
    # (0,-w3/w2) and (-w3/w1,0) (assuming that both w1 and w2 are non-zero)
    # 
    # Args:
    #   w: The weight vector
    #   color: The color of the line to draw
    w <- as.numeric(w)
    
    if (w[3] != 0) {
      b <- w[1]
      w1 <- w[2]
      w2 <- w[3]
      
      slope <- -(w1 / w2)
      intercept <- -(b / w2)
      
      abline(a   = intercept,
             b   = slope,
             col = color,
             ...)
    }
  }
  
  converged <- F
  alpha <- 50
  data <- as.matrix(data)
  
  if (visual == T) {
    max.item = max(data)
    x.lim = max.item + max.item * .15
    MyPlot(data, label, xlim = c(-x.lim, x.lim), ylim = c(-x.lim, x.lim))
    # Fix for rmd pdf generation, this put every plot below each other, no overflows
    cat('\r\n\r\n')
  }
  
  # Add x0
  data <- cbind(rep(1, nrow(data)), data)
  
  # Convert weights as matrix
  as.matrix(vini)
  
  # Begin the perceptron
  nIters <- 0
  
  vini.best <- vini
  e.in.best <- nrow(data)
  
  while (!converged && nIters <= max_iter) {
    # while there are mis-classifications
    
    nIters <- nIters + 1
    # Calculate h(x) with the weight vector vini and the data input data
    h.function <- GetSign(vini %*% t(data))
    
    # Calculate the misclassified mask
    misclassified.subseting <- h.function != label

    if (all(misclassified.subseting == F)) {
      converged <- T
    } else {
      # Update the weight vector for a point randomly selected
      
      # Get the misclassified points out
      misclassified.points <- data[misclassified.subseting, , drop = F]
      misclassified.points.labels <- label[misclassified.subseting]
      
      # Get one of them
      misclassified.point.index <- sample(dim(misclassified.points)[1], 1)
      misclassified.point <- misclassified.points[misclassified.point.index,
                                                  , drop = F]
      misclassified.point.label <-
        misclassified.points.labels[misclassified.point.index]
      
      # Always store the best of all results
      if (memory == T && !all(vini == 0)) {
        e.in.current <- sum(misclassified.subseting)
        if (e.in.current < e.in.best) {
          e.in.best <- e.in.current
          vini.best <- vini
        }
      }
      
      # update the weights
      vini <- vini + misclassified.point.label %*% misclassified.point
      
      
      if (visual == T) {
        if (alpha >= 128) {
          alpha <- 10
        }
        PLADrawBoundary(vini, rgb(82/255,95/255,127/255, alpha/255))
        alpha <- alpha + 1
      }
      
    }
  }

  if (!converged) {
    nIters <- -1
    
    # Return the best solution found in all iterations, not just the last one
    if (memory == T && !all(vini == 0)) {
      vini <- vini.best
    }
    
    if (show.last.iter == T && visual == T) {
      PLADrawBoundary(vini, "#D2372B", lwd = 3)
      legend('bottomright',
           c("Last Iter", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#D2372B","#525F7F"))
    }
    
  } else if (visual == T) {
    PLADrawBoundary(vini, "#F9CE0C", lwd = 3)
    legend('bottomright',
           c("Final", "PLA History"),
           lty = c(1,1),
           lwd = c(2.5,2.5),
           col = c("#F9CE0C","#525F7F"))
  }
  
  # return as a unit vector
  vini <- vini / sqrt(sum(vini * vini))
  
  data.frame(vini = vini,
             iter = nIters)
}
```

# Modelos Lineales
 
## Ejercicio 1

Gradiente Descendente. Implementar el algoritmo de gradiente descendiente.

- Considerar la función no lineal de error $E(u,v) = (ue^v - 2ve^{-u})^2$. Usar
graciente descendente y minimizar esta función de error, comenzando desde el punto
$(u,v) = (1,1)$ y usando una tasa de aprendizaje $\eta=.1$

1. Calcular analíticamente y mostrar la expresión del gradiente de la función 
  de error. 
$$\frac{\partial}{\partial u}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2v)(e^{u+v}+2v)$$
$$\frac{\partial}{\partial v}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2)(ue^{u+v}-2 v)$$

2. ¿Cuantas iteraciones tarda el algoritmo en obtener por primera vez un valor
de  $E(u,v)$ inferior a $10^{-14}$?.

```{r Ex 1.1a Gradient Descent}
learning.ratio <- 0.1
w <- as.double(c(1, 1))

ErrorFunction <- function(u, v)
  (u * exp(v) - 2 * v * exp(-u)) ^ 2
dEdu <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2 * v) * (exp(u + v) + 2 * v)
dEdv <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2) * (u * exp(u + v) - 2 * v)

cost <- 10000
nIters <- 0
while (cost > 1e-14) {
  u <-  w[1] - dEdu(w[1], w[2]) * learning.ratio
  v <-  w[2] - dEdv(w[1], w[2]) * learning.ratio
  w <- c(u, v)
  cost <- ErrorFunction(w[1], w[2])
  nIters <- nIters + 1
}
cat("Number of iterations until cost is below 1e-14:", nIters)
```

3. ¿Qué valores de $(u,v)$ obtuvo en el apartado anterior cuando alcanzó el error
de $10^{-14}$?

*El valor de $u=`r u`$ y el de $v=`r v`$*

- Considerar ahora la función $f(x,y) = x^2 + 2y^2 + 2\sin(2\pi x)\sin(2\pi y)$

1. Usar como valores iniciales $x_0 = 1$, $y_0 = 1$, la tasa de aprendizaje $\eta = 0.01$
y un máximo de $50$ iteraciones. Generar un gráfico de cómo desciende el valor de
la función con las iteraciones. Repetir el experimento pero usando $\eta = 0.1$,
comentar las diferencias.

```{r ex 1.1b}
# TODO: Reescribir documentación
GradientDescent <- function(initial.point = c(1, 1),
                            dEdx,
                            dEdy,
                            learning.ratio = 0.1,
                            maxIters = 100,
                            best.values = F) {
  # Compute gradient descent
  #
  # Args:
  #   initial.point:  The point from where to start
  #   dEdx: Partial derivative of function
  #   dEdy: Partial derivative of function
  #   learning.ratio: The ratio at which to learn
  #   maxIters: How many iterations as maximun
  #   best.values: If true, returned value includes best x,y and its error
  #
  #  Returns:
  #    A list with (cost, cost.history, x, y, nIters. [best])
  cost.current <- Inf
  nIters <- 0
  
  old.x <- dEdx(initial.point[1], initial.point[2])
  old.y <- dEdy(initial.point[1], initial.point[2])
  
  history <- c(abs(old.x + old.y))
  
  if (best.values == T) {
    cost.best <- Inf
  }
  
  while (nIters < maxIters) {
    new.x <-  old.x - dEdx(old.x, old.y) * learning.ratio
    new.y <-  old.y - dEdy(old.x, old.y) * learning.ratio
    
    cost.current <- abs(new.x + new.y)
    history <- c(history, cost.current)
    
    if (best.values == T && cost.current < cost.best) {
      best.x <- new.x
      best.y <- new.y
      cost.best <- cost.current
    }
    
    old.x <- new.x
    old.y <- new.y
    
    nIters <- nIters + 1
  }
  
  r <-
    list(theta = c(new.x, new.y),
         history = history[-1],
         iters = nIters)
  
  if (best.values == T) {
    r$best = list(x = best.x, y = best.y, cost = cost.best)
  }
  
  r
}

ErrorFunc <- function(x, y)
  x ^ 2 + 2 * y ^ 2 + 2 * sin(2 * pi * x) * (2 * pi * y)

dEdx <- function(x, y)
  2 * (2 * pi * cos(2 * pi * x) * sin(2 * pi * y) + x)
dEdy <- function(x, y)
  4 * (pi * sin(2 * pi * x) * cos(2 * pi * y) + y)

result <- mapply(
  GradientDescent,
  learning.ratio = c(0.01, 0.1),
  MoreArgs       = list(
    dEdx     = dEdx,
    dEdy     = dEdy,
    maxIters = 50
  )
)
```
```{r echo=F}
qplot(
  c(1:result[, 1]$iters),
  result[, 1]$history,
  geom = "line",
  xlab = "Iterations",
  ylab = "Error",
  main = "Learning rate = 0.01"
)
qplot(
  c(1:result[, 2]$iters),
  result[, 2]$history,
  geom = "line",
  xlab = "Iterations",
  ylab = "Error",
  main = "Learning rate = 0.1"
)
```

*Cuando $\eta = 0.01$, se avanza muy lentamente hacia el mínimo, esto tiene sus
pros y sus contras. Como pro nos aseguramos que no daremos un salto grande y nos
saldremos del mínimo hacia otro sitio. Como contra, si estamos en un mínimo local,
por la misma razón anterior, no saldremos de él y por tanto no encontraremos una 
buena solución.*

*En el caso de $\eta = 0.1$ nos encontramos en el caso de ir saltando de un lado
a otro, y por tanto el error fluctua mucho.*

- Obtener el valor mínimo y los valores de las variables que lo alcanzan cuando el
punto de inicio se fija: $(0.1, 0.1), (1, 1),(−0.5, −0.5),(−1, −1)$. Generar una tabla
con los valores obtenidos ¿Cuál sería su conclusión sobre la verdadera dificultad
de encontrar el mínimo global de una función arbitraria?

```{r}
result <- mapply(GradientDescent,
       learning.ratio = c(rep(.01, 4), rep(.1, 4)),
       initial.point = list(c(.1, .1), c(1,1), c(-.5, -.5), c(-1,-1)),
       MoreArgs = list(dEdx        = dEdx,
                       dEdy        = dEdy,
                       maxIters    = 50,
                       best.values = T))
```
```{r echo=F}
data <- data.frame(L.Rate = c(rep(.01, 4), rep(.1, 4)),
                   points = c("(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)",
                              "(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)"),
                   x      = c(result[,1]$best$x, result[,2]$best$x, result[,3]$best$x, result[,4]$best$x,
                              result[,5]$best$x, result[,6]$best$x, result[,7]$best$x, result[,8]$best$x),
                   y      = c(result[,1]$best$y, result[,2]$best$y, result[,3]$best$y, result[,4]$best$y,
                              result[,5]$best$y, result[,6]$best$y, result[,7]$best$y, result[,8]$best$y),
                   error  = c(result[,1]$best$cost, result[,2]$best$cost, result[,3]$best$cost, result[,4]$best$cost,
                              result[,5]$best$cost, result[,6]$best$cost, result[,7]$best$cost, result[,8]$best$cost)

        )
knitr::kable(data, caption = "Comparison between learning rates and starting points")
```

*Como se aprecia en la tabla, la tasa de aprendizaje que se acerga más a un mínimo
es $\eta = 0.1$ mientras que $\eta = 0.01$ al ir más lento, y con tan pocas iteraciones
no consigue acercarse al mínimo local. Aún habiendo encontrado un valor cercano 
a cero para $\eta = 0.1$, esto no quiere decir que estemos ante el mínimo global,
podríamos haber caido en un local.*

## Ejercicio 2

**Coordenada descendente**. En este ejercicio comparamos la eficiencia de la
técnica de optimización de “coordenada descendente” usando la misma función del ejercicio
1.1a. En cada iteración, tenemos dos pasos a lo largo de dos coordenadas. En el Paso-1
nos movemos a lo largo de la coordenada $u$ para reducir el error (suponer que se verifica
una aproximación de primer orden como en gradiente descendente), y el Paso-2 es para
reevaluar y movernos a lo largo de la coordenada $v$ para reducir el error ( hacer la misma
hipótesis que en el paso-1). Usar una tasa de aprendizaje $\eta = 0.1$.

```{r Coordinate Descent}
nIters <- 0
w <- c(1,1)
learning.ratio <- 0.1
cost <- 0
while (nIters < 15) {
  u <-  w[1] - dEdu(w[1], w[2]) * learning.ratio
  w[1] <- u
  v <-  w[2] - dEdv(w[1], w[2]) * learning.ratio
  w[2] <- v
  cost <- ErrorFunction(w[1], w[2])
  nIters <- nIters + 1
}
```

*En este caso se obtiene un valor mucho peor para $E(u,v)$, en concreto $`r cost`$.
Mientras que con Gradiente Descendente, en 10 iteraciones ya obtenemos un valor 
en torno a los $10^{-15}$, valor que dista mucho del que se obtiene con Coordenada
descendente, el cual se queda muy lejos del óptimo.*

*Este método tiene problemas cuando las funciones no son suaves, ya que se queda
estancado en puntos no estacionarios cuando la curva de la función no es suave.
Sin embargo, cuando se tienen funciones no derivables o difíciles de derivar, 
gradiente descendente no se puede aplicar, por lo que es recomendable usar este otro método.*


## Ejercicio 3

Método de Newton Implementar el algoritmo de minimización de Newton y aplicarlo
a la función $f(x, y)$ dada en el ejercicio 1b. Desarrolle los mismos experimentos
usando los mismos puntos de inicio.

```{r Newton\'s method}
dfdx2 <- function(x) 
  2 - 8 * pi ^ 2 * sin(2 * pi * x[1]) * sin(2 * pi * x[2]) #dx
dfdy2 <- function(x) 
  4 - 8 * pi ^ 2 * sin(2 * pi * x[1]) * sin(2 * pi * x[2]) #dy

Newton <- function(initial.point = c(1,1), 
                   dEdx,
                   dEdy,
                   dfdx2,
                   dfdy2,
                   maxIters = 100) {
  # Newton's Method implementation
  #
  # Args:
  #   initial.point:  The point from where to start
  #   dEdx,dEdy:  Partial derivative of function
  #   dfdx2,dfdx2:  Second derivative of function
  #   maxIters: How many iterations as maximun
  #   
  #  Returns:
  #    A list with (theta, cost.history, nIters)
  nIters <- 0
  
  old.values <- matrix(c(dEdx(initial.point[1], initial.point[2]),
                         dEdy(initial.point[1], initial.point[2])))
  
  history <- c(sum(old.values))
  threshold <- F
  
  while (nIters < maxIters && !threshold) {
    
    inv.diag <- dfdx2(old.values) + dfdy2(old.values)
    
    H <- matrix(c(dfdx2(old.values),
                  inv.diag,
                  inv.diag,
                  dfdy2(old.values)), 2, 2, byrow = T)
    
    # duv <- svd(H)
    # H.inv <- duv$v %*% diag(1 / duv$d) %*% t(duv$u)
    H.inv <- solve(H)
    
    new.values <- -H.inv %*% old.values
    
    cost.current <- abs(sum(new.values))
    history <- c(history, cost.current)
    
    if (norm(abs(old.values - new.values), type = "F") < 1e-5)
      threshold <- T
  
    old.values <- new.values
    nIters <- nIters + 1
  }
  
  r <- list(theta = c(new.values[1], new.values[2]), history = history[-1], iters = nIters)
  r
}

result <- mapply(Newton,
                 initial.point = list(c(.1, .1), c(1,1), c(-.5, -.5), c(-1,-1)),
                 MoreArgs =  list(dEdx = dEdx, dEdy = dEdy, dfdx2 = dfdx2, dfdy2 = dfdy2, maxIters = 50))

```


```{r echo=F}
p1 <- qplot(c(1:result[,1]$iters), result[,1]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (.1, .1)")
p2 <- qplot(c(1:result[,2]$iters), result[,2]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (1, 1)")
p3 <- qplot(c(1:result[,3]$iters), result[,3]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (-.5, -.5)")
p4 <- qplot(c(1:result[,4]$iters), result[,4]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (-1, -1)")

grid.arrange(p1,p2,p3,p4, nrow = 2, ncol = 2, top = "Results of Newton's Method", bottom = "It computes a direct solution")
```

*Como observamos en las gráicas, el método de Newton calcula de forma directa el
mínimo de la función. En este caso se ha cortado la ejecución cuando la diferencia
entre la mejora estaba en torno a $10^{-5}$, ya que salvo en el primer caso, se 
obtiene el mínimo en la segunda iteración.*

## Ejercicio 4

Regresión Logística: En este ejercicio crearemos nuestra propia función
objetivo $f$ (probabilidad en este caso) y nuestro conjunto de datos $\mathcal{D}$
para ver cómo funciona regresión logística. Supondremos por simplicidad que $f$
es una probabilidad con valores $0/1$ y por tanto que y es una función determinista
de $\mathbf{x}$.

```{r Logistic regression}
SGD <- function(data,
                h,
                dh,
                tolerance = 0.01,
                learning.rate = 0.01,
                how.many.epochs = F) {
  # Stochastic gradient descent
  #
  # Args:
  #   data: The data, the last columns contains the labels
  #   h: Error function to minimize
  #   dh: Gradient of the error function
  #   tolerance: When SGD should stop searching for the minimun
  #   learning.rate: Learning rate when updating the weights
  #   how.many.epochs: If true returns also how many epochs took to converge
  #
  #  Returns:
  #    Final weights and how many epochs took to converge, if specified
  
  # Create a new enviroment to place w so that update function throught apply can modify it
  w.env <- new.env()
  w.env$w <- matrix(rep(0,3))
  above.tolerance <- T
  
  update <- function(x) {
    ## In order to use apply, we need x containing also the label (in the last column)
    y <- as.vector(x[4])
    x <- as.vector(x[-4])
    gradient <- dh(y %*% t(x), y * (t(w.env$w) %*% x))
    w.env$w <- w.env$w - learning.rate * gradient
  }
  
  if (how.many.epochs == T) {
    nEpochs <- 0
  }
  while (above.tolerance) {
    
    w.old <- w.env$w
    # We only need the last element of w, as it is the last update
    apply(data, 1, update)
    
    # Apply a permutation to the data in each epoch
    data <- data[sample(nrow(data)),]

    if (how.many.epochs == T) {
      nEpochs <- nEpochs + 1
    }
    
    if (norm(w.old - w.env$w, type = "F") < tolerance) {
      above.tolerance <- F
    }
  }
  
  if (how.many.epochs) {
    list(w = w.env$w, epochs = nEpochs)
  } else 
    w.env$w
}
# Generate the DataSet
line <- GetLine()
data <- NewDataSet(100, line = line$points)
```
```{r echo=F}
label <- factor(as.integer(data$y))
ggplot(data = as.data.frame(data$x[,-1]), aes(V1, V2)) + geom_point(aes(colour = label)) + theme_bw()
```
```{r}
# Error function
h <- function(x) log(1 + exp(as.double(-x)))
# Gradient if error function
dh <- function(yx, ywx) t(-yx/(1 + exp(as.double(ywx))))

# Prepare the data for SGD
sgd.data <- cbind(data$x, as.vector(data$y))
w <- SGD(sgd.data, h, dh)
rm(sgd.data)
```

*La hipótesis que obtenemos con el algoritmo (El vector de pesos) es el siguiente:*

$$w = 
\begin{bmatrix}
`r w[1]` \\ 
`r w[2]` \\
`r w[3]`
\end{bmatrix}
$$

```{r}
cross.entropy.error <- mean(sapply(data$y * (t(w) %*% t(data$x)), h))
# Compute the cross entropy error, with new data for our hypothesis
data <- NewDataSet(1000000, line = line$points)
cross.entropy.error <- mean(sapply(data$y * (t(w) %*% t(data$x)), h))
```

*Para estimar $E_{out}$ usamos el error de entropía cruzada, el cual es el siguiente
(Para una muestra nueva de $1000000$ puntos):*

$$E_{out} = `r cross.entropy.error`$$

```{r Replicate 100 times}
data <- NewDataSet(100)
sgd.data <- cbind(data$x, as.vector(data$y))
cross.entropy.error <- replicate(100, {
  w <- SGD(sgd.data, h, dh, how.many.epochs = T)
  error <- sum(parSapply(cl, data$y * (t(w$w) %*% t(data$x)), h))/nrow(data$x)
  
  c(epochs = w$epochs, error = error)
})
rm(sgd.data)
stopCluster(cl)
```

*Si replicamos el experimento 100 veces usando funciones de frontera distintas 
cada vez, para un conjunto de 100 datos, el valor de $E_{out}$ medios es:*

$$E_{out} = `r mean(cross.entropy.error["error",])`$$

*En cuanto a la media de épocas necesarias para converger, el valor es 
$`r mean(cross.entropy.error["epochs",])`$*


## Ejercicio 5

Clasificación de Dígitos. Considerar el conjunto de datos de los dígitos
manuscritos y seleccionar las muestras de los dígitos 1 y 5. Usar los ficheros de entrenamiento
(training) y test que se proporcionan. Extraer las características de intensidad promedio
y simetría en la manera que se indicó en el ejercicio 3 del trabajo 1.

Plantear un problema de clasificación binaria que considere el conjunto de entrenamiento
como datos de entrada para aprender la función g. Usando el modelo de Regresión Lineal para
clasificación seguido por PLA-Pocket como mejora. Responder a las siguientes cuestiones.

```{r Digit Classification}
ReadDigits <- function(dataset){
  # Reads and prepare the digits dataset
  digits.traninig <- read.table(dataset,
                       quote = "",
                       comment.char = "",
                       stringsAsFactors = F)
  
  digits.traninig <- digits.traninig[digits.traninig$V1 == 1 | digits.traninig$V1 == 5, ]
  labels.training <- digits.traninig[,1]
  digits.traninig <- digits.traninig[,-1]
  digits.traninig <- lapply(1:nrow(digits.traninig), function(i){matrix(digits.traninig[i, ], nrow = 16)})
  
  # Compute mean and symmetry
  digits.mean <- sapply(1:length(digits.traninig), function(i) mean(as.double(digits.traninig[[i]])))
  
  # Compute vertical simmetry
  digits.inverted <- lapply(1:length(digits.traninig),
                          function(i) digits.traninig[[i]][, 16:1])
  digits.vertical.symmetry <- sapply(1:length(digits.traninig),
                                   function(i) -sum(abs(as.numeric(digits.traninig[[i]])
                                               - as.numeric(digits.inverted[[i]]))))
  
  # Preparing data to apply Linear Regression
  digits.traninig <- data.frame(intensity = digits.mean, symmetry = digits.vertical.symmetry)
  
  list(digits = digits.traninig, labels = labels.training)
}

ds <- ReadDigits("./datos/DigitosZip/zip.train")
labels.training <- ds$labels
digits.traninig <- ds$digits
rm(ds)

g <- lm(formula = labels.training ~ ., digits.traninig)

line.g <- GetInterceptAndSlope(g$coefficients)

# Prerare data for Pocket
labelscopy <- labels.training
labelscopy[labelscopy == 1] <- 1
labelscopy[labelscopy == 5] <- -1

# Run pocket with starting point from Linear Regression
g2 <- AjustaPla(digits.traninig, label = labelscopy, vini = g$coefficients, memory = T)
line.g2 <- unlist(GetInterceptAndSlope(g2[,-4]))
```

*En este ejercicio hemos ejecutado una regresión lineal sobre los datos para 
obtener un vector de pesos inicial, en este caso se ha calculado el siguiente:*

$$w_{lm} = 
\begin{bmatrix}
`r g$coefficients[1]` \\ 
`r g$coefficients[2]` \\
`r g$coefficients[3]`
\end{bmatrix}
$$

*Hecho esto, le pasamos este vector de pesos al algoritmo pocket, implementado 
en la práctica 1. Y nos calculó el siguiente vector de pesos*:

$$w_{pocket} = 
\begin{bmatrix}
`r g2[1]` \\ 
`r g2[2]` \\
`r g2[3]`
\end{bmatrix}
$$

*A contunuación se muestran dos gráficas con la función __g__ calculada para los
datos de training y test:*

```{r Visualización de datos, echo=F}
plot2 <- ggplot(digits.traninig, aes(intensity, symmetry)) + ggtitle("Train") +
  geom_point(aes(colour = factor(as.integer(labels.training))), shape = 19) + theme_bw() +
  geom_abline(mapping = aes(slope = line.g$slope, intercept = line.g$intercept, colour = "Regression")) +
  geom_abline(mapping = aes(intercept = -line.g2[1], slope = -line.g2[2], colour = "Pocket")) +
  scale_colour_manual(name = "Legend", values = c(1,2,3,4), guide = guide_legend(override.aes = list(linetype = c(rep("blank", 2), "solid", "solid"), shape = c(rep(19, 2), NA, NA))))
plot2

ds <- ReadDigits("./datos/DigitosZip/zip.test")
digits.test <- ds$digits
labels.test <- ds$labels
labelscopy.test <- labels.test
labelscopy.test[labelscopy.test == 1] <- 1
labelscopy.test[labelscopy.test == 5] <- -1
rm(ds)

plot2 <- ggplot(digits.test, aes(intensity, symmetry)) + ggtitle("Train") +
  geom_point(aes(colour = factor(as.integer(labels.test))), shape = 19) + theme_bw() +
  geom_abline(mapping = aes(slope = line.g$slope, intercept = line.g$intercept, colour = "Regression")) +
  geom_abline(mapping = aes(intercept = -line.g2[1], slope = -line.g2[2], colour = "Pocket")) +
  scale_colour_manual(name = "Legend", values = c(1,2,3,4), guide = guide_legend(override.aes = list(linetype = c(rep("blank", 2), "solid", "solid"), shape = c(rep(19, 2), NA, NA))))
plot2
```

*Pasamos ahora a calcular los errores en los datos de entrenamiento y test ($E_{in}$ y 
$E_{test}$, respectivame)*

```{r "E_in E_out"}
w.pocket <- as.matrix(g2[-4])
x.training <- cbind(rep(1, nrow(digits.traninig)), digits.traninig)
# Compute in sample error
e.in <- sum(sign(w.pocket %*% t(x.training)) != labelscopy)

x.test <- cbind(rep(1, nrow(digits.test)), digits.test)
# Compute the out of sample error
e.test <- sum(sign(w.pocket %*% t(as.matrix(x.test))) != labelscopy.test)
```

*Como se puede apreciar en ambas gráficas, el error tanto dentro como fuera de la 
muestra es de $`r e.in`$ dígitos mal clasificados para $E_{in}$ y $`r e.test`$ para $E_{test}$*

*Calcularemos ahora las cotas sobre el verdadero valor de $E_{out}, una basada
en $E_{in}$ y otra en $E_{test}$, con una tolerancia $\delta = 0.05$, la fórmula es:*.

$$E_{out}(g)  \leq E_{in}(g) + \sqrt{\frac{8}{N}\ln\left(\frac{4(2N)^{d_{vc}}+ 1}{\delta}\right )}$$

*En el caso del perceptrón, la dimensión $VC$ es 3.*

```{r Ex5.c real eout}
N <- nrow(x.training)
vc.generalization.bound.for.e.in <- sqrt(8/N * log((4 * (2*N) ^ 3 + 1) / 0.05))
N <- nrow(x.test)
vc.generalization.bound.for.e.test <- sqrt(8/N * log((4 * (2*N) ^ 3 + 1) / 0.05))
```

*Dado el valor de $\delta$, con una confianza de más del $95\%$ tenemos que*

$$E_{out}(g)  \leq E_{in}(g) + `r vc.generalization.bound.for.e.in`$$

lo cual quiere decir que incluso haciendo $E_{in}$ lo más cercana a cero posible, 
$E_{out}$ estará acotada por $`r vc.generalization.bound.for.e.in`$, lo cual no es
un buen valor (aproximádamente un $60\%$). 

Sin embargo en el caso de $E_{test}$ es aún peor, ya que tenemos

$$E_{out}(g)  \leq E_{in}(g) + `r vc.generalization.bound.for.e.test`$$

lo cual no nos aporta ninguna información. Es posible que este resultado tan malo
se deba a los pocos datos que tenemos para test junto con que el error tanto para 
$E_{in}, E_{test}$ es el mismo ($`r e.in`$ y $`r e.test`$ puntos mal clasificados). 
Sin embargo si tenemos en cuenta la cantidad de datos, para test es mucho más alto
el error relativo ($`r (e.test * 100)/nrow(x.test)`\%$) mientras que para entrenamiento
es del $`r (e.in * 100)/nrow(x.training)`\%$

# SobreAjuste

## Ejercicio 1

```{r}
GenerateLegendreDataset <- function(Qf, N, sigma) {
  # Generate a data set from Legendre polynomials of degree Qf and size N
  #
  # Args:
  #   Qf: Degree of the polynomial that creates the datase
  #   N: Size of the DataSet
  #   sigma: Used to generate noise
  #
  #  Returns:
  #    The Data Set

  # Generate Data
  x <- matrix(runif(N, -1, 1))
  # Legendre Polynomials
  normalized.p.list <- legendre.polynomials(Qf, normalized = T)

  # L. Polynomials with coefficients
  lY =  sapply(x, function(x)
    polynomial.values(normalized.p.list, x))
  fn = matrix(unlist(lY), nrow = N, byrow = T)

  # Generate the coefficients
  aux_nor <- 0:Qf
  aux_nor <- sqrt(sum(1 / (2 * aux_nor + 1)))
  coeff_a <- rnorm(Qf + 1, sigma)
  coeff_a <- coeff_a / aux_nor   # coeficientes normalizados

  cost.function <- function(L, coeff) {
    if (!is.matrix(L))
      L <- as.matrix(L)
    if (!is.vector(coeff))
      coeff <- as.vector(coeff)
    L %*% coeff
  }

  fn <- cost.function(fn, coeff_a)
  # Add Noise to data
  yn <- fn + sqrt(sigma) * rnorm(N)

  data.frame(x = x, y = yn)
}

# Generate two datasets, one from Legendre(10) and other Legendre(2)
X.10.degree <- GenerateLegendreDataset(50, 100, 1)
X.2.degree <- GenerateLegendreDataset(2, 100, 1)

model.g2 <- lm(y ~ poly(x, degree = 2), data = X.10.degree)
model.g10 <- lm(y ~ poly(x, degree = 10), data = X.10.degree)
model.g2.from.degree2 <- lm(y ~ poly(x, degree = 2), data = X.2.degree)
model.g10.from.degree2 <- lm(y ~ poly(x, degree = 10), data = X.2.degree)

X.10.degree <- cbind(X.10.degree, predicted.from.g2 = predict(model.g2))
X.10.degree <- cbind(X.10.degree, predicted.from.g10 = predict(model.g10))
X.2.degree <- cbind(X.2.degree, predicted.from.g2 = predict(model.g2.from.degree2))
X.2.degree <- cbind(X.2.degree, predicted.from.g10 = predict(model.g10.from.degree2))
```

*Hemos generado un DataSet a partir de los polinomios de Legendre de grado 10, a
partir de estos datos intentaremos predecir con dos modelos, uno basado en polinomios
de grado 2 y otro de grado 10. A estas hipótesis las llamaremos $g_2$ y $g_{10}$.
El resultado de ajustar los modelos es el siguiente:*

```{r models }
summary(model.g2)
summary(model.g10)
```

*Veamos el resultado de estas hipótesis cuando los datos se crearon originalmente
con un polinomio de Legendre de grado 10:*

```{r echo=F}
p <- (ggplot(X.10.degree, aes(x = x, y = y)) 
    + ggtitle("Data from a 50 Degree Legendre Polynomial")      
    + geom_point()
    + theme_bw())

(p + geom_line(aes(y = X.10.degree$predicted.from.g2, colour = "2nd Order Fit"), size = 1)
   + geom_line(aes(y = X.10.degree$predicted.from.g10, colour = "10th Order Fit"), size = 1)
   + scale_color_discrete(name = "Polynomial fits") )
```

*Y cuando lo hacemos con datos generados originalmente con un polinomio de Legendre
de grado 2:*

```{r echo=F}
p <- (ggplot(X.2.degree, aes(x = x, y = y)) 
    + ggtitle("Data from a 2 Degree Legendre Polynomial")      
    + geom_point()
    + theme_bw())

(p + geom_line(aes(y = X.2.degree$predicted.from.g2, colour = "2nd Order Fit"), size = 1)
   + geom_line(aes(y = X.2.degree$predicted.from.g10, colour = "10th Order Fit"), size = 1)
   + scale_color_discrete(name = "Polynomial fits"))
```

*A partir de ahora el análisis realizado será sobre los datos generados a partir 
del polinomio de Legendre de grado 10, cuyos coeficientes para cada una de las 
estimaciones son:*

$$w_{g_2} = 
\begin{bmatrix}
`r model.g2$coefficients[1]` \\ 
`r model.g2$coefficients[2]` \\
`r model.g2$coefficients[3]`
\end{bmatrix}
$$

$$w_{g_{10}} = 
\begin{bmatrix}
`r model.g10$coefficients[1]` \\ 
`r model.g10$coefficients[2]` \\
`r model.g10$coefficients[3]` \\
`r model.g10$coefficients[4]` \\
`r model.g10$coefficients[5]` \\
`r model.g10$coefficients[6]` \\
`r model.g10$coefficients[7]` \\
`r model.g10$coefficients[8]` \\
`r model.g10$coefficients[9]` \\
`r model.g10$coefficients[10]` \\
`r model.g10$coefficients[11]`
\end{bmatrix}
$$

*El hecho de normalizar los datos es necesario para que el ruido ($\sigma^2$) tenga
sentido. Es decir, para obtener un ratio significativo entre el valor de la función
y el ruido.*


*Veamos ahora que pasa si fijamos $Q_f = 20, N = 50, \sigma = 1$ y ejecutamos 100
veces*

```{r 2.2}
Qf <- 20
N <- 50
sigma2 <- 1

# Generate two datasets, one from Legendre(10) and other Legendre(2)
X.Qf.degree <- GenerateLegendreDataset(Qf, N, sigma2)

model.g2 <- lm(y ~ poly(x, degree = 2), data = X.Qf.degree)
model.g10 <- lm(y ~ poly(x, degree = 10), data = X.Qf.degree)

result <- replicate(100, {

  ds <- GenerateLegendreDataset(Qf = Qf, N = N, sigma = sigma2)
  
  # Compute by hand E_out as $\frac{1}{N}\sum_1^N(w^t Z_n - y_n)^2$
  w.10 <- as.vector(model.g10$coefficients)
  w.2 <- as.vector(model.g2$coefficients)
  
  z2 <- data.frame(rep(1, N), ds$x, ds$x^2)
  z2 <- as.matrix(z2)
  z10 <- data.frame(rep(1, N), ds$x, ds$x^2, ds$x^3, ds$x^4, ds$x^5, ds$x^6, ds$x^7, ds$x^8, ds$x^9, ds$x^10)
  z10 <- as.matrix(z10)
  
  eout.w10 <- mean((w.10 %*% t(z10) - ds$y)^2)
  eout.w2 <- mean((w.2 %*% t(z2) - ds$y)^2)
  
  # Compute the error of our models
  predicted2 <- predict(model.g2, newdata = ds, se.fit = T)
  predicted10 <- predict(model.g10, newdata = ds, se.fit = T)
  
  eout.residuals2 <- mean(predicted2$se.fit)
  eout.residuals10 <- mean(predicted10$se.fit)
  
  list(eout.w10 = eout.w10,
       eout.w2 = eout.w2,
       eout.residuals2 = eout.residuals2,
       eout.residuals10 = eout.residuals10)
})
eout.H10 <- mean(unlist(result[1,]))
eout.H2 <- mean(unlist(result[2,]))
eout.H2.residuals <- mean(unlist(result[3,]))
eout.H10.residuals <- mean(unlist(result[4,]))
```

*Tras probar con estos datos, obtenemos un $E_{out}(\mathcal{H_2}) = `r eout.H2`$
y un $E_{out}(\mathcal{H_{10}}) = `r eout.H10`$. Es de esperar un valor mucho peor
para la hipótesis de $\mathcal{H_{10}}$, ya que estamos forzando mucho más el sobre
ajuste. Aunque el error dentro de la muestra sea mucho menor con respecto a $\mathcal{H_2}$,
cuando probamos muestras nuevas se obtienen resultados mucho peores al haber puntos
bastante mal clasificados que hacen que suba el error, como se puede apreciar en
la gráfica:*

```{r echo=F}
ds <- GenerateLegendreDataset(Qf = Qf, N = N, sigma = sigma2)
p <- (ggplot(ds, aes(x = x, y = y)) 
    + ggtitle("Data from a Qf = 20 Degree Legendre Polynomial")      
    + geom_point()
    + theme_bw())

ds <- cbind(ds, g2 = predict(model.g2), g10 = predict(model.g10))

(p + geom_line(aes(y = ds$g2, colour = "2nd Order Fit"), size = 1)
   + geom_line(aes(y = ds$g10, colour = "10th Order Fit"), size = 1)
   + scale_color_discrete(name = "Polynomial fits"))
```

*Como vemos, los picos del polinomio de grado $10$ perjudican mucho al error.*

*Definamos una medida de sobreajuste *

$$E_{out}(\mathcal{H_10}) - E_{out}(\mathcal{H_2})$$

*Con los datos actuales, esta medida es $`r eout.H10 - eout.H2`$. Si la diferencia
de esta operación es muy positiva ($>>0$), quiere decir que el error fuera de la
muestra para $H_{10}$ es muy alto, y por tanto hemos estado sobre ajustando dentro
de la muestra.*

# Regularización

## Ejercicio 1

```{r Weight Decay}
N <- 50
d <- 3
x <- matrix(rnorm(d * N), ncol = d)
x <- cbind(rep(1, N), x)
w.f <- matrix(rnorm(d + 1))
sigma <- .5
# y <- wf %*% x + sigma * epsilon
y <- t(w.f) %*% t(x) +  sqrt(sigma) * rnorm(N)
ds <- as.data.frame(cbind(x[, -1], y = as.vector(y)))
lambda <- .05 / N

lm.ridge.fit <- lm.ridge(y ~ ., ds, lambda = lambda)

# Predicted values
y.fit <- t(as.matrix(lm.ridge.fit$coef)) %*% t(as.matrix(x[, -1]))

# glm.fit <- glmnet(x, y, alpha = 0, lambda = lambda)
# w.reg <- coef(glm.fit)

error.function <- function(yn, y.fit) {
  # Error function
  #
  # Args:
  #   yn: Real y value
  #   y.fit: Predicted y value
  #
  #  Returns:
  #   Error
  abs(yn - y.fit)
}

Cross.Validation <- function(data) {
  # Cross Validation
  #
  # Args:
  #   data: Data
  #  Returns:
  #    The cross validation error
  N <- length(data)
  d <- 3
  x <- matrix(rnorm(d * N), ncol = d)
  x <- cbind(rep(1, N), x)
  w.f <- matrix(rnorm(d + 1))
  sigma <- .5
  # y <- wf %*% x + sigma * epsilon
  y <- t(w.f) %*% t(x) + sqrt(sigma) * rnorm(N)
  ds <- as.data.frame(cbind(x[, -1], y = as.vector(y)))
  lambda <- .05 / N

  x <- x[,-1]

  cv.error <- sapply(seq_along(1:length(data)), function(i) {
    # Compute cross validation with leave one out
    # Adjust the model
    lm.ridge.fit <- lm.ridge(y ~ ., ds[-i,], lambda = lambda)
    # Predicted values with the validation point
    y.fit <-
      t(as.matrix(lm.ridge.fit$coef)) %*% x[i,]
    error.function(y[,i], y.fit)
  })

  cv.error
}

# Generate a list of vectors each one with the lengt of each data set we want.
N <- sapply(seq(d + 15, 125, 10), function(x)
  seq(1:x))

cross.validation.errors <- sapply(N, Cross.Validation)
# Compute $E_{cv}$
e.cv <- sapply(cross.validation.errors, mean)
```

*Tras calcular el error de validación cruzada, calculamos la media de cada uno 
de ellos para obtener $E_{cv}$*

$$E_{cv} = 
\begin{bmatrix}
`r e.cv[1]` \\
`r e.cv[2]` \\
`r e.cv[3]` \\
`r e.cv[4]` \\
`r e.cv[5]` \\
`r e.cv[6]` \\
`r e.cv[7]` \\
`r e.cv[8]` \\
`r e.cv[9]` \\
`r e.cv[10]` \\
`r e.cv[11]`
\end{bmatrix}
$$

*Esto corresponde al $E_{cv}$ correspondiente a cada uno de los subconjunto que 
se habían generado, ${d + 15, \dots , d + 115}$, la media de todos estos errores
es:*

$$E_{cv} = `r mean(e.cv)`$$

```{r 1.b}
# Replicate the above experiment 10^3 times
run.x.times <- 1000
result <- replicate(run.x.times, {
  
  cross.validation.errors <- sapply(N, Cross.Validation)
  
  e1 <- unlist(cross.validation.errors[1])
  e2 <- unlist(cross.validation.errors[2])
  
  # Compute $E_{cv}$
  e.cv <- sapply(cross.validation.errors, mean)
  
  list(e1 = mean(e1),
       e2 = mean(e2),
       e.cv = mean(e.cv),
       e1.var = var(e1),
       e2.var = var(e2),
       e.cv.var = var(e.cv)
  )
})

result <- apply(matrix(unlist(result), ncol = run.x.times), 1, mean)
```

*Repitiendo el anterior experimento $10^3$ veces obtenemos los siguientes valores
para la media y varianza de $e_1, e_2, E_{cv}$:*

|      | $e_1$         | $e_2$         | $E_{cv}$      |
|------|---------------|---------------|---------------|
| mean | `r result[1]` | `r result[2]` | `r result[3]` |
| var  | `r result[4]` | `r result[5]` | `r result[6]` |


*La relación entre los promedios de $e_1$ y el de los valores de $E_{cv}$ es que
deben ser próximos. Inicialmente uno podría pensar que al dejar sólamente una muestra
para validación debería de dar una estimación pobre sobre $E_{cv}$, sin embargo 
al repetir la validación cruzada (_Leave one out_) $N$ veces, y estimando el modelo
en cada iteración (Y por tanto obteniendo una nueva hipótesis cada vez), nos
vamos acercando al valor real de estimación fuera de la muestra. Lo mismo ocurre para $e_2$.*

*Como resultado de todo esto, tenemos que el error de validación cruzada tiene a 
ser cercano al de fueda de la muestra, es decir $E_{out}\approx E_{cv}$*

*Con respecto a qué contribuye a la varianza de los valores de $e_1$ podríamos
atribuirlo al ruido. Es posible que la estimación del punto de validación quede 
muy alejada con respecto a su valor real, y por ello se obtiene una varianza alta.
Además, los outliers nos van a causar que la varianza aumente. Cuando coincida
que el punto que dejamos para validación sea un outlier, obtendremos una varianza
mayor.*

*Si los errores de validación cruzada fueran independientes, *

*Calculemos ahora el número efectivo de muestras nuevas usadas en el cálculo de $E_{cv}$ como $\frac{var(e_1)}{var(E_{cv})}$*

```{r 1.f}
neff <- result[4]/result[6]

sapply(cross.validation.errors, var)/var(e.cv)
```


# Cuando toque interpretar el sobre ajuste, los gráficos del tema 4
