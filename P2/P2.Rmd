---
title: "Aprendizaje Automático - Práctica 2"
author: "Alejandro Alcalde"
date: "17/04/2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output: 
  # md_document:
  #   variant: markdown_github
  #   toc: true
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: zenburn
    # keep_tex: yes
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">>")
set.seed(1000000007)

# Función para verificar si un paquete está instalado o no
is.installed <- function(paquete) is.element(
  paquete, installed.packages())

if (!is.installed("ggplot2"))
  install.packages("ggplot2", dependencies = T)
if (!is.installed("gridExtra"))
  install.packages("gridExtra", dependencies = T)

library(ggplot2)
library(gridExtra)  # for presenting plots side by side
```

# Modelos Lineales
 
## Ejercicio 1

Gradiente Descendente. Implementar el algoritmo de gradiente descendiente.

- Considerar la función no lineal de error $E(u,v) = (ue^v - 2ve^{-u})^2$. Usar
graciente descendente y minimizar esta función de error, comenzando desde el punto
$(u,v) = (1,1)$ y usando una tasa de aprendizaje $\eta=.1$

1. Calcular analíticamente y mostrar la expresión del gradiente de la función 
  de error. 
$$\frac{\partial}{\partial u}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2v)(e^{u+v}+2v)$$
$$\frac{\partial}{\partial v}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2)(ue^{u+v}-2 v)$$

2. ¿Cuantas iteraciones tarda el algoritmo en obtener por primera vez un valor
de  $E(u,v)$ inferior a $10^{-14}$?.

```{r Ex 1.1a Gradient Descent}
learning.ratio <- 0.1
w <- as.double(c(1, 1))

ErrorFunction <- function(u,v) 
  (u * exp(v) - 2 * v * exp(-u)) ^ 2
dEdu <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2 * v) * (exp(u + v) + 2 * v)
dEdv <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2) * (u * exp(u + v) - 2 * v)

cost <- 10000
nIters <- 0
while (cost > 1e-14) {
  u <-  w[1] - dEdu(w[1], w[2]) * learning.ratio
  v <-  w[2] - dEdv(w[1], w[2]) * learning.ratio
  w <- c(u,v)
  cost <- u + v
  nIters <- nIters + 1
}
cat("Number of iterations until cost is below 1e-14:", nIters)

```

3. ¿Qué valores de $(u,v)$ obtuvo en el apartado anterior cuando alcanzó el error
de $10^{-14}$?

*El valor de $u=`r u`$ y el de $v=`r v`$*

- Considerar ahora la función $f(x,y) = x^2 + 2y^2 + 2\sin(2\pi x)\sin(2\pi y)$

1. Usar como valores iniciales $x_0 = 1$, $y_0 = 1$, la tasa de aprendizaje $\eta = 0.01$
y un máximo de $50$ iteraciones. Generar un gráfico de cómo desciende el valor de
la función con las iteraciones. Repetir el experimento pero usando $\eta = 0.1$,
comentar las diferencias.

```{r ex 1.1b}
# TODO: Reescribir documentación
GradientDescent <- function(initial.point = c(1,1), 
                            dEdx,
                            dEdy,
                            learning.ratio = 0.1,
                            maxIters = 100,
                            best.values = F) {
  # Compute gradient descent
  #
  # Args:
  #   initial.point:  The point from where to start
  #   dEdx: Partial derivative of function
  #   dEdy: Partial derivative of function
  #   learning.ratio: The ratio at which to learn
  #   maxIters: How many iterations as maximun
  #   best.values: If true, returned value includes best x,y and its error
  #   
  #  Returns:
  #    A list with (cost, cost.history, x, y, nIters. [best])
  cost.current <- Inf
  nIters <- 0
  
  old.x <- dEdx(initial.point[1], initial.point[2])
  old.y <- dEdy(initial.point[1], initial.point[2])
  
  history <- c(old.x + old.y)
  
  if (best.values == T) {
    cost.best <- Inf
  }
  
  while (nIters < maxIters) {
    
    new.x <-  old.x - dEdx(old.x, old.y) * learning.ratio
    new.y <-  old.y - dEdy(old.x, old.y) * learning.ratio
        
    cost.current <- abs(new.x + new.y)
    history <- c(history, cost.current)
    
    if (best.values == T && cost.current < cost.best) {
      best.x <- new.x
      best.y <- new.y
      cost.best <- cost.current
    }
    
    old.x <- new.x
    old.y <- new.y
    
    nIters <- nIters + 1
  }
  
  r <- list(theta = c(new.x, new.y), history = history[-1], iters = nIters)

  if (best.values == T) {
    r$best = list(x = best.x, y = best.y, cost = cost.best)
  }
  
  r
}

ErrorFunc <- function(x, y)
  x ^ 2 + 2 * y ^ 2 + 2 * sin(2 * pi * x) * (2 * pi * y)

dEdx <- function(x, y)
  2 * (2 * pi * cos(2 * pi * x) * sin(2 * pi * y) + x)
dEdy <- function(x, y)
  4 * (pi * sin(2 * pi * x) * cos(2 * pi * y) + y )

result <- mapply(GradientDescent, 
       learning.ratio = c(0.01, 0.1), 
       MoreArgs = list(dEdx     = dEdx,
                       dEdy     = dEdy,
                       maxIters = 50))

qplot(c(1:50), result[,1]$history, 
      geom = "line", xlab = "Iterations", ylab = "Error", main = "Learning rate = 0.01")
qplot(c(1:50), result[,2]$history, 
      geom = "line", xlab = "Iterations", ylab = "Error", main = "Learning rate = 0.1")
```

*Cuando $\eta = 0.01$, se avanza muy lentamente hacia el mínimo, esto tiene sus
pros y sus contras. Como pro nos aseguramos que no daremos un salto grande y nos
saldremos del mínimo hacia otro sitio. Como contra, si estamos en un mínimo local,
por la misma razón anterior, no saldremos de él y por tanto no encontraremos una 
buena solución.*

*En el caso de $\eta = 0.1$ nos encontramos en el caso de ir saltando de un lado
a otro, y por tanto el error fluctua mucho.*

- Obtener el valor mínimo y los valores de las variables que lo alcanzan cuando el
punto de inicio se fija: $(0.1, 0.1), (1, 1),(−0.5, −0.5),(−1, −1)$. Generar una tabla
con los valores obtenidos ¿Cuál sería su conclusión sobre la verdadera dificultad
de encontrar el mínimo global de una función arbitraria?

```{r}
result <- mapply(GradientDescent,
       learning.ratio = c(rep(.01, 4), rep(.1, 4)),
       initial.point = list(c(.1, .1), c(1,1), c(-.5, -.5), c(-1,-1)),
       MoreArgs = list(dEdx        = dEdx,
                       dEdy        = dEdy,
                       maxIters    = 50,
                       best.values = T))
```
```{r echo=F}
data <- data.frame(L.Rate = c(rep(.01, 4), rep(.1, 4)),
                   points = c("(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)",
                              "(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)"),
                   x      = c(result[,1]$best$x, result[,2]$best$x, result[,3]$best$x, result[,4]$best$x,
                              result[,5]$best$x, result[,6]$best$x, result[,7]$best$x, result[,8]$best$x),
                   y      = c(result[,1]$best$y, result[,2]$best$y, result[,3]$best$y, result[,4]$best$y,
                              result[,5]$best$y, result[,6]$best$y, result[,7]$best$y, result[,8]$best$y),
                   error  = c(result[,1]$best$cost, result[,2]$best$cost, result[,3]$best$cost, result[,4]$best$cost,
                              result[,5]$best$cost, result[,6]$best$cost, result[,7]$best$cost, result[,8]$best$cost)

        )
knitr::kable(data, caption = "Comparison between learning rates and starting points")
```

*Como se aprecia en la tabla, la tasa de aprendizaje que se acerga más a un mínimo
es $\eta = 0.1$ mientras que $\eta = 0.01$ al ir más lento, y con tan pocas iteraciones
no consigue acercarse al mínimo local. Aún habiendo encontrado un valor cercano 
a cero para $\eta = 0.1$, esto no quiere decir que estemos ante el mínimo global,
podríamos haber caido en un local.*

