---
title: "Aprendizaje Automático - Práctica 2"
author: "Alejandro Alcalde"
date: "17/04/2016"
mainfont: Ubuntu Light
monofont: "Ubuntu Mono"
fontsize: 11pt
output: 
  # md_document:
  #   variant: markdown_github
  #   toc: true
  pdf_document:
    latex_engine: xelatex
    toc: true
    highlight: pygments
    # keep_tex: yes
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">>")
set.seed(1000000007)

# Función para verificar si un paquete está instalado o no
is.installed <- function(paquete) is.element(
  paquete, installed.packages())

if (!is.installed("ggplot2"))
  install.packages("ggplot2", dependencies = T)
if (!is.installed("gridExtra"))
  install.packages("gridExtra", dependencies = T)
if (!is.installed("sos"))
  install.packages("sos")
if (!is.installed("pracma"))
  install.packages("pracma")
if (!is.installed("parallel"))
  install.packages("parallel")

library(ggplot2)
library(gridExtra)  # for presenting plots side by side
library(sos)
library(pracma)
library(parallel)

# Calculate the number of cores
cores.number <- detectCores()
cl <- makeCluster(cores.number, type = "FORK")
# Initialize seed
clusterSetRNGStream(cl, iseed = 1000000007)

# Utils
GetLine <- function(interval = c(-1, 1)){
  # Computes the slope and intercept values of two points in the given interval
  #
  # Args:
  #   interval: Interval from which to pick to random points
  #
  # Returns:
  #   A vector [m, l, p1, p2] where:
  #     - m is the slope 
  #     - l is the intercept
  #     - p1, p2, are the points randomly selected
  thePoints  <- matrix(runif(4, min = interval[1], max = interval[2]), 2)
  
  # Compute the slope
  m <- (thePoints[1,1] - thePoints[2,2]) / (thePoints[1,1] - thePoints[2,1])
  
  # Compute y − y1 <- m(x − x1)
  line <- m * (thePoints[1,1] - thePoints[2,1]) + thePoints[2,2]
  
  res <- list(slope  = m,
              line   = line,
              points = thePoints)
  
  res
}

LabelData <- function(p, line = NULL){
  # Returns the corresponding label to the data, giving +1/-1 depending on
  # which side of the line the point lies
  #
  # Args:
  #   p: The points to label
  #   line: If we want to give a line instead of generate one randomly
  #
  # Returns:
  #   A list with the labels (+1/-1) and the line
  
  if (is.null(line)) {
    # Initialize a random plane to separate the -1, +1
    line <- matrix(runif(4, -1, 1), 2, 2)
  }
  
  # Given two points, determine if a point from the data set lies on the -1
  # side or the 1 side.
  # The points are A, B, the query points (X,Y)
  # The equation is (Bx - Ax) * (Y - Ay) - (By - Ay) * (X - Ax)
  values <- (line[2,1] - line[1,1]) * (p[,2] - line[1,2]) -
    (line[2,2] - line[1,2]) * (p[,1] - line[1,1])
  
  res = list(labels = ifelse(values > 0, 1, -1), 
             line = line)
  
  res
}

NewDataSet <- function(n, range = c(-1, 1), line = NULL) {
  # Generate a new data set from uniform random data in the given range
  # 
  # Args:
  #   n: The size of the data set
  #   range: In which range get the data
  #   line: If we want to give a line instead of generate one randomly 
  # 
  # Returns:
  #   A matrix with the data labeled by a random plane
  xx <- matrix(runif(n*2, range[1],range[2]), n, 2)
  yy <- as.vector(LabelData(xx, line))
  xx <- cbind(rep(1, n), xx)

  list(x = xx, y = yy$labels)  
}
```

# Modelos Lineales
 
## Ejercicio 1

Gradiente Descendente. Implementar el algoritmo de gradiente descendiente.

- Considerar la función no lineal de error $E(u,v) = (ue^v - 2ve^{-u})^2$. Usar
graciente descendente y minimizar esta función de error, comenzando desde el punto
$(u,v) = (1,1)$ y usando una tasa de aprendizaje $\eta=.1$

1. Calcular analíticamente y mostrar la expresión del gradiente de la función 
  de error. 
$$\frac{\partial}{\partial u}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2v)(e^{u+v}+2v)$$
$$\frac{\partial}{\partial v}(ue^v - 2ve^{-u})^2 = 2 e^{-2u}(ue^{u+v}-2)(ue^{u+v}-2 v)$$

2. ¿Cuantas iteraciones tarda el algoritmo en obtener por primera vez un valor
de  $E(u,v)$ inferior a $10^{-14}$?.

```{r Ex 1.1a Gradient Descent}
learning.ratio <- 0.1
w <- as.double(c(1, 1))

ErrorFunction <- function(u, v)
  (u * exp(v) - 2 * v * exp(-u)) ^ 2
dEdu <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2 * v) * (exp(u + v) + 2 * v)
dEdv <- function(u, v)
  2 * exp(-2 * u) * (u * exp(u + v) - 2) * (u * exp(u + v) - 2 * v)

cost <- 10000
nIters <- 0
while (cost > 1e-14) {
  u <-  w[1] - dEdu(w[1], w[2]) * learning.ratio
  v <-  w[2] - dEdv(w[1], w[2]) * learning.ratio
  w <- c(u, v)
  cost <- ErrorFunction(w[1], w[2])
  nIters <- nIters + 1
}
cat("Number of iterations until cost is below 1e-14:", nIters)
```

3. ¿Qué valores de $(u,v)$ obtuvo en el apartado anterior cuando alcanzó el error
de $10^{-14}$?

*El valor de $u=`r u`$ y el de $v=`r v`$*

- Considerar ahora la función $f(x,y) = x^2 + 2y^2 + 2\sin(2\pi x)\sin(2\pi y)$

1. Usar como valores iniciales $x_0 = 1$, $y_0 = 1$, la tasa de aprendizaje $\eta = 0.01$
y un máximo de $50$ iteraciones. Generar un gráfico de cómo desciende el valor de
la función con las iteraciones. Repetir el experimento pero usando $\eta = 0.1$,
comentar las diferencias.

```{r ex 1.1b}
# TODO: Reescribir documentación
GradientDescent <- function(initial.point = c(1, 1),
                            dEdx,
                            dEdy,
                            learning.ratio = 0.1,
                            maxIters = 100,
                            best.values = F) {
  # Compute gradient descent
  #
  # Args:
  #   initial.point:  The point from where to start
  #   dEdx: Partial derivative of function
  #   dEdy: Partial derivative of function
  #   learning.ratio: The ratio at which to learn
  #   maxIters: How many iterations as maximun
  #   best.values: If true, returned value includes best x,y and its error
  #
  #  Returns:
  #    A list with (cost, cost.history, x, y, nIters. [best])
  cost.current <- Inf
  nIters <- 0
  
  old.x <- dEdx(initial.point[1], initial.point[2])
  old.y <- dEdy(initial.point[1], initial.point[2])
  
  history <- c(abs(old.x + old.y))
  
  if (best.values == T) {
    cost.best <- Inf
  }
  
  while (nIters < maxIters) {
    new.x <-  old.x - dEdx(old.x, old.y) * learning.ratio
    new.y <-  old.y - dEdy(old.x, old.y) * learning.ratio
    
    cost.current <- abs(new.x + new.y)
    history <- c(history, cost.current)
    
    if (best.values == T && cost.current < cost.best) {
      best.x <- new.x
      best.y <- new.y
      cost.best <- cost.current
    }
    
    old.x <- new.x
    old.y <- new.y
    
    nIters <- nIters + 1
  }
  
  r <-
    list(theta = c(new.x, new.y),
         history = history[-1],
         iters = nIters)
  
  if (best.values == T) {
    r$best = list(x = best.x, y = best.y, cost = cost.best)
  }
  
  r
}

# ErrorFunc <- function(x, y)
#   x ^ 2 + 2 * y ^ 2 + 2 * sin(2 * pi * x) * (2 * pi * y)

dEdx <- function(x, y)
  2 * (2 * pi * cos(2 * pi * x) * sin(2 * pi * y) + x)
dEdy <- function(x, y)
  4 * (pi * sin(2 * pi * x) * cos(2 * pi * y) + y)

result <- mapply(
  GradientDescent,
  learning.ratio = c(0.01, 0.1),
  MoreArgs       = list(
    dEdx     = dEdx,
    dEdy     = dEdy,
    maxIters = 50
  )
)
```
```{r echo=F}
qplot(
  c(1:result[, 1]$iters),
  result[, 1]$history,
  geom = "line",
  xlab = "Iterations",
  ylab = "Error",
  main = "Learning rate = 0.01"
)
qplot(
  c(1:result[, 2]$iters),
  result[, 2]$history,
  geom = "line",
  xlab = "Iterations",
  ylab = "Error",
  main = "Learning rate = 0.1"
)
```

*Cuando $\eta = 0.01$, se avanza muy lentamente hacia el mínimo, esto tiene sus
pros y sus contras. Como pro nos aseguramos que no daremos un salto grande y nos
saldremos del mínimo hacia otro sitio. Como contra, si estamos en un mínimo local,
por la misma razón anterior, no saldremos de él y por tanto no encontraremos una 
buena solución.*

*En el caso de $\eta = 0.1$ nos encontramos en el caso de ir saltando de un lado
a otro, y por tanto el error fluctua mucho.*

- Obtener el valor mínimo y los valores de las variables que lo alcanzan cuando el
punto de inicio se fija: $(0.1, 0.1), (1, 1),(−0.5, −0.5),(−1, −1)$. Generar una tabla
con los valores obtenidos ¿Cuál sería su conclusión sobre la verdadera dificultad
de encontrar el mínimo global de una función arbitraria?

```{r}
result <- mapply(GradientDescent,
       learning.ratio = c(rep(.01, 4), rep(.1, 4)),
       initial.point = list(c(.1, .1), c(1,1), c(-.5, -.5), c(-1,-1)),
       MoreArgs = list(dEdx        = dEdx,
                       dEdy        = dEdy,
                       maxIters    = 50,
                       best.values = T))
```
```{r echo=F}
data <- data.frame(L.Rate = c(rep(.01, 4), rep(.1, 4)),
                   points = c("(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)",
                              "(.1,.1)","(1,1)","(-.5,-.5)","(-1,-1)"),
                   x      = c(result[,1]$best$x, result[,2]$best$x, result[,3]$best$x, result[,4]$best$x,
                              result[,5]$best$x, result[,6]$best$x, result[,7]$best$x, result[,8]$best$x),
                   y      = c(result[,1]$best$y, result[,2]$best$y, result[,3]$best$y, result[,4]$best$y,
                              result[,5]$best$y, result[,6]$best$y, result[,7]$best$y, result[,8]$best$y),
                   error  = c(result[,1]$best$cost, result[,2]$best$cost, result[,3]$best$cost, result[,4]$best$cost,
                              result[,5]$best$cost, result[,6]$best$cost, result[,7]$best$cost, result[,8]$best$cost)

        )
knitr::kable(data, caption = "Comparison between learning rates and starting points")
```

*Como se aprecia en la tabla, la tasa de aprendizaje que se acerga más a un mínimo
es $\eta = 0.1$ mientras que $\eta = 0.01$ al ir más lento, y con tan pocas iteraciones
no consigue acercarse al mínimo local. Aún habiendo encontrado un valor cercano 
a cero para $\eta = 0.1$, esto no quiere decir que estemos ante el mínimo global,
podríamos haber caido en un local.*

## Ejercicio 2

**Coordenada descendente**. En este ejercicio comparamos la eficiencia de la
técnica de optimización de “coordenada descendente” usando la misma función del ejercicio
1.1a. En cada iteración, tenemos dos pasos a lo largo de dos coordenadas. En el Paso-1
nos movemos a lo largo de la coordenada $u$ para reducir el error (suponer que se verifica
una aproximación de primer orden como en gradiente descendente), y el Paso-2 es para
reevaluar y movernos a lo largo de la coordenada $v$ para reducir el error ( hacer la misma
hipótesis que en el paso-1). Usar una tasa de aprendizaje $\eta = 0.1$.

```{r Coordinate Descent}
nIters <- 0
w <- c(1,1)
learning.ratio <- 0.1
cost <- 0
while (nIters < 15) {
  u <-  w[1] - dEdu(w[1], w[2]) * learning.ratio
  w[1] <- u
  v <-  w[2] - dEdv(w[1], w[2]) * learning.ratio
  w[2] <- v
  cost <- ErrorFunction(w[1], w[2])
  nIters <- nIters + 1
}
```

*En este caso se obtiene un valor mucho peor para $E(u,v)$, en concreto $`r cost`$.
Mientras que con Gradiente Descendente, en 10 iteraciones ya obtenemos un valor 
en torno a los $10^{-15}$, valor que dista mucho del que se obtiene con Coordenada
descendente, el cual se queda muy lejos del óptimo.*

*Este método tiene problemas cuando las funciones no son suaves, ya que se queda
estancado en puntos no estacionarios cuando la curva de la función no es suave.
Sin embargo, cuando se tienen funciones no derivables o difíciles de derivar, 
gradiente descendente no se puede aplicar, por lo que es recomendable usar este otro método.*


## Ejercicio 3

Método de Newton Implementar el algoritmo de minimización de Newton y aplicarlo
a la función $f(x, y)$ dada en el ejercicio 1b. Desarrolle los mismos experimentos
usando los mismos puntos de inicio.

```{r Newton\'s method}
dfdx2 <- function(x) 
  2 - 8 * pi ^ 2 * sin(2 * pi * x[1]) * sin(2 * pi * x[2]) #dx
dfdy2 <- function(x) 
  4 - 8 * pi ^ 2 * sin(2 * pi * x[1]) * sin(2 * pi * x[2]) #dy

Newton <- function(initial.point = c(1,1), 
                   dEdx,
                   dEdy,
                   dfdx2,
                   dfdy2,
                   maxIters = 100) {
  # Newton's Method implementation
  #
  # Args:
  #   initial.point:  The point from where to start
  #   dEdx,dEdy:  Partial derivative of function
  #   dfdx2,dfdx2:  Second derivative of function
  #   maxIters: How many iterations as maximun
  #   
  #  Returns:
  #    A list with (theta, cost.history, nIters)
  nIters <- 0
  
  old.values <- matrix(c(dEdx(initial.point[1], initial.point[2]),
                         dEdy(initial.point[1], initial.point[2])))
  
  history <- c(sum(old.values))
  threshold <- F
  
  while (nIters < maxIters && !threshold) {
    
    inv.diag <- dfdx2(old.values) + dfdy2(old.values)
    
    H <- matrix(c(dfdx2(old.values),
                  inv.diag,
                  inv.diag,
                  dfdy2(old.values)), 2, 2, byrow = T)
    
    duv <- svd(H)
    H.inv <- duv$v %*% diag(1 / duv$d) %*% t(duv$u)
    
    new.values <- -H.inv %*% old.values
    
    cost.current <- abs(sum(new.values))
    history <- c(history, cost.current)
    
    if (abs(sum(old.values) - sum(new.values)) < 1e-5)
      threshold <- T
    
    old.values <- new.values
    nIters <- nIters + 1
  }
  
  r <- list(theta = c(new.values[1], new.values[2]), history = history[-1], iters = nIters)
  r
}

result <- mapply(Newton,
                 initial.point = list(c(.1, .1), c(1,1), c(-.5, -.5), c(-1,-1)),
                 MoreArgs =  list(dEdx = dEdx, dEdy = dEdy, dfdx2 = dfdx2, dfdy2 = dfdy2, maxIters = 50))

```


```{r echo=F}
p1 <- qplot(c(1:result[,1]$iters), result[,1]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (.1, .1)")
p2 <- qplot(c(1:result[,2]$iters), result[,2]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (1, 1)")
p3 <- qplot(c(1:result[,3]$iters), result[,3]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (-.5, -.5)")
p4 <- qplot(c(1:result[,4]$iters), result[,4]$history, 
            geom = "line", xlab = "Iterations", ylab = "Error", main = "Initial Point: (-1, -1)")

grid.arrange(p1,p2,p3,p4, nrow = 2, ncol = 2, top = "Results of Newton's Method", bottom = "It computes a direct solution")
```

*Como observamos en las gráicas, el método de Newton calcula de forma directa el
mínimo de la función. En este caso se ha cortado la ejecución cuando la diferencia
entre la mejora estaba en torno a $10^{-5}$, ya que salvo en el primer caso, se 
obtiene el mínimo en la segunda iteración.*

## Ejercicio 4

Regresión Logística: En este ejercicio crearemos nuestra propia función
objetivo $f$ (probabilidad en este caso) y nuestro conjunto de datos $\mathcal{D}$
para ver cómo funciona regresión logística. Supondremos por simplicidad que $f$
es una probabilidad con valores $0/1$ y por tanto que y es una función determinista
de $\mathbf{x}$.

```{r Logistic regression}
SGD <- function(data,
                h,
                dh,
                tolerance = 0.01,
                learning.rate = 0.01,
                how.many.epochs = F) {
  # Stochastic gradient descent
  #
  # Args:
  #   data: The data, the last columns contains the labels
  #   h: Error function to minimize
  #   dh: Gradient of the error function
  #   tolerance: When SGD should stop searching for the minimun
  #   learning.rate: Learning rate when updating the weights
  #   how.many.epochs: If true returns also how many epochs took to converge
  #
  #  Returns:
  #    Final weights and how many epochs took to converge, if specified
  
  # Create a new enviroment to place w so that update function throught apply can modify it
  w.env <- new.env()
  w.env$w <- matrix(rep(0,3))
  above.tolerance <- T
  
  update <- function(x) {
    ## In order to use apply, we need x containing also the label (in the last column)
    y <- as.vector(x[4])
    x <- as.vector(x[-4])
    gradient <- dh(y %*% t(x), y * (t(w.env$w) %*% x))
    w.env$w <- w.env$w - learning.rate * gradient
  }
  
  if (how.many.epochs == T) {
    nEpochs <- 0
  }
  while (above.tolerance) {
    
    w.old <- w.env$w
    # We only need the last element of w, as it is the last update
    apply(data, 1, update)
    
    # Apply a permutation to the data in each epoch
    data <- data[sample(nrow(data)),]

    if (how.many.epochs == T) {
      nEpochs <- nEpochs + 1
    }
    
    if (norm(w.old - w.env$w, type = "F") < tolerance) {
      above.tolerance <- F
    }
  }
  
  if (how.many.epochs) {
    list(w = w.env$w, epochs = nEpochs)
  } else 
    w.env$w
}
# Generate the DataSet
line <- GetLine()
data <- NewDataSet(100, line = line$points)
```
```{r echo=F}
label <- factor(as.integer(data$y))
ggplot(data = as.data.frame(data$x[,-1]), aes(V1, V2)) + geom_point(aes(colour = label)) + theme_bw()
```
```{r}
# Error function
h <- function(x) log(1 + exp(as.double(-x)))
# Gradient if error function
dh <- function(yx, ywx) t(-yx/(1 + exp(as.double(ywx))))

# Prepare the data for SGD
sgd.data <- cbind(data$x, as.vector(data$y))
w <- SGD(sgd.data, h, dh)
rm(sgd.data)
```

*La hipótesis que obtenemos con el algoritmo (El vector de pesos) es el siguiente:*

$$w = 
\begin{bmatrix}
`r w[1]` \\ 
`r w[2]` \\
`r w[3]`
\end{bmatrix}
$$

```{r}
# Compute the cross entropy error, with new data for our hypothesis
data <- NewDataSet(10000, line = line$points)
cross.entropy.error <- sum(sapply(data$y * (t(w) %*% t(data$x)), h))/nrow(data$x)
```

*Para estimar $E_{out}$ usamos el error de entropía cruzada, el cual es el siguiente
(Para una muestra nueva de $1000$ puntos):*

$$E_{out} = `r cross.entropy.error`$$

```{r Replicate 100 times}
data <- NewDataSet(100)
sgd.data <- cbind(data$x, as.vector(data$y))
cross.entropy.error <- replicate(100, {
  w <- SGD(sgd.data, h, dh, how.many.epochs = T)
  error <- sum(parSapply(cl, data$y * (t(w$w) %*% t(data$x)), h))/nrow(data$x)
  
  c(epochs = w$epochs, error = error)
})
rm(sgd.data)
stopCluster(cl)
```

*Si replicamos el experimento 100 veces usando funciones de frontera distintas 
cada vez, para un conjunto de 100 datos, el valor de $E_{out}$ medios es:*

$$E_{out} = `r mean(cross.entropy.error["error",])`$$

*En cuanto a la media de épocas necesarias para converger, el valor es 
$`r mean(cross.entropy.error["epochs",])`$*


