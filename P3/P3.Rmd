---
title: "Aprendizaje Automático - Práctica 3"
author: "Alejandro Alcalde"
date: "05/05/2016"
fontsize: 10pt
monofont: "DejaVu Sans Mono"
# monofont: "Source Code Pro Light"
# mathfont: "Source Code Pro Light"
# mainfont: "Ubuntu Light"
output:
  pdf_document:
    includes:
      in_header: header.tex
    latex_engine: xelatex
    toc: true
    highlight: tango
    keep_tex: true
    # md_extensions: +latex_macros+raw_tex
# fc-list :outline -f "%{family}\n"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = F)
```

```{r Load libraries, echo = F, include = F}
is.installed <- function(paquete) is.element(
  paquete, installed.packages())

if (!is.installed("dplyr"))
  install.packages("dplyr")
if (!is.installed("ggplot2"))
  install.packages("ggplot2")
if (!is.installed("GGally"))
  install.packages("GGally")
if (!is.installed("ISLR"))
  install.packages("ISLR")
if (!is.installed("gridExtra"))
  install.packages("gridExtra")
if (!is.installed("caret"))
  install.packages("caret")
if (!is.installed("class"))
  install.packages("class")
if (!is.installed("parallel"))
  install.packages("parallel")
if (!is.installed("e1071"))
  install.packages("e1071")
if (!is.installed("plotROC"))
  install.packages("plotROC")
if (!is.installed("glmnet"))
  install.packages("glmnet")
if (!is.installed("randomForest"))
  install.packages("randomForest")
if (!is.installed("gbm"))
  install.packages("gbm")
if (!is.installed("tree"))
  install.packages("tree")

library(dplyr)
library(ggplot2)
library(GGally)
library(ISLR)
library(gridExtra)
library(caret)
library(class)
library(parallel)
library(e1071)
library(ROCR)
library(MASS)
library(plotROC)
library(glmnet)
library(randomForest)
library(gbm)
library(tree)

devtools::install_github("hadley/ggplot2")
devtools::install_github("sachsmc/plotROC")

set.seed(1000000008)

GetParCluster <- function(seed = 1000000007) {
  # Calculate the number of cores
  cores.number <- detectCores()
  cl <- makeCluster(cores.number, type = "FORK")
  # Initialize seed
  clusterSetRNGStream(cl, iseed = seed)
  
  cl
}
```

\newpage

# Ejercicio 1

__Usar el conjunto de datos Auto que es parte del paquete ISLR.__

__En este ejercicio desarrollaremos un modelo para predecir si un coche tiene un
consumo de carburante alto o bajo usando la base de datos Auto. Se considerará alto
cuando sea superior a la mediana de la variable mpg y bajo en caso contrario.__

- __Usar las funciones de R `pairs()` y `boxplot()` para investigar la dependencia entre
`mpg` y las otras características. ¿Cuáles de las otras características parece más
útil para predecir `mpg`? Justificar la respuesta. (0.5 puntos)__

En primer lugar cargaremos los datos, y vemos de qué tipo son:

```{r EXERCISE 1.A}
# Information dense summary of the data
(Auto <- dplyr::tbl_df(ISLR::Auto))
dplyr::glimpse(Auto)
Auto <- Auto %>% dplyr::select(-name)
```
```{r scatterplots, echo=F}
debug <- F
Auto$origin <- factor(Auto$origin)
if (!debug){
GGally::ggpairs(Auto, aes(colour = origin, alpha = .5),
                 upper = list(continuous = wrap("cor", size = 2.5)),
                 lower = list(continuous = "points"),
                 diag = "blank", axisLabels = "none", columns = 1:7,
                 title = "Scatterplots of Auto Auto",
                 columnLabels = c("mpg", "cyl", "displ", "cv", "weight", "acc", "year"))
}
```

Los colores de los puntos corresponden al origen del coche (Americano, Europeo o
Japonés), se puede intuir que algunos coches gastan más en función del continente.

Como se aprecia en la gráfica, hay varias variables correlacionadas tanto positiva
como negativamente. Pero a nosotros solo nos interesan las relacionadas con el 
consumo de combustible. Con ello en mente, podríamos decir que las variables más 
correladas son la capacidad del motor (__Displacement__), el peso y la potencia
(__horsepower__) del coche. Veámoslas mejor:

```{r plotting corr vars, echo = F}
v <- Auto %>% dplyr::select(mpg, horsepower, weight, displacement, origin)

ggplot2::ggplot(data = v, aes(horsepower, mpg)) + 
  geom_point(aes(colour = origin)) + 
  theme_bw() + 
  scale_color_discrete(name = "Origin", labels = c("American", "European", "Japanese"))

ggplot2::ggplot(data = v, aes(weight, mpg)) + 
  geom_point(aes(colour = origin)) + 
  theme_bw() + 
  scale_color_discrete(name = "Origin", labels = c("American", "European", "Japanese"))

ggplot2::ggplot(data = v, aes(displacement, mpg)) + 
  geom_point(aes(colour = origin)) + 
  theme_bw() + 
  scale_color_discrete(name = "Origin", labels = c("American", "European", "Japanese"))
rm(v)
```

Veamos ahora la correlación mediante `boxplot`:

```{r Boxplots, echo = F}
# A car has a low fuel comsumption if mpg is higher (It travels more milles per gallon of fuel).
# Consider the cars with mpg above the median as low consumption (1), and high
# comsumption those bellow the median
Auto <- dplyr::mutate(Auto, mpg01 = as.factor(ifelse(mpg > median(mpg), 1, 0)))

bp1 <- ggplot(Auto) + geom_boxplot(aes(mpg01, cylinders)) + labs(x = "")
bp2 <- ggplot(Auto) + geom_boxplot(aes(mpg01, displacement)) + labs(x = "")
bp3 <- ggplot(Auto) + geom_boxplot(aes(mpg01, horsepower)) + labs(x = "")
bp4 <- ggplot(Auto) + geom_boxplot(aes(mpg01, weight)) + labs(x = "")
bp5 <- ggplot(Auto) + geom_boxplot(aes(mpg01, acceleration)) + labs(x = "")
bp6 <- ggplot(Auto) + geom_boxplot(aes(mpg01, year)) + labs(x = "")

grid.arrange(bp1, bp2, bp3, bp4, bp5, bp6,
             nrow = 2, ncol = 3,
             top = "Variables vs mpg01")
rm(bp1, bp2, bp3, bp4, bp5, bp6)
```

Como vemos, llegamos a la misma conclusión, la interpretación en boxplots es la
siguiente: Si los boxplots no se solapan entre sí, las variables están correladas.
Además, a mayor sea la distancia que los separa, más correladas estarán. Por tanto
salvo la aceleración y el año, el resto de variables están correladas en mayor o 
menor medida con las millas por galón de combustible que consume el coche.

- __Seleccionar las variables predictoras que considere más relevantes.__

Como hemos comentado en el punto anterior, las variables que parecen estar más
relacionadas con el consumo de combustible son `Horsepower, Weight` y `Displacement`.
Por lo tanto las usaremos como predictoras.

- __Particionar el conjunto de datos en un conjunto de entrenamiento ($80\%$) y otro
de test ($20\%$). Justificar el procedimiento usado.__

Para particionar los datos debemos asegurarnos que tanto la partición para training
como la de test mantengan la misma distribución, ya que de lo contrario podríamos
ajustar mal el modelo. Para conseguir esto, se hace uso de la librería `caret`
y su método `createDataPartition`. Ya que la variable `mpg` es contínua, se ha
decidido distribuir las muestas equitativamente en función del origien del coche.

Se ha creado un método `partitionDistribution` que muestra cómo se sigue manteniendo
la proporción de los datos tras el particionado. Esta estrategia se llama particionamiento
estratificado.

```{r EXERCISE 1.C}
partitionDistribution <- function(partition) {
  print(paste('Training: ', nrow(partition$training), 'instances'))
  print(paste('Test: ', nrow(partition$test), 'instances'))
}

indexes <- createDataPartition(Auto$origin, p = .8, list = FALSE)
partition <- list(training = Auto[indexes,], test = Auto[-indexes,])
partitionDistribution(partition)
```

Podemos ver en la salida enterior como se ha mantenido la proporción de los 
orígenes de coches en ambas particiones.

- __Crear una variable binaria, `mpg01`, que será igual a $1$ si la variable `mpg`
contiene un valor por encima de la mediana, y $-1$ si `mpg` contiene un valor por
debajo. La mediana se puede calcular usando la función `median()`.__

```{r EXERCISE 1.D}
# A car has a low fuel comsumption if mpg is higher (It travels more milles per 
# gallon of fuel). Consider the cars with mpg above the median as low consumption
# (1), and high comsumption those bellow the median 
# Auto <- dplyr::mutate(Auto, mpg01 = as.factor(ifelse(mpg > median(mpg), 1, 0)))
training <- partition$training
test <- partition$test
```

- __Ajustar un modelo de regresión Logística a los datos de entrenamiento y
predecir `mpg01` usando las variables seleccionadas en b). ¿Cuál es el error
de test del modelo? Justificar la respuesta. (1 punto)__

```{r EXERCISE 1.D.1}
glm.fit <- glm(mpg01 ~ horsepower + weight + displacement, 
               data = training, 
               family = binomial)

summary(glm.fit)

# Compute the test error
glm.probs <- predict(glm.fit, newdata = test, type = "response")
glm.pred <- ifelse(glm.probs > .5, 1, 0)

confusion.table <- table(glm.pred, test$mpg01)
test.error <- mean(glm.pred != test$mpg01)*100
```

Para ajustar el modelo logístico se ha usado `glm`, la matríz de confusión es la
siguiente:

 |     	| $0$                          	| $1$                          	|
 |-----	|------------------------------	|------------------------------	|
 | $0$ 	| $`r confusion.table[1,1]`$ 	  | $`r confusion.table[1,2]`$ 	  |
 | $1$ 	| $`r confusion.table[2,1]`$ 	  | $`r confusion.table[2,2]`$ 	  |

Esta matríz muestra en la diagonal secundaria los falsos positivos
y los negativos ($`r confusion.table[1,2]`$ y $`r confusion.table[2,1]`$ respetivamente).

El error de test que hemos obtenido es del $`r test.error`\%$, lo cual 
quiere decir que estamos fallando en clasificar correctamente ese porcentaje de veces.

- __Ajustar un modelo K-NN a los datos de entrenamiento y predecir `mpg01`
usando solamente las variables seleccionadas en b). ¿Cuál es el error de
test en el modelo? ¿Cuál es el valor de K que mejor ajusta los datos?
Justificar la respuesta. (Usar el paquete class de R) (1 punto)__

```{r EXERCISE 1.D.2}
ytraining <- training %>% dplyr::select(mpg01)
training.X <- training %>% dplyr::select(horsepower, weight, displacement)
training.X <- scale(training.X)
test.X <- test %>% dplyr::select(horsepower, weight, displacement)
test.X <- scale(test.X)[,]

data <- data.frame(train = training.X, y = ytraining)

# Find the best k with cross validation with 10 partitions
k <- e1071::tune.knn(data[,-4], data[,4], k = 1:30, 
                     tunecontrol = tune.control(sampling = "cross"))
k <- k$best.parameters$k

pred <- class::knn(data[,-4],
                   test.X,
                   data[,4],
                   k = k,
                   prob = T)

prob <- attr(pred, "prob")

# this is necessary because k-NN by default outputs
# the posterior probability of the winning class
prob[pred == 0] <- 1 - prob[pred == 0]

confusion.table <- table(pred, test$mpg01)
test.error <- mean(pred != test$mpg01)*100

rm(data, cl)
```

Primero preparamos los datos necesarios para aplicar `class:knn`, luego se ha
buscado el mejor valor de `k` comprendido entre $1$ y $30$, el mejor ha sido
$`r k`$. Con este valor para `k` se ha obtenido la siguiente matriz de confusión:

|     	| $0$                          	| $1$                          	|
|-----	|------------------------------	|------------------------------	|
| $0$ 	| $`r confusion.table[1,1]`$ 	  | $`r confusion.table[1,2]`$ 	  |
| $1$ 	| $`r confusion.table[2,1]`$ 	  | $`r confusion.table[2,2]`$ 	  |

Y un error de test de $`r test.error`\%$, el cual es bastante bueno.

Se probó a buscar un valor para `k` usando `tune.knn`, pero se obtenían peores
resultados para los valores de `k` propuestos.

En cuanto a la eleccion de `k` no es tarea fácil, ya que en función de la elección
podemos obtener efectos muy variantes en el clasificador. Una regla que puede 
servir de guía es que a medida que `k` aumenta, la frontera de decisión se vuelve
más rígida (Más lineal), y a medida que decrece es más flexible. Por tanto es
importante encontrar un punto medio para no obtener ni sobreajuste ni demasiado
error en la clasificación

- __Pintar las curvas ROC (instalar paquete ROCR en R) y comparar y valorar los
resultados obtenidos para ambos modelos. (0.5 puntos)__

```{r EXERCISE 1.D.3}
ROCCurve <- function(data) {
  # Draw a ROC curve
  #
  # Args:
  #   data: Data frame with three columns (M for predicted objects, D for true
  #         classes and model if more than one ROC curve wants to be drawn)
  #   
  # Returns:
  #   The Area Under ROC Curve (AUC)
  # TODO: Check dataframe columns (Error handling)
  g <- ggplot(data, aes(d = D, m = M, color = model)) + geom_roc(n.cuts = 0) + style_roc()
  auc <- round(calc_auc(g), 6)
  g <- g + annotate("text", x = .75, y = .25, 
                    label = paste("AUC_knn = ", auc$AUC[2], "\nAUC_glm = ", auc$AUC[1]))

  print(g)
  
  list(auc$AUC[2], auc$AUC[1])
}

pred.object.knn <- prediction(as.numeric(prob), as.numeric(test$mpg01))
pred.object.glm <- prediction(glm.probs, test$mpg01)

data <- data.frame(M = c(unlist(attr(pred.object.knn, "predictions")), 
                         unlist(attr(pred.object.glm, "predictions"))),
                   D = c(unlist(attr(pred.object.knn, "labels")), 
                         unlist(attr(pred.object.glm, "labels"))),
                   model = c(rep("knn", 77), rep("glm", 77)))
auc <- ROCCurve(data)
```

En el código anterior se han pintado y calculado las curvas _ROC_ para `KNN` y 
regresión logística (`glm`). Como se puede apreciar en las gráficas, el clasificador
que obtiene mejores resultados es `knn`. Esto se pone de manifiesto si calculamos
el área bajo la curva _ROC_. A mayor área, mejor rendimiento tiene el clasificador.
En concreto, $AUC_{knn} = `r auc[1]`$ y $AUC_{glm} = `r auc[2]`$

- __Bonus-1. (1 punto) Estimar el error de test de ambos modelos (`RL, K-NN`) pero
usando Validación Cruzada de 5-particiones. Comparar con los resultados
obtenidos en el punto anterior.__

- __Bonus-2 (1 punto): Ajustar el mejor modelo de regresión posible considerando
la variable `mpg` como salida y el resto como predictoras. Justificar el modelo
ajustado en base al patrón de los residuos. Estimar su error de entrenamiento y
test.__

# Ejercicio 2

__Usar la base de datos `Boston` (en el paquete MASS de R) para ajustar un modelo que
prediga si dado un suburbio este tiene una tasa de criminalidad (`crim`) por encima o
por debajo de la mediana. Para ello considere la variable `crim` como la variable salida y
el resto como variables predictoras.__

- __Encontrar el subconjunto óptimo de variables predictoras a partir de un
modelo de regresión-LASSO (usar paquete `glmnet` de R) donde seleccionamos
solo aquellas variables con coeficiente mayor de un umbral prefijado. (1
punto)__

```{r EXERCISE 2.1}
# Partition data into test and training
indexes <- createDataPartition(Boston$crim, p = .8, list = FALSE)
partition <- list(training = Boston[indexes,], test = Boston[-indexes,])
partitionDistribution(partition)
# Prepare the data for the model
data.boston.training.X <- as.matrix(partition$training[,-1])
data.boston.training.Y <- as.matrix(partition$training[,1])
data.boston.test.X <- as.matrix(partition$test[,-1])
data.boston.test.Y <- as.matrix(partition$test[,1])

# Generate a set of lambdas to prove
grid <- 10 ^ seq(10 , -2 , length = 1000)
# grid <- 10 ^ seq(-1, -0.2, by = 0.01)
# Fit a Lasso model
lasso.model <- glmnet(data.boston.training.X,
                      data.boston.training.Y, alpha = 1, lambda = grid)
# Compute the cross validation error
cv.lasso.out <- cv.glmnet(data.boston.training.X, 
                          data.boston.training.Y, alpha = 1, lambda = grid)
# Select the best lambda from the cross validation test
best.lambda <-  cv.lasso.out$lambda.min
# Make predictions and compute the associate test error
lasso.pred <- predict(lasso.model, s = best.lambda, newx = data.boston.test.X)
lasso.test.error <- mean((lasso.pred - data.boston.test.Y ) ^ 2)

# How many variables are being used to predict crime rates?
out <- glmnet(as.matrix(Boston[,-1]),
              as.matrix(Boston[,1]), alpha = 1, lambda = grid)
# Out of 12 variables, the model uses 10 to predict crime rates.
(lasso.coef <- predict(out, type = "coefficients", s = best.lambda))
```

En el código de arriba hemos ajustado el modelo usando la técnica _LASSO_. Con 
esta técnica podemos ver que hay dos variables que no son de utilidad a la hora
de predecir la tasa de criminalidad, estas dos variables son `age` y `tax`. La 
variable que parece tener más importancia es `nox`. También se ha calculado
el error fuera de la muestra despues de aplicar validación cruzada, habiendo
obtenido

$$E_{test} = `r lasso.test.error`\%$$

Los coeficientes que se obtienen son:

$$\mathbf{w} = \begin{bmatrix}
zn = `r lasso.coef[2]` \\
indus = `r lasso.coef[3]` \\
chas = `r lasso.coef[4]` \\
nox = `r lasso.coef[5]` \\
rm = `r lasso.coef[6]` \\
dis = `r lasso.coef[8]` \\
rad = `r lasso.coef[9]` \\
ptratio = `r lasso.coef[11]` \\
black = `r lasso.coef[12]` \\
lstat = `r lasso.coef[13]` \\
mdev = `r lasso.coef[14]` \\
\end{bmatrix}$$

Basándonos en estos valores aún podríamos descartar algunas variables más, por 
ejemplo todas aquellas con un coeficiente $< 0.01$.

- __Ajustar un modelo de regresión regularizada con “`weight-decay`” (`ridge-regression`) 
y las variables seleccionadas. Estimar el error residual del modelo
y discutir si el comportamiento de los residuos muestran algún indicio de
“underfitting”. (1 punto)__

```{r EXERCISE 2.2}
data.boston.training.selected.variables.X <- data.boston.training.X[,c(-6,-9)]
ridge.model <- glmnet(data.boston.training.selected.variables.X,
                      data.boston.training.Y, alpha = 0, lambda = best.lambda)

data.boston.test.selected.variables.X <- data.boston.test.X[,c(-6,-9)]
w.reg <- coef(ridge.model)
ridge.pred <- predict(ridge.model, s = best.lambda,
                      newx = data.boston.test.selected.variables.X)

# https://en.wikipedia.org/wiki/Errors_and_residuals
residual.error <- mean(ridge.pred - data.boston.test.Y)
```

Tras ajustar el modelo y predecir los valores usando los datos de test, se ha 
obtenido un error residual del $`r residual.error * 100`\%$, lo cual es bastante
alto y puede significar que se esté produciendo _underfitting_. Un posible motivo
pudiera ser la no correcta elección de las variables predictoras.

- __Definir una nueva variable con valores -1 y 1 usando el valor de la mediana de
la variable `crim` como umbral. Ajustar un modelo SVM que prediga la nueva
variable definida. (Usar el paquete `e1071` de R). Describir con detalle cada
uno de los pasos dados en el aprendizaje del modelo SVM. Comience
ajustando un modelo lineal y argumente si considera necesario usar algún
núcleo. Valorar los resultados del uso de distintos núcleos. (1 punto)__

```{r EXERCISE 2.3}
data.boston <- dplyr::tbl_df(Boston)
data.boston <- data.boston %>% dplyr::mutate(crim01 = factor(ifelse(crim > median(crim), 1, -1)))
# Remove crim variable, since we are going to use crim01
data.boston <- data.boston %>% dplyr::select(-crim)

indexes <- createDataPartition(data.boston$crim01, p = .8, list = FALSE)
partition <- list(training = data.boston[indexes,], test = data.boston[-indexes,])

data.boston.training <- dplyr::as.tbl(partition$training)
data.boston.test <- dplyr::as.tbl(partition$test)

# Linear kernel
svm.model <- e1071::svm(crim01 ~ ., data.boston.training, kernel = "linear",
                        cost = 10, scale = T)
svm.predicted <- predict(svm.model, dplyr::select(data.boston.test, -crim01))
confusion.table.linear <- table(svm.predicted, data.boston.test$crim01)
test.error.linear <- mean(svm.predicted != data.boston.test$crim01)*100

# Polynomial kernel
svm.model <- e1071::svm(crim01 ~ ., data.boston.training, kernel = "polynomial",
                        cost = 10, scale = T)
svm.predicted <- predict(svm.model, dplyr::select(data.boston.test, -crim01))
confusion.table.polynomial <- table(svm.predicted, data.boston.test$crim01)
test.error.polynomial <- mean(svm.predicted != data.boston.test$crim01)*100

# Radial kernel
svm.model <- e1071::svm(crim01 ~ ., data.boston.training, kernel = "radial",
                        cost = 10, scale = T)
svm.predicted <- predict(svm.model, dplyr::select(data.boston.test, -crim01))
confusion.table.radial <- table(svm.predicted, data.boston.test$crim01)
test.error.radial <- mean(svm.predicted != data.boston.test$crim01)*100
```

En principio se ha ajustado con un núcle lineal, pero no se han obtenido buenos 
resultados, ya que el error obtenido en test ha sido del $`r test.error.linear`\%$. Con
una matriz de confusión como la siguiente:

|     	| $-1$                                	| $1$                                 	|
|-----	|-------------------------------------	|-------------------------------------	|
| $-1$ 	| $`r confusion.table.linear[1,1]`$ 	  | $`r confusion.table.linear[1,2]`$ 	  |
| $1$ 	| $`r confusion.table.linear[2,1]`$ 	  | $`r confusion.table.linear[2,2]`$ 	  |

Debido a que los datos seguramente no sean linealmente separables, probamos ahora
con un kernel polinómico, obteniendo unos resultados algo mejores:

|     	| $-1$                                	| $1$                                 	|
|-----	|-------------------------------------	|-------------------------------------	|
| $-1$ 	| $`r confusion.table.polynomial[1,1]`$ 	  | $`r confusion.table.polynomial[1,2]`$ 	  |
| $1$ 	| $`r confusion.table.polynomial[2,1]`$ 	  | $`r confusion.table.polynomial[2,2]`$ 	  |

Y un error de test del $`r test.error.polynomial`\%$, lo cual es una mejora, poca,
pero algo mejor.

Probamos un kernel de base radial esta vez, su matriz de confusión es:

|     	| $-1$                                	| $1$                                 	|
|-----	|-------------------------------------	|-------------------------------------	|
| $-1$ 	| $`r confusion.table.radial[1,1]`$ 	  | $`r confusion.table.radial[1,2]`$ 	  |
| $1$ 	| $`r confusion.table.radial[2,1]`$ 	  | $`r confusion.table.radial[2,2]`$ 	  |

Con un error para test del $`r test.error.radial`\%$

Sin embargo podemos obtener mejores resultados si usamos validación cruzada
para encontrar una combinación de parámetros adecuada, veamos:

```{r EXERCISE 2.3.tune}
tune.radial <- tune.svm(crim01 ~ ., 
                        data = data.boston.training, 
                        kernel = "radial",
                        cost = c(0.1, 1, 10),
                        gamma = c(0.5, 1, 2, 3, 4),
                        scale = T,
                        tunecontrol = tune.control(sampling = "cross"))

tune.poly <- tune.svm(crim01 ~ ., 
                        data = data.boston.training, 
                        kernel = "polynomial",
                        cost = c(0.1, 1, 10),
                        gamma = c(0.5, 1, 2, 3, 4),
                        degree = c(1, 3, 4, 5, 2),
                        scale = T,
                        tunecontrol = tune.control(sampling = "cross"))

# Radial kernel
svm.model <- e1071::svm(crim01 ~ ., data.boston.training, kernel = "radial",
                        cost = tune.radial$best.parameters$cost, 
                        gamma = tune.radial$best.parameters$gamma,
                        scale = T)
svm.predicted <- predict(svm.model, dplyr::select(data.boston.test, -crim01))
confusion.table.radial <- table(svm.predicted, data.boston.test$crim01)
test.error.radial <- mean(svm.predicted != data.boston.test$crim01)*100

# Polynomial kernel
svm.model <- e1071::svm(crim01 ~ ., data.boston.training, kernel = "polynomial",
                        cost = tune.poly$best.parameters$cost, 
                        degree = tune.poly$best.parameters$degree,
                        gamma = tune.poly$best.parameters$gamma,
                        scale = T)
svm.predicted <- predict(svm.model, dplyr::select(data.boston.test, -crim01))
confusion.table.polynomial <- table(svm.predicted, data.boston.test$crim01)
test.error.polynomial <- mean(svm.predicted != data.boston.test$crim01)*100
```

Tras ajustar los parámetros para los kernel radial y poinómico, se han obtenido
mejores resultados, como era de esperar. A continuación se muestra la matriz de
confusión para un kernel radial, con parámetros $\gamma = `r tune.radial$best.parameters$gamma`$:

|     	| $-1$                                	| $1$                                 	|
|-----	|-------------------------------------	|-------------------------------------	|
| $-1$ 	| $`r confusion.table.radial[1,1]`$ 	  | $`r confusion.table.radial[1,2]`$ 	  |
| $1$ 	| $`r confusion.table.radial[2,1]`$ 	  | $`r confusion.table.radial[2,2]`$ 	  |

Con un error para test del $`r test.error.radial`\%$.

Y para un kernel polinómico, con parámetros $\gamma = `r tune.poly$best.parameters$gamma`$ 
y $degree = `r tune.poly$best.parameters$degree`$:

|     	| $-1$                                	| $1$                                 	|
|-----	|-------------------------------------	|-------------------------------------	|
| $-1$ 	| $`r confusion.table.polynomial[1,1]`$ 	  | $`r confusion.table.polynomial[1,2]`$ 	  |
| $1$ 	| $`r confusion.table.polynomial[2,1]`$ 	  | $`r confusion.table.polynomial[2,2]`$ 	  |

Y un error de test del $`r test.error.polynomial`\%$.

- __Bonus-3 (1 punto): Estimar el error de entrenamiento y test por validación cruzada de
5 particiones.__

# Ejercicio 3

__Usar el conjunto de datos Boston y las librerías `randomForest` y `gbm` de R.__

- __1. Dividir la base de datos en dos conjuntos de entrenamiento (80%) y test (20%).__

```{r EXERCISE 3.1}
data.boston <- dplyr::tbl_df(Boston)

indexes <- createDataPartition(data.boston$medv, p = .8, list = FALSE)
partition <- list(training = data.boston[indexes,], test = data.boston[-indexes,])

data.boston.training <- dplyr::as.tbl(partition$training)
data.boston.test <- dplyr::as.tbl(partition$test)
```

- __2. Usando la variable medv como salida y el resto como predictoras, ajustar un
modelo de regresión usando bagging. Explicar cada uno de los parámetros
usados. Calcular el error del test. (1 punto)__

```{r EXERCISE 3.2}
bagging.model <- randomForest(medv ~.,
                              data = data.boston.training,
                              mtray = ncol(data.boston.training) - 1,
                              importance = T)

bagging.predicted <- predict(bagging.model, newdata = data.boston.test)
bagging.test.error <- mean((bagging.predicted - data.boston.test$medv) ^ 2)
```

Para ajustar un modelo mediante _bagging_ hemos de decirle al método `randomForest`
que use todas las variables predictoras del dataset. Además, hemos fijado `importance = T`
para que se nos devuelva la importancia de cada variable predictora. Tras ajustar
el modelo, obtenemos un error en test de $`r bagging.test.error`\%$, y las variables
predictoras más importantes son:

```{r echo = F}
importance(bagging.model)
```

- __3. Ajustar un modelo de regresión usando “Random Forest”. Obtener una
estimación del número de árboles necesario. Justificar el resto de parámetros
usados en el ajuste. Calcular el error de test y compararlo con el obtenido con
bagging. (1 punto)__

```{r EXERCISE 3.3}
randomForest.model <- randomForest(medv ~.,
                                   data = data.boston.training,
                                   importance = T)

randomForest.predicted <- predict(randomForest.model, newdata = data.boston.test)
randomForest.test.error <- mean((randomForest.predicted - data.boston.test$medv) ^ 2)
```

Para obtener un valor óptimo para el número de árboles necesarios basta con mirar
cuando se estabiliza el error conforme aumenta el número de árboles, o coger directamente
el número de árboles que da el mínimo error. 

El error de test que se ha obtenido para este primer ajuste de _RandomForest_ ha 
sido del $`r randomForest.test.error`\%$, veamos si somos capaces de mejorarlo
ajustando el número de árboles.

En la siguiente figura se muestra el número de árboles en función de su error 
cuadrático, también se ha señalado el número de árboles para el que se obtiene 
el menor error:

```{r echo = F}
minmse <- which(randomForest.model$mse == min(randomForest.model$mse))
meanmse <-  mean(randomForest.model$mse)
myplot <- data.frame(Trees = 1:randomForest.model$ntree, MSE = randomForest.model$mse)
ggplot2::ggplot(data = myplot, aes(x = Trees, y = MSE)) + 
  geom_line() + 
  geom_point(aes(x = minmse, y = randomForest.model$mse[minmse], colour = "Min MSE")) +
  scale_color_discrete(name = "") + 
  theme_bw()
```

Tras conocer estos datos, es interesante volver a ejecutar _RandomForest_ para ver
si obtenemos mejores resultados ajustando el parámetro `ntree`. 

```{r echo = F}
randomForest.model <- randomForest(medv ~.,
                                   data = data.boston.training,
                                   ntree = minmse,
                                   mtry = 7,
                                   importance = T)

randomForest.predicted <- predict(randomForest.model, newdata = data.boston.test)
randomForest.test.error <- mean((randomForest.predicted - data.boston.test$medv) ^ 2)
```

Tras realizar un nuevo ajuste con $ntree = `r minmse`$ se ha obtenido un error 
para test del $`r randomForest.test.error`\%$, es una mejora pequeña, pero mejora
al fin y al cabo.

Como análisis quizá deberíamos de considerar que al elegir el `ntree` de menor MSE
podamos estar cometiendo sobre ajuste, sin embargo, tras realizar varios experimentos
se ha comprobado que elegir el `ntree` de menor MSE es el que mejor resultados
da para test.

La principal diferencia con _bagging_ es que este primero usa todas las variables 
predictoras para generar los árboles, mientras que _RandomForest_ usa un subconjunto
de predictores. En el caso que nos ocupa (regresión), se usan $\frac{p}{3}$ variables
predictoras, donde $p$ es el número de variables.

La razón de usar un conjunto de variables predictoras de forma aleatoria cada
vez es para rebajar la correlación entre árboles. De esta forma, si hay una variable
con gran potencial predictor, no estará siempre la primera en el árbol, lo cual
daría resultado a árboles muy parecidos. Seleccionando subconjuntos de variables
predictoras nos aseguramos que los árboles generados estén poco correlados.

- __4. Ajustar un modelo de regresión usando Boosting (usar gbm con distribution =
’gaussian’). Calcular el error de test y compararlo con el obtenido con bagging y
Random Forest. (1 punto)__

```{r EXERCISE 3.4}
boosting.model <- gbm(medv ~ .,
                      data = data.boston.training, 
                      distribution = "gaussian", 
                      n.trees = 5000,
                      interaction.depth = 4)
```

Tras ajustar usando _Boosting_, podemos ver qué variables son las más importantes
en la siguiente gŕafica:

```{r echo = F}
summary(boosting.model)
```

Como se puede apreciar, las más predictoras son `lstat` y `rm`.

```{r EXERCISE 3.4.test.error}
boosting.predicted <- predict(boosting.model, newdata = data.boston.test,
                              n.trees = 5000)
boosting.test.error <- mean((boosting.predicted - data.boston.test$medv) ^ 2)
```

Para calcular el error en test ha sido del $`r boosting.test.error`\%$. 

# Ejercicio 4

__Usar el conjunto de datos OJ que es parte del paquete ISLR.__

- __1. Crear un conjunto de entrenamiento conteniendo una muestra aleatoria de 800
observaciones, y un conjunto de test conteniendo el resto de las observaciones.
Ajustar un árbol a los datos de entrenamiento, con “Purchase” como la variable
respuesta y las otras variables como predictores (paquete tree de R).__

```{r EXERCISE 4.1}
# Do stratified partitioning (So the probability distribution of 
# the response variable is the same in training and test)
data.oj <- dplyr::tbl_df(ISLR::OJ)
indexes <- createDataPartition(data.oj$Purchase, p = .7473, list = FALSE)
partition <- list(training = data.oj[indexes,], test = data.oj[-indexes,])

data.oj.training <- dplyr::as.tbl(partition$training)
data.oj.test <- dplyr::as.tbl(partition$test)

# Fitting a tree
tree.model <- tree::tree(Purchase ~ ., 
                         data = data.oj.training)
```

- __2. Usar la función summary() para generar un resumen estadístico acerca del árbol y
describir los resultados obtenidos: tasa de error de “training”, número de nodos
del árbol, etc. (0.5 puntos)__

El `summary` obtenido para el modelo es:

```{r EXERCISE 4.2, echo = F}
summary(tree.model)
```

Como vemos, las variables más predictoras son `LoyalCH, PriceDiff, ListPriceDiff`
y `PctDiscMM`. Con un error para training del $16\%$ y únicamente 7 nodos terminales.

- __3. Crear un dibujo del árbol e interpretar los resultados (0.5 puntos)__

```{r echo = F}
plot(tree.model)
text(tree.model, pretty = 0)
```

La interpretación que podemos darle a este árbol podría ser que el factor decisivo
que lleva a los consumidores a comprar un tipo de zumo u otro es cómo de leales
son a la marca _CH_, aunque con un porcentaje casi cercano al $50\%$ podría decirse
que es aleatorio. Son más interesantes las siguientes ramas del árbol. Entre
los consumidores no leales a _CH_ (Rama de la izquierda) el consumidor directamente
compra la otra marca de zumo (_MM_). Sin embargo, entre los leales a _CH_, existe
la posiblidad de que compren zumo de la competencia si el ususario tiene un porcentaje
de lealtad inferior al $76\%$, la diferencia de precio es menor de 0.23 y además 
la competencia hace un descuento superior al $19\%$, bajo estas condiciones
el consumidor comprará _MM_.

- __4. Predecir la respuesta de los datos de test, y generar e interpretar la matriz de
confusión de los datos de test. ¿Cuál es la tasa de error del test? ¿Cuál es la
precisión del test? (1 punto)__

```{r EXERCISE 4.3}
tree.predicted <- predict(tree.model,
                          newdata = data.oj.test,
                          type = "class")
confusion.table <- table(tree.predicted, data.oj.test$Purchase)
tree.test.error <- mean(tree.predicted != data.oj.test$Purchase)*100
```

La matriz de confusión calculada es la siguiente:

|     	| CH                            | MM                          	|
|-----	|------------------------------	|------------------------------	|
| CH 	  | $`r confusion.table[1,1]`$ 	  | $`r confusion.table[1,2]`$ 	  |
| MM 	  | $`r confusion.table[2,1]`$ 	  | $`r confusion.table[2,2]`$ 	  |

De la cual podemos saber que la tasa de error para test del modelo de árboles
es del $`r tree.test.error`\%$, lo cual es bastante alta. La precisión por tanto
es del $`r 100 - tree.test.error`\%$

- __5. Aplicar la función cv.tree() al conjunto de “training” y determinar el tamaño óptimo
del árbol. ¿Qué hace cv.tree? (0.5 puntos)__

- __Bonus-4 (1 punto). Generar un gráfico con el tamaño del árbol en el eje x (número de
nodos) y la tasa de error de validación cruzada en el eje y. ¿Qué tamaño de árbol
corresponde a la tasa más pequeña de error de clasificación por validación cruzada?__


